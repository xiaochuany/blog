{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Blog","text":"<p>subscribe via RSS</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:ai-assistant","title":"AI assistant","text":"<ul> <li>            A case study for AI assisted review of regulatory texts          </li> <li>            Design an IRB model validation library          </li> <li>            LLM eval for my personal usage          </li> </ul>"},{"location":"tags/#tag:data-engineering","title":"data engineering","text":"<ul> <li>            A dimensional approach to data quality principles          </li> <li>            Essentials of polars for pandas experts          </li> <li>            Exploring a large database          </li> <li>            Generate test dataframe with polars (powered by hypothesis)          </li> <li>            High performance data loading with grain          </li> <li>            Hit ECB data API          </li> <li>            Intro to Snowpark API          </li> <li>            Polars streaming tricks          </li> <li>            Puzzling query optimization behaviour and dtypes shrinking in polars          </li> <li>            SQLite: the absolute basics          </li> <li>            Use code when Dataiku's UI gets in the way          </li> <li>            Working with large datasets in Snowflake          </li> <li>            duckdb basics          </li> </ul>"},{"location":"tags/#tag:dev-tools","title":"dev tools","text":"<ul> <li>            A guide to convolution parameters in neural nets          </li> <li>            A patching pattern          </li> <li>            Build a mortgate calculator with fasthtml          </li> <li>            Cheatsheet for regular expression          </li> <li>            Design an IRB model validation library          </li> <li>            Fork, merge and PR          </li> <li>            High performance data loading with grain          </li> <li>            Logging in python          </li> <li>            Mini-tutorial for python packaging, release and publish          </li> <li>            Static site generator using fasthtml          </li> <li>            Technical writing with material for mkdocs          </li> <li>            Use code when Dataiku's UI gets in the way          </li> </ul>"},{"location":"tags/#tag:life-hack","title":"life hack","text":"<ul> <li>            2026 Resolution          </li> <li>            A guide to preparing tax form in Luxembourg          </li> </ul>"},{"location":"tags/#tag:low-latency-programming","title":"low latency programming","text":"<ul> <li>            GPU puzzles annotated          </li> <li>            High performance data loading with grain          </li> <li>            Intro to JAX          </li> <li>            JAX as a differential calculus tool          </li> <li>            Scalable bootstrapping          </li> </ul>"},{"location":"tags/#tag:quant-methods","title":"quant methods","text":"<ul> <li>            A tale of convexity in credit rating assignment problem          </li> <li>            Hierarchical clustering with 36 LOC          </li> <li>            Monotonic binning with PAVA          </li> <li>            Random forest scaling mini-benchmark          </li> <li>            Repurpose hierarchical clustering for feature engineering          </li> <li>            Scalable bootstrapping          </li> <li>            The Cox survival guide: hazards, models and time          </li> <li>            Unpacking the k-1 in the chi-square test          </li> <li>            Unpacking the n-1 in the t test          </li> <li>            Vasicek in Basel          </li> </ul>"},{"location":"tags/#tag:regulations","title":"regulations","text":"<ul> <li>            A case study for AI assisted review of regulatory texts          </li> <li>            A guide to preparing tax form in Luxembourg          </li> <li>            A simple overview to EU banking regulations          </li> <li>            Basel Framework          </li> <li>            Vasicek in Basel          </li> </ul>"},{"location":"tags/#tag:research","title":"research","text":"<ul> <li>            Repurposing gitHub for mathematical research          </li> </ul>"},{"location":"2024/12/18/mkdocs-intro/","title":"Technical writing with material for mkdocs","text":"<p>Abstract</p> <p>Material for mkdocs is a documentation framework adopted by many popular python libraries e.g. this, this and this. The plugins and markdown extensions make writing rich technical content delightful e.g. maths, code, docstring parsers, and more. The main apeal of the project is that \"it's just markdown\" (with a bit of extended syntax). This tutorial provides a step by step intro to setting up a blog website (such as this one!) with this framework.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#credit","title":"Credit","text":"<p>template by material team</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#first-steps-to-follow","title":"First steps to follow","text":"<ol> <li><code>pip install mkdocs-material</code></li> <li>initialize docs locally with <code>mkdocs new .</code></li> <li>create empty remote repo and push local to remote</li> <li>[github] add CI config file as per material docs</li> <li>[github] set publish branch to <code>gh-pages</code></li> </ol> <p>After these steps, a site is published and CI should start working. New edits of docs in the main branch would trigger rebuild of the site, i.e. focus on the writing and let robots build the site automatically.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#customize-the-theme","title":"Customize the theme","text":"<p>In <code>mkdocs.yml</code>, config the theme, then customize it e.g. add dark/light toggle</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-blog-plugin","title":"Add blog plugin","text":"<p>Without any setting, the plugin creates a directory structure (<code>/docs/blog/posts</code>).</p> <p>Setting the following should be straightforward</p> <ol> <li>blog_toc</li> <li>archive_date_format</li> <li>categories_allowed</li> <li>pagination_per_page</li> </ol> <p>The <code>post_slugify</code> setting makes use of python markdown extension package, which is a dependency of <code>material</code> and installed with it.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-tags-plugin","title":"Add tags plugin","text":"<p>Add <code>tags</code> plugin in <code>mkdocs.yml</code> and create a file <code>tags.md</code> in <code>docs</code>.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-rss-plugin","title":"Add rss plugin","text":"<p>rss is a third party plugin which requires installation.</p> <p>It is important that have the site_name, site_description and site_url settings configured. The RSS plugin makes use of this information to construct the feed, so make sure you have configured them.</p> <ul> <li><code>pip install mkdocs-rss-plugin</code></li> <li>add rss plugin in  <code>mkdocs.yml</code></li> <li>add the required package also in ci: run <code>pip install mkdocs-rss-plugin</code></li> </ul>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#extra","title":"Extra","text":"<p>We can add social media links in the footer. </p> <p>We can also add in the background an analytics tool. The easiest solution is GA4 from google since we do not own a custom domain.  In GA, enhanced GA measurement toggles are mostly optional, for blogs, only page views, scrolls, outbound clicks matter. </p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#markdown-extension-maths","title":"Markdown extension: maths","text":"<p>Add markdown extension for writing maths</p> <ul> <li>add <code>arithmatex</code> extension in <code>mkdocs.yml</code></li> <li>add <code>mathjax.js</code> to extra_javascript (create a js file in <code>docs/js</code>, define macros as needed)</li> </ul> <p>One can use <code>katex</code> instead per documentation.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#markdown-extension-code-block","title":"Markdown extension: code block","text":"<p>Add necessary markdown extension for writing code.</p> <ul> <li>highlight</li> <li>inlinehilite</li> <li>snippets</li> <li>superfences</li> </ul>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#nav","title":"Nav","text":"<p>Rename nav sidebar and/or turn it into tabs.</p> <p>Add to <code>features</code> in <code>theme</code></p> <ul> <li>navigation.tabs : tabs</li> <li>navigation.indexes : index attached to sections (overview page)</li> </ul> <p>Also add <code>nav</code> section to be explicit what to include in the sidebar/tabs.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#author","title":"Author","text":"<p>Add author metadata <code>docs/blog/.author.yml</code> and use it in all posts (by specifying <code>authors</code> in the header of posts).</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#metadata-of-posts","title":"Metadata of posts","text":"<p>Include metadata in the header of posts:</p> <ul> <li>date (enough)</li> <li>authors</li> <li>tags</li> <li>categories</li> <li>slug (if want to customize)</li> <li>readtime</li> </ul>","tags":["dev tools"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/","title":"Essentials of <code>polars</code> for <code>pandas</code> experts","text":"<p>Abstract</p> <p><code>pandas</code> is a standard tool for every data professional, although it does not scale well in production. Yet, being a standard is a strategic position to be, as libraries coming to solve the scale issue tend to meet data professionals where they are, by mimicing the <code>pandas</code> API (think: <code>dask</code>, <code>pyspark.pandas</code>).</p> <p><code>polars</code> is a new-ish tool that is probably replacing <code>pandas</code> at the time of writing. The goal of this post is to introduce the kind of mindset change needed to fully exploit <code>polars</code> in production.</p> <p>What does a dataframe library do? A few things came to mind.</p> <ul> <li>merge/join</li> <li>group by</li> <li>aggregate</li> <li>windows function</li> <li>rolling windows</li> <li>etc</li> </ul> <p>There are a plethora of dataframe libraries doing all these things. Yet <code>polars</code> seems to me a clear winner in the game of \"finding the successor of <code>pandas</code>\". </p> Update Feb 2025 <p><code>polars</code> may well replace <code>pyspark</code> with their annoucement  of the cloud offering for vertical and horizontal scaling. Indeed, <code>polars</code> solves the same scaling problem as <code>pyspark</code> does and its API is very close to <code>pyspark.sql</code> although their implementations are very different.</p> <p>Born in 2020, <code>polars</code> released its version 1.0 in mid-2024, officially marking its production readiness. A popular saying about <code>polars</code> is that people \"came for the speed, stay for the syntax\". This pronounces two ways in which <code>polars</code> is awesome</p> <ul> <li>fast</li> <li>elegant syntax: both intuitve and expressive.</li> </ul> <p>The goal of this tutorial is to present some basic concetps for effective use of the library. But first, let's briefly mention</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#why-polars-is-fast-in-a-nutshell","title":"Why polars is fast in a nutshell","text":"<p>The speed is achieved by lazy execution, query optimizer, vectorized query engine, parallelism. It would be too arrogant of me to claim that I can explain all these terms with complete precision.  The interested reader is invited to watch talks<sup>1</sup> <sup>2</sup> <sup>3</sup> by the creator of the library Ritchie Vink for details.</p> <p>In a nutshell,</p> <ul> <li>Lazy execution builds the computation graph of data transformations without loading any data into memory. Think of this as composition of functions where no input is required (the schema of the input must be known though).</li> <li>Query optimizer optimizes user's query, making them more efficient. The laziness leaves the room for optimising user's query e.g. changing the order of certain operations, fusing them, and all sorts of smart tricks that can boost the computation efficiency before any data is loaded. This is similar to  what machine learnig compilers would do (think <code>torch.compile</code> and <code>jax.jit</code>).</li> <li>Vectorized query engine leverages columnar memory format (<code>arrow</code>) and hardware optimizations (e.g. SIMD) for array manipulations.</li> <li>Parallelism is a paradigm to distribute computation workloads effectively across all the CPU/GPU cores.</li> </ul>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#eager-vs-lazy","title":"Eager vs Lazy","text":"<p>Eager is the \"opposite\" of lazy. In eager mode, data is loaded into memory before the first operation, and all operations are executed sequentially, one after another as user's query. This can be both wasteful and inefficient.</p> <p><code>pandas</code> only operates in eager mode, while <code>polars</code> operates in eager/lazy as per user's needs. The lazy API of <code>polars</code> is almost identical to the eager one so there is little mental overhead to users.</p> <p>Lazy is good for speed, but this does not mean that Eager is useless. Data professionals rarely build data pipelines in one go, rahter, they build iteratively, one step at a time. This is where eager mode shines.</p> Example <p>Think about this query in the diagram where the input <code>in</code> is processed in some ways to achieve the result <code>out</code>.</p> <p><pre><code>graph LR\n    A[in] --&gt; B[result1];\n    A --&gt; C[result2];\n    A --&gt; D[result3];\n    C --&gt; D;\n    B --&gt; E[out];\n    D --&gt; E;</code></pre> With the eager API, result2, result1 must be computed fully and stored in memory, then result3, before producing the output. In developement, it indeed makes sense to produce all these intermediate results and stored them in memory to check the accuracy of the results. However, it can be the case that the <code>out</code> is what really matters in production, and the intermediate results are just implementation details. Lazy execution would be the way to go in such cases. With the lazy API, the query is optimized, vectorized, multi-threaded so the execution can be ~10x faster.</p> <p>Concretely, the data structure to operate on with the eager API is called <code>DataFrame</code>, and for the lazy API it is called <code>LazyFrame</code>.</p> <p>There is one caveat/gotcha when working with lazy API. One might use blindly all data operations that are available in the eager API and surprised that they are not availabe in the lazy API.  To understand why this happens, it is crucial to know that <code>LazyFrame</code> must be agnostic of the data values and it must be aware of the schema. The schema of the output of <code>pivot</code> operation can NOT be determined if the data values are not known (how many columns are there?). It is the same story with <code>transpose</code>.</p> <p>Once the query reaches a point where lazy API cannot do what the user wants, it is time to switch to the eager API, do that operation eagerly, and switch back to lazy for the speed.</p> <p>In code, the pattern looks like this</p> <pre><code>lf.collect().pivot(...).lazy().other_lazy_operations()\n</code></pre> <p>Here <code>.collect()</code> would turn a <code>LazyFrame</code> into <code>DataFrame</code> (so <code>lf</code> is materialized/stored in memory). After the <code>pivot</code> operation, <code>.lazy()</code> would turn the dataframe back to <code>LazyFrame</code>.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#expression","title":"Expression","text":"<p><code>polars</code> offers a beloved Expression API. In essence, expressions are functions that associate an ouput array to an input array (think <code>numpy</code> operators). As operators, they can/should be isolated from data (Frame/Series). Expression can be appiled to an existing Expression to obtain a new Expression in the sense of function composition. This allows users to build complicated queries by composing building blocks offered in the Expression API.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#syntax-difference","title":"Syntax difference","text":"<p>The migration guide from official docs serves as a good summary<sup>4</sup>.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#go-further","title":"Go further","text":"<p>To make full advantage of <code>polars</code>, using lazy execution whenever possible and getting familiar with Expression are probably sufficient for most of the everyday data processing jobs.</p> <p>To go further, the official User Guide is the best source, which is being revised and improved actively (~ end of 2024).</p> <ol> <li> <p>https://www.youtube.com/watch?v=GOOYbl3cqlc&amp;ab_channel=PyConLithuania \u21a9</p> </li> <li> <p>https://www.youtube.com/watch?v=ubqF0yGyphU&amp;t=3556s&amp;ab_channel=SuperDataScience%3AML%26AIPodcastwithJonKrohn \u21a9</p> </li> <li> <p>https://www.youtube.com/watch?v=yYAVrVMGaMY&amp;ab_channel=PyData \u21a9</p> </li> <li> <p>https://docs.pola.rs/user-guide/migration/pandas/#key-syntax-differences \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/","title":"Mini-tutorial for python packaging, release and publish","text":"<p>Abstract</p> <p>This mini-tutorial is a beginner's cheatsheet to python packaging. Check Python packaing user guide for an authoritative guidance on the topic.  </p>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#classical-way","title":"Classical way","text":"<p>It is good practice to setup an isolated and clean environment e.g. with standard library <code>venv</code>. After that, </p> <ul> <li> <p>install packages for building wheels and source distributions:</p> <pre><code>pip install wheel build\n</code></pre> </li> <li> <p>create <code>setup.py</code> where one can specify the requirements and meta-data:  </p> <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name='my_package',\n    version='0.1.0',\n    packages=find_packages(),\n    install_requires=[\n        'numpy',\n        'pandas',\n    ],\n    # Optional: Add more metadata\n)\n</code></pre> </li> <li> <p>actually create wheels and source distribution:</p> <pre><code>python -m build\n</code></pre> </li> </ul>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#alternatively-with-uv","title":"Alternatively, with <code>uv</code>","text":"<p><code>uv</code> is a modern python dev tool, see features and  install guide.</p> <p>It is compliant with PEP 517 <sup>1</sup>, PEP 518 <sup>2</sup>. </p> <p>Use the project interface of <code>uv</code> to init project and add dependencies. </p> <pre><code>uv init\nuv add DEPENDENCIES\n</code></pre> <p>then </p> <pre><code>uv build\n</code></pre>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#create-a-tag-and-release","title":"Create a tag and release","text":"<pre><code>git tag -a v0.1.0 -m 'release message'\ngit push --tags\n</code></pre> <p>This will create a tag named \"v0.1.0\" with the message \"release message\". The distribuion files will be displayed as assets for the tag.</p>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#publish","title":"Publish","text":"<p>Publish to pypi index requires the developer to setup an account and get the API token. Using <code>uv</code>, run this</p> <pre><code>uv publish --token TOKEN\n</code></pre> <p>with the dev's API token in place of TOKEN. </p> <p>Useful</p> <p>https://pydevtools.com/handbook/</p> <ol> <li> <p>why <code>pyproject.toml</code>? https://peps.python.org/pep-0518/ \u21a9</p> </li> <li> <p>understand build frontend, build backend. https://peps.python.org/pep-0517 \u21a9</p> </li> </ol>","tags":["dev tools"]},{"location":"2025/01/12/intro-to-jax/","title":"Intro to JAX","text":"<p>Abstract</p> <p>Mutiple packages came to solve the speed issue of <code>python</code> in scientific computing. The de facto standard is of course <code>numpy</code>. Think also <code>numba</code>, <code>jax</code>, <code>torch</code> or even a new language  that go beyond what <code>numpy</code> offers.  This post is a quick intro to my personal preference as of Jan 2025. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#pytree","title":"PyTree","text":"<p>At the center of JAX is undoubtedly <code>jax.Array</code>.  A nested container of these objects is called a PyTree, e.g. lists of dicts of Array. The Array's are the leaves of the PyTree. It is possible to register a custom Pytree class.</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#numpy-operations","title":"Numpy operations","text":"<p><code>jax.numpy</code> module is easy to use for <code>numpy</code> users because they share near identical API, intentionally. Operations in <code>jax.numpy</code> are built on top of the XLA compiler primitives for high performance numerical computations. </p> <p>There is the distinction between abstract structure and concete value of arrays in jax. A <code>ShapeDtypeStruct</code> object captures dtype and shape, whereas <code>Array</code> carries the concrete values too.</p> <p>The caveat of using <code>jax.Array</code> is that they are immutable. The \"mutation\" syntax is different from <code>numpy</code> and actually generates new arrays instead of mutate them in place, in constrast to <code>numpy</code>. </p> <pre><code>import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\n\ny = x.at[1:].set(0)\nprint(y)\n\nz = x.at[1:].add(1)\nprint(z)\n</code></pre> <pre><code>[1 0 0]\n[1 3 4]\n</code></pre> <p>Broadcasting is arguably an elegant design of the numpy array API. It works the same for jax arrays. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#vmap","title":"<code>vmap</code>","text":"<p><code>vmap</code> vectorizes a function by adding a batch dimension at the leading axis to all the leave nodes of a function's arguments. In other words, vmap is like <code>np.stack</code> individual results of a sequence of function calls. We use <code>vmap</code> to avoid manual batching, manual stacking etc.  </p> <p>Since the arguments of the function at hand can be an arbitrary PyTree, the axis 0 will be the batch dimension for all arrays in each  PyTree, and the shape of the supplied arrays embedded in the PyTrees should be compatible with that.</p> <p>More control over the operating axes of <code>vmap</code> is possible. Below  <code>in_axes=(None,0)</code> adds batch dimension at the leading axis for the  second positional argument of the function only. If we do not specify <code>in_axes</code> (defaults to 0 or equivalently (0,0)), the vectorized function <code>vmap(f)</code> would add batch dimension to all leave nodes of the input PyTree's. This behaviour is not compatible with the inputs in our example, because beta.shape is (5,) and x.shape is (2,5), so their leading dimensions are not the same.</p> <pre><code>import jax\nimport jax.numpy as jnp\n\ndef li(beta, x):\n    \"\"\"linear predictor with multiple regressors\"\"\"\n    return beta.dot(x)\n\nbeta = jnp.arange(5)\n\njax.vmap(li, in_axes=(None,0))(\n    beta, \n    jr.normal(jr.key(1), (2,5))\n)\n</code></pre> <p>Additionally, we can specify <code>out_axes</code> along which we stack the results, to use our <code>np.stack</code> analogy again. But in our example above since the output is 1D array, we can specify nothing other than <code>out_axes=0</code>, which is the default already.  </p> <p>More details about vmap here</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#random-numbers","title":"Random numbers","text":"<p>Be aware that JAX follows the functional programming paradigm. This implies explicit key handling for samplers. Samplers can be composed with <code>vmap</code> to achieve vectorization across all parameters. </p> <p>For example,  <code>random.t</code> has two parameters key and df, one can supply one array of keys to generate a collection of t distributed random variables with the same degree of freedom like so </p> <pre><code>jax.vmap(random.t, in_axes=(0,None))(keys:Array, df=2)\n</code></pre> <p>or generate a collection each with a different degree of freedom like so </p> <pre><code># identical\njax.vmap(random.t)(keys:jax.Array, df:jax.Array)\njax.vmap(random.t, in_axes=0)(keys:jax.Array, df:jax.Array)\njax.vmap(random.t, in_axes=(0,0))(keys:jax.Array, df:jax.Array)\n</code></pre> <p>Here is an example of working with PRNG keys. The warning from the official docs is NEVER reuse a key. Split them.  </p> <pre><code>import jax\nimport jax.random as jr\nimport jax.numpy as jnp\n\nkey = jr.key(42)\nout1 = jr.normal(key, (10,))\nout2 = jax.vmap(jr.normal)(jr.split(key, 10)) # vmap adds batch dimension at leading axis by default\nout3 = jax.vmap(jr.t)(\n    key = jr.split(jr.key(21),10),\n    df = jnp.arange(10)+1 # degree of freedom of 10 t-distributions\n)\n\nprint(out1)\nprint(out2)\nprint(out3)\n</code></pre> <pre><code>[-0.02830462  0.46713185  0.29570296  0.15354592 -0.12403282  0.21692315\n -1.440879    0.7558599   0.52140963  0.9101704 ]\n[ 0.07592554  0.60576403  0.4323065  -0.2818947   0.6549178  -0.2166012\n -0.25440374  0.2886397   0.14384735 -1.3462586 ]\n[ 0.6342837   0.6981538   1.959329    0.35705897  0.95073795 -2.2646627\n  0.93203527  0.50947154  1.1138752  -0.03552625]\n</code></pre> <p>Again we provide <code>df</code> of type Array because JAX expects Array for vectorized arguments. Providing list (pytree!) of the same size wouldn't work. </p> <p>See details in the official docs. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#jit-if-you-can","title":"<code>jit</code> if you can","text":"<p>In eager mode, JAX transformations/opertors run sequentially one at a time. With <code>jit</code> compilation, the computation graph of a jax program is optimized (e.g. rearrange, fuse transformations) by XLA compiler so that it runs faster.</p> <p>The idea of <code>jit</code> compilation is to run/compile the program in python once, and cache the compiled program for repetitive evaluations. Compilation introduces overhead. So what we want is that inputs of the same dtype and shape would not trigger re-compilation. jax achieves this by tracing the dtype and shape of all operands in a computational graph, and optimize this abstract structure without having to know the exact values.  </p> <p>The approach has some implications. JAX transformations such as vmap must be agnostic of the values of the inputs, and they must know the shape and dtype of the inputs and outputs to comply with the XLA's requirement of being compile-time static. </p> <p>One should <code>jit</code> everything for speed but this is not always possible. Consider this.</p> <pre><code># NOT WORKING!\nfrom jax import jit\n\n@jit\ndef f(x):\n    if x &gt; 0:\n        return x\n    else:\n        return jnp.stack([x,x])\n\ntry: f(3)\nexcept Exception as e: print(e)\n</code></pre> <p>This function is not <code>jit</code>'able because at compile time the compiler cannot get the shape of the ouput without knowking the concete value of <code>x</code>. Of course one can get around the <code>if</code> statement with <code>jnp.where</code> which create branches in the computation graph. Conveniently, ALL <code>jax.numpy</code> operations are jittable.</p> <p>More broadly, none of the python control flows is jit compatible.  Consider this function </p> <pre><code>def xfibonacci(x, n):\n\n    a, b = 0, 1\n    for _ in range(1, n):\n        a, b = b, a + b\n\n    return x*b\n</code></pre> <p>We cannot jit compile this function because the number of iterations cannot be deduced from the shape and dtype of the input <code>n</code>. Jax needs to know the exact number of operations to include in the computational graph before compilation. </p> <p>There are two possible fix.  First, we can use <code>static_argnums</code> to make a positional argument static at  compile time i.e. treating <code>n</code> as constant. Effectively the loop is unrolled at compile time. </p> <pre><code>from jax import jit\nfrom functools import partial\n\n@partial(jit,static_argnums=1)\ndef xfibonacci(x, n):\n\n    a, b = 0, 1\n    for _ in range(1, n):\n        a, b = b, a + b\n\n    return x*b\n</code></pre> <p>Or we can use a JAX control flow primitive. </p> <pre><code>@jax.jit\ndef xfibonnacci(x, n):\n\n    def body_fun(i,val):\n        a,b = val\n        return b, a+b\n\n    _, out  = fori_loop(1, n, body_fun, (0,1)) \n\n    return x*out\n</code></pre> <p>For while loops, the number of iterations can be dependent of the input values, but we can still use the JAX while loop primitive. Consider Newton's algorithm for the square root:  </p> <pre><code>def newton(N, tol=1e-6):\n    x = N/2.0\n    error = tol + 1.\n    while error&gt;tol:\n        x_next = 0.5*(x+N/x)\n        error, x = abs(x - x_next), x_next\n    return x\n</code></pre> <p>We can rewrite this function using JAX control flow primitive <code>jax.lax.while_loop</code>. We need to supply the primitive with condition function and body function.   </p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.lax import while_loop\n\n@jax.jit\ndef newton_sqrt(N, tol=1e-6):\n\n    def cond_fun(val):\n        x, e = val\n        return e &gt; tol\n\n    def body_fun(val):\n        x, _ = val\n        x_next = 0.5 * (x + N / x)\n        e = abs(x - x_next)\n        return x_next, e\n\n    init = N / 2.0, tol + 1.0 # Ensure the loop runs at least once\n\n    final_estimate, _ = while_loop(cond_fun, body_fun, init)\n    return final_estimate\n</code></pre> <p>Details here. More on control flow operators: check this page.</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#grad-grad","title":"<code>grad</code> <code>grad</code>!","text":"<p>When you can compose grad and jacobian to get Hessian, you know automatic differentiation of <code>jax</code> is done right.   Define your function and grad your way out with respect to any variable you are interested. Compose it with <code>jit</code> for performance. Some obvious caveats:</p> <ul> <li>functions must be scalar-valued (there is <code>jax.jacobian</code> for vector valued funcs)</li> <li>inputs must be continous (e.g. float)</li> <li>functions must be differentiable (indexing, argmax etc are not ok)</li> </ul>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#simple-profilingtesting","title":"Simple profiling/testing","text":"<p>The common pitfall of profiling jax is the unawareness of the async dispatch feature. This is the feature that allows JAX programs to run subsequent code before some heavy computation finishes. </p> <p>For instance, below we have a heavy matrix multiplication to run first, then a light query of array shape. In reality we first see the shape printed, then the result. The reason is  </p> <ol> <li>jax does not wait for <code>x.dot(x)</code> to finish before executing the print statment;</li> <li>even though out is not completed, its shape and dtype are known and passed to the print statement.    </li> </ol> <pre><code>import jax.random as jr\n\nx = jr.normal(jr.key(1), (10000,10000))\n\ndef fun(x):\n    out = x.dot(x)\n    print(out.shape, out.dtype)\n    return out\n\nfun(x)\n</code></pre> <p>Therefore, we should block main process until the computation before measuring time like so </p> <pre><code>fun(x).block_until_ready()\n</code></pre> <p>There is no testing module in <code>jax</code>.  Use <code>np.testing.assert_allclose</code> to check the results.  </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#type-hints","title":"Type hints","text":"<p>Use <code>jax.typing.ArrayLike</code> for array input and <code>jax.Array</code> for array output. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#reference","title":"Reference","text":"<p>https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods https://jax.readthedocs.io/en/latest/working-with-pytrees.html#example-of-jax-tree-map-with-ml-model-parameters https://jax.readthedocs.io/en/latest/working-with-pytrees.html#custom-pytree-nodes https://jax.readthedocs.io/en/latest/stateful-computations.html#simple-worked-example-linear-regression</p>","tags":["low latency programming"]},{"location":"2025/02/15/survive/","title":"The Cox survival guide: hazards, models and time","text":"<p>Abstract</p> <p>This note is an overview of survival analysis with a focus on the Cox proportional hazards model, a key tool for modelling time-to-event data. We define hazard functions, address right-censoring, and derive estimators like Nelson-Aalen and Kaplan-Meier. The Cox model's semiparametric form and partial likelihood estimation are discussed, along with evaluation metrics such as concordance probability.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#what-is-hazard","title":"What is hazard?","text":"<p>The cumulative hazard function for a time-to-event (e.g. death in medical trials) distribution can be defined in the most generality using the Lebesgue-Stieltjes integral, which accommodates both continuous and discrete distributions and more.</p> <p>For a survival time \\( T \\) with survival function \\( S(t) = P(T &gt; t) \\), the cumulative hazard function \\( H(t) \\) is defined as:</p> \\[ H(t) = \\int_0^t \\frac{dF(s)}{S(s^-)}, \\] <p>where \\( F(s) = P(T \\leq s) \\) and \\( S(s^-) = \\lim_{u \\uparrow s} S(u) = P[T\\ge s] \\) is the left-continuous survival function. Here the integral is a Lebesgue-Stieltjes integral (TLDR this makes sense because F is monotone thus of finite variation).</p> <p>Continuous Distributions:</p> <p>If \\( T \\) is a continuous random variable with density \\(f\\), then \\( S(s^-) = S(s) \\) and the cumulative hazard reduces to</p> \\[ \\begin{align} H(t) &amp;= \\int_0^t h(s) ds \\notag\\\\ h(t) &amp;= \\frac{f(t)}{S(t)} \\label{e-hazard-func} \\end{align} \\] <p>Notice that \\(f(t)=-S'(t)\\) so by \\(\\eqref{e-hazard-func}\\) the survival function satisfies the ODE</p> \\[h(t) = - S'(t)/S(t)\\] <p>with boundary values \\(S(0)=1, S(\\infty)=0\\). One concludes that \\( S(t) = \\exp(-H(t)) \\).  Given an estimator of \\(H\\) or \\(h\\), one can estimate \\(S\\) through this relation.</p> <p>Discrete Distributions:</p> <p>If \\( T \\) is discrete with support \\(\\{ t_1, t_2, \\dots \\}\\), the cumulative hazard is a sum over discrete hazards:</p> \\[ H(t) = \\sum_{t_j \\leq t} h(t_j), \\quad \\text{where } h(t_j) =  \\frac{F(t_j)-F(t_j^-)}{P[T\\ge t_j]} =  P[T = t_j \\mid T \\geq t_j]. \\] <p>We can and will assume that \\(t_1&lt;t_2&lt;t_3\\) and so on. Any \\(t\\ge t_1\\) there exists a unique \\(j\\) such that \\(t_j\\le t&lt;t_{j+1}\\). For \\(t\\ge t_1\\), we have</p> \\[ \\begin{align*} S(t) &amp;= P[T&gt;t] = P[T&gt;t_i, \\forall i = 1, ..., j] \\\\ &amp; = P[T\\ge t_1] P[T&gt;t_1|T\\ge t_1] P[T&gt;t_2 | T&gt;t_1] ... P[T&gt;t_j|T&gt;t_{j-1}] \\end{align*} \\] <p>where we used the fact that \\(P[T\\ge t_1]=1\\). Notice that \\(\\{T&gt;t_{i-1}\\}= \\{T\\ge t_i\\}\\), hence</p> \\[ S(t) = \\prod_{t_j \\leq t} \\left(1 - h(t_j)\\right) \\] <p>Note that \\( H(t) \\neq -\\log S(t) \\) here; instead, \\( -\\log S(t) = \\sum_{t_j \\leq t} -\\log(1 - h(t_j)) \\), which differs from \\( H(t) \\).</p> <p>Again given an estimator of \\(h\\) (note this \\(h\\) is different from the \\(h\\) in the continuous case), one can estimator \\(S\\) through this relation.</p> <p>Other cases:</p> <p>For mixed distributions with both continuous and discrete components, \\( H(t) \\) combines integrals over continuous regions and sums over discrete jumps. But it can get more interseting, think Cantor's function (devil's staircase), how can we  express the hazard differently?</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#right-cencoring","title":"Right-cencoring","text":"<p>In medical applications, it is often necessary to incorporate a censoring distribution on top of the time-to-event distribution. Let \\(C\\) denote a nonnegative random variable and assume that the observation is \\(Y = \\min(T,C)\\) and \\(\\Delta = I(T\\le C)\\). The indicator evaluates to 1 if the event happens no later than the censoring time (eg dead at time \\(Y\\)), and 0 otherwise (eg alive but left the trial at time \\(Y\\)). Typically one assume independence between \\(T\\) and \\(C\\). What we really cares about is the time to event distribution which is not observed directly. How to estimate the hazard (hence the survival function from discussion above) in such case?</p> <p>It is a nice exercise to show that the independence assumption yields (see note<sup>1</sup> for derivation) </p> \\[ H_T(t) = \\int_0^t \\frac{dF_{Y,1}(u)}{S_Y(u^-)} \\] <p>where \\(H_T\\) is the cumulative hazard function of \\(T\\) and \\(F_{Y,1}(u)=P[Y\\le u, \\Delta=1]\\). Now the whole thing is estimable by replacing all terms by their empirical counterpart, which is</p> \\[ \\hat H_T(t) = \\sum_{i=1}^n \\frac{\\Delta_i I(Y_i\\le t)}{\\sum_{j} I(Y_j\\ge Y_i)}. \\] <p>Simple observations about the function \\(\\hat H_T\\):</p> <ol> <li>piecewise constant</li> <li>right continuous</li> <li>it jumps and only jumps at \\(T_i\\) when \\(\\Delta_i=1\\) (equivalently \\(T_i=Y_i\\))</li> <li>those \\(T_i\\) with \\(\\Delta_i=1\\) do not have to be distinct, it is possible that \\(T_i=T_j, \\Delta_i=\\Delta_j=1\\) for some \\(i\\neq j\\).</li> </ol> <p>Let \\(\\{t_1, ...,t_m\\} = \\mbox{set}(\\{T_i: \\Delta_i=1\\})\\). We conclude that \\(\\hat H\\)</p> <ul> <li>jumps at all \\(t_i\\)</li> <li>with magnitude \\(D_i/N_i\\) where \\(D_i = \\sum_j I(T_j=t_i, \\Delta_j = 1)\\) is the number of observed events at time \\(t_i\\), and \\(N_i=\\sum_{j}I(Y_j\\ge t_i)\\) is the number of invididuals at risk just before \\(t_i\\)</li> </ul> <p>Summing over these ratios gives an equivalent expression</p> \\[ \\hat H_T(t) = \\sum_{i:t_i\\le t} \\frac{D_i}{N_i}. \\] <p>Let's recap: we estimate the hazard of \\(T\\) using empirical distribution \\(Y\\) and empirical conditional distribution of \\(Y\\) given \\(\\Delta=1\\), then using the general relationship between hazard and survival function to get an estimate of the survival function of \\(T\\) so we come full circle!</p> <p>We have thus two estimators for the survival function of \\(T\\).</p> \\[ \\begin{align*} \\hat S_T^{\\rm{NA}}(t) &amp;= \\exp( - \\hat H_T(t)) \\\\ \\hat S_T^{\\rm{KM}}(t) &amp;= \\prod_{t_i\\le t} (1 - \\frac{D_i}{N_i}) \\end{align*} \\] <p>where KM stands for Kaplan-Meier and NA for Nelson-Aalen.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#cox-proportional-hazard-model","title":"Cox proportional hazard model","text":"<p>So far we have discussed observation of time-to-event data (censored or not) without covariate effect. Cox PH model incorporates the covariate effect into the modelling of hazard.</p> <p>Fun fact</p> <p>Google scholar reported 63k citations [data as of 11 Feb 2025] of the paper in 1972 by Cox <sup>2</sup> on his nowadays called \"proportional hazard model\". Some says that this is the 2nd most cited papers in science, but that is probably not true any more given that the deep learning industry has exploded the number of publications e.g. the transformer paper <sup>3</sup> has 152k citations now, which was published in 2017.</p> <p>Anyhow, Cox proportional hazard model remains the de facto standard model in survival analysis. It assumes a semiparametric form for the hazard (assuming the hazard measure absolutely continuous)</p> \\[ h_{T|X}(t|x) = h_0(t) \\exp(\\beta\\cdot x) \\quad t&gt;0, \\, x\\in \\RR^p. \\] <p>It is proportional with regard to the change of covariate \\(x\\) for each invididual i.e. \\(\\frac{h(t|x)}{h(t|x+\\delta)}\\) is constant over time. This is a strong assumption yet quite flexible because \\(h_0\\) is an arbitrary function and it allows for (parametric) covariate effect.</p> <p>Estimation and hypothesis testing of \\(\\beta\\) is obviously of utmost importance. This is done by maximizating partial likelihood (likelihood conditioning on the occurence time of the observed events, due to PH assumption, we have only \\(\\beta\\) in the partial likelihood). The exposition of some lecture notes<sup>4</sup> is very clear so I will not waste tokens on it. See also the notes mentioned before<sup>1</sup> for a slightly different perspective.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#evaluating-models","title":"Evaluating models","text":"<p>Having estimated the beta parameters in the Cox PH model, we can rank the riskiness of individuals using  \\(g(x)= x\\cdot\\hat\\beta\\) which assigns the same ranking to individuals as with the hazard model at any point in time,  thanks to the PH assumption and the monotonicity of exponential function. </p> <p>A higher risk score translates to a shorter time to event, while a lower risk score translates to a longer time to event.  A natural evaluation metric is the quality of this ranking. One widely used measure of this type is the concordance probability.  The idea is to capture the probability that  a pair of independent samples \\((T_1,C_1,X_1), (T_2,C_2,X_2)\\) are ordered correctly by the risk scores \\(g(X_1)\\) and \\(g(X_2)\\).  To formulate this precisely, introduce for \\(i\\in \\{1,2\\}\\) the events</p> \\[ \\begin{align*} E_i &amp;= \\{T_i&lt;\\max(T_1,T_2)\\} \\\\ F_i &amp;= \\{g(X_i)&gt;\\min(g(X_1),g(X_2))\\} \\end{align*} \\] <p>Clearly \\(E_1\\cup E_2 = \\{T_1\\neq T_2\\}\\). </p> <p>The concordance probability is </p> \\[ \\begin{equation} \\label{e-cpop} P[ \\cup_{i=1}^2 (E_i\\cap F_i) | E_1\\cup E_2 ] = \\frac{ P[F_1 \\cap E_1] + P[F_2 \\cap E_2]}{P[E_1]+P[E_2]} \\end{equation} \\] <p>Note however that \\(T_1,T_2\\) are not always observable. To relate with the observable \\(Y\\) and \\(\\Delta\\) defined earlier, observe that </p> \\[ \\begin{align*} E_1 =&amp; \\{T_1&lt;T_2\\} \\\\ =&amp;\\{\\Delta_1=1, \\Delta_2=1, Y_1&lt;Y_2\\} \\\\ &amp;\\cup \\{\\Delta_1=1, \\Delta_2=0, Y_1&lt;T_2\\} \\\\  &amp;\\cup \\{\\Delta_1=0, \\Delta_2=1, T_1&lt;Y_2\\} \\\\ &amp;\\cup \\{\\Delta_1=0, \\Delta_2=0, T_1&lt;T_2\\} \\\\ \\supset&amp; \\{\\Delta_1 = 1, Y_1&lt;Y_2\\} =:D_1 \\end{align*}  \\] <p>and similarly \\(E_2\\supset \\{\\Delta_2 = 1, Y_1&gt;Y_2\\}=: D_2\\).</p> <p>The standard C-index of Harrell is not a consistent estimator of \\(\\eqref{e-cpop}\\), but rather one that estimates analogous  probability with \\(D_i\\) in place of \\(E_i\\). More concretely, it is the proportion of concordant pairs with the smaller observation being the event time (\\(\\Delta=1\\))  amongst all pairs with  with smaller one being the event time (aka the comparable pairs). One way to address the inconsistency is the CPE method<sup>5</sup> which is robost with  regard to how censoring is performed. Yet another proposal is the C-index IPCW<sup>6</sup> which is a weighted version of C-index using some consistent estimate of the survival function of the censoring distribution in the weights. </p> <ol> <li> <p>https://faculty.washington.edu/yenchic/short_note/note_hazard.pdf \u21a9\u21a9</p> </li> <li> <p>https://www.medicine.mcgill.ca/epidemiology/hanley/c626/cox_jrssB_1972_hi_res.pdf \u21a9</p> </li> <li> <p>https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf \u21a9</p> </li> <li> <p>https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture28.pdf \u21a9</p> </li> <li> <p>https://www.mskcc.org/sites/default/files/node/2246/documents/cpe.pdf \u21a9</p> </li> <li> <p>https://biostats.bepress.com/cgi/viewcontent.cgi?referer=&amp;httpsredir=1&amp;article=1108&amp;context=harvardbiostat \u21a9</p> </li> </ol>","tags":["quant methods"]},{"location":"2025/04/05/fork-merge-and-pr/","title":"Fork, merge and PR","text":"<p>This is a common workflow: </p> <ul> <li>fork a repo (<code>gh repo fork</code> with the gh cli fork the repo interactively)  </li> <li>work on some code then <code>git add</code>, <code>git commit</code> </li> <li>fetch upstream (<code>git remote add upstream REPO</code> then <code>git fetch upstream</code>)  </li> <li>merge upstream (<code>git merge</code> say <code>upstream/main</code>)   </li> <li>push back to the fork (<code>git push</code>)  </li> <li>ask for a pull request (<code>gh pr create</code> interactively)   </li> </ul> <p>Unfortunately the merge step often result in conflicts, and manual intervention is necessary. Fortunately, editors like vscode offer GUI to help visualize the process, making this step less of a pain (after messing up once/twice to understand the GUI).  </p> <p>Alternative workflow (after fetch upstream)</p> <ul> <li>... same until fetch</li> <li>rebase instead of merge: <code>git rebase upstream/main</code>  (then resolve conflicts, if any, commit by commit)</li> <li><code>git push --force-with-lease</code></li> <li>... PR as before</li> </ul> <p>To avoid fetch and rebase everytime, use </p> <pre><code>git config --global pull.rebase true &amp;&amp; git pull upstream main\n</code></pre>","tags":["dev tools"]},{"location":"2025/04/05/fork-merge-and-pr/#how-to-undo-changes","title":"How to undo changes","text":"<p>Conceptually, we may think of git as managing three copies of the same project, they are called </p> <ul> <li>working directory  </li> <li>index/staging area</li> <li>HEAD</li> </ul> <p>Initially they are all the same thing. After some edits, the working directory changes but the index and HEAD remain untouched.</p> <p>Once we run <code>git add</code>, the edits are synced to index so working directory and the index are the same thing. </p> <p>Once we run <code>git commit</code>, the edits are synced to the HEAD and we are once again in a state of all being equal. </p> <p>At any point in time of this process, we can undo changes. </p> <ul> <li><code>git restore .</code> removes modifications in the current directory that are not yet staged. </li> <li>Once staged (after <code>git add</code>), we can drop these changes using <code>git restore --staged .</code> </li> <li>Once commit, we have all three copies the same thing, but we can still drop the changes by restore the previous version of the HEAD (or any other version before) <code>git reset --soft HEAD~1</code> </li> </ul>","tags":["dev tools"]},{"location":"2025/04/05/fork-merge-and-pr/#what-happens-during-the-fetch-and-merge-process","title":"What happens during the fetch and merge process","text":"<p>Again conceptually, we may think of remote branch yet another copy of the same project. When we run <code>git fetch</code>, we sync the remote branch with the latest content in the remote repository.  Assuming we fetch remote content before edits, the working directory is same as index and HEAD, <code>git merge</code> would compare remote with local and merge them. It's similar if you decide to rebase. </p>","tags":["dev tools"]},{"location":"2025/04/06/polars-streaming-tricks/","title":"Polars streaming tricks","text":"<p>Background</p> <p>Polars' lazy execution model (see my previous post) allows query optimizer to \"rewrite\" the query to avoid loading all data into memory such as predicate/project pushdown, constant folding etc. However, the default execusion engine of the optimized query is still in-memory, i.e. loading all the data required in the optimized query into memory, therefore can be problematic if the required data is too large.  The streaming engine is an alternative.    </p> <p><code>polars</code> offers a convenient streaming capability via the <code>LazyFrame.collect(streaming=True)</code> method. Under the hood, this processes the large dataset in chunks, process them, cache the intermediate results and so on. </p> <p>A key limitation of this approach is that the final result (and possibly some intermediate results) must still fit entirely within the machine's RAM. When the final result itself is too large for RAM but all the intermediate results fit into RAM, <code>polars</code> provides the sink_* methods as an alternative. These methods allow to write the output directly to disk. Here is an example from the official documentation: <pre><code>lf = pl.scan_csv(\"my_dataset/*.csv\").filter(pl.all().is_not_null())\n\nlf.sink_parquet(\n    pl.PartitionMaxSize(\n        \"my_table_{part}.parquet\",  # {part} is replaced by partition number\n        max_size=512_000           # bytes\n    )\n)\n</code></pre> This code processes the LazyFrame and stores the results as a sequence of Parquet files on disk, ensuring no single file exceeds the max_size. Because the output is written incrementally to disk, this method effectively removes the RAM limitation for the final result size.</p> <p>One can use this pattern to break a complicated query into sequences of subquery-sink routines, which would remove the RAM constraint at the cost of longer query running time (round trips to disc is slower than operating in RAM).   </p> <p>Github issues tracker for the new streaming engine <sup>1</sup> can be used before a dedicated page on streaming engine functionalities/roadmap. In particular it is in the roadmap that the new streaming engine will support automatically writing to disc when intermediate results are too large, so the afore-mentioned pattern can be performed without user figuring out how to break queries themselves. </p> <ol> <li> <p>https://github.com/pola-rs/polars/issues/20947 \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/04/13/mortgate/","title":"Build a mortgate calculator with fasthtml","text":"<p>There are app frameworks out there for domain experts to easily get started with app buiding, e.g. <code>streamlit</code> for data scientists. </p> <p>A possible downside of these approaches is the lack of flexibility and scalability. <code>fasthtml</code> is a new python framework that is flexible and,  from what I read, pretty scalable too. It provides a unified python interface that maps 1-to-1 to HTTP, html, and htmx for interactivity. This guarantees flexibility without unnecessary abstractions and minimizes client side logic. Also the code often looks pretty neat, therefore temping for new users to get started by copy-pasting some basic examples from their repo. </p> <p>That said, if a new user does not have any knowledge of the web (eg http, how anchor/form tags work, client-server communication, htmx attributes), there are concepts to learn before they can make sense of the framework. But these concepts are worth learning anyway for someone building a web app, so the time invested in the learning is not wasted.  </p> <p>Here is a toy example.</p> <p></p> <p>The whole app is one python file with 20 lines of code (see below). Features highlighted: </p> <ul> <li>html tags are python object (title, input, button, div, details).</li> <li>router and function names map to  url and http methods (put, get ...).</li> <li>htmx for (server centric) interactivity, avoiding writing too much javascript (hx_include, hx_put) </li> <li>by default pico css is used for styling (it can be swapped by tailwind via <code>monsterui</code>, or customized css)</li> </ul> <pre><code>from fasthtml.common import *\n\napp, rt = fast_app(pico=True) # default True\n\n@rt(\"/\")\ndef get():\n    return Titled(\"mortgate calculator\", \n        Grid(\n            Input(type=\"number\", placeholder=\"Loan Amount\", id=\"P\"),\n            Input(type=\"number\", placeholder=\"Interest rate (per cent)\", id=\"r\"),\n            Input(type=\"number\", placeholder=\"Loan Term (months)\", id=\"n\"),\n            Button(\"Calculate\", target_id=\"result\", hx_put=\"/calculate\", hx_include=\"#P,#r,#n\"),),\n        Div(id=\"result\")\n    )\n\n@rt(\"/calculate\")\ndef put(P:float,r:float,n:int):\n    r= r/ 100 / 12\n    m = P*r*(1+r)**n / ((1+r)**n - 1)\n    return Details(\n        Summary(H3(f\"monthly payment {m:.2f}\")), \n        Ul(Li(f\"interest payment {P*r:.2f}\"), Li(f\"principle payment {m-P*r:.2f}\")),\n        open=True)\n\nserve()\n</code></pre> <p>No form element is used because any html element is allowed (via htmx) to trigger HTTP requests. The payload picks up the values of the ids mentioned in <code>hx_include</code> attribute (equivalent to using a form which wraps the inputs and button). </p>","tags":["dev tools"]},{"location":"2025/04/20/df-gen-polars/","title":"Generate test dataframe with <code>polars</code> (powered by <code>hypothesis</code>)","text":"<p>The sub-module <code>polars.testing.parametric</code> provide tools for generating fake data for testing purposes. Here is an example showing what can be done with just <code>dataframes</code> and <code>column</code> functions in this module <pre><code>import polars as pl\nfrom polars.testing.parametric import dataframes, column\n\ndef generate(size=5):\n    return dataframes(\n    [\n        column(\"id\", dtype=pl.UInt16, unique=True, allow_null=False), \n        column(\"value\", dtype=pl.Int16, allow_null=True), \n        column(\"cat\", dtype =pl.Enum(\"XYZ\"), allow_null=False)\n    ], \n    min_size=size, max_size=size)\n\noriginal = generate().example()\n</code></pre></p> <p>The output is random, i.e. evey call to the <code>example</code> method would generate a new dataframe with the prescribed characteristics (this method is for interactive use only). One can test their data pipelines on fake data with precise schema and simulated data quality deficiencies (eg null values, nan, inf, etc). </p> <p>For unittesting, here is an example from the offical docs</p> <pre><code>from hypothesis import given\n@given(df=dataframes(min_size=3, max_size=5))\ndef test_df_height(df: pl.DataFrame) -&gt; None:\n    assert 3 &lt;= df.height &lt;= 5\n</code></pre>","tags":["data engineering"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/","title":"LLM eval for my personal usage","text":"<p>As common evaluation metrics for LLM performance are being gamified, everyone starts to run some evaluation thingy most relevant for their own use case.</p> <p>In this post I keep track of queries that I asked multiple LLMs and record the winner who gave the best answer.  Here are the contesters: </p> contester stands for G Gemini D deepseek C ChatGPT X Grok <p>The reason for excluding Claude is personal: the last time I checked, it appears to be not as generous as other frontier labs to offer their best-ish model to free tier users. And now I have too many chatbot tabs to manage </p> <p>I include the date of the query to highlight the dynamic nature of these evals and help identify model versions if I/anyone wanted to. </p> <p>By default, I always enable the most powerful model available  to a free tier user (think and search for ChatGPT. Gemini 2.5 Pro etc in April 2025) for a \"fair\" comparison.</p> <p>This post is regularly updated. </p>","tags":["AI assistant"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#paste-image-to-markdown-20250424","title":"Paste image to markdown - 20250424","text":"<p>in vscode in WSL, paste image from clipboard to markdown in customized directory.</p> <p>C  wins. Other LLMs refer to some obsolete vscode extension which does not work. In think mode C started thinking about using the same extensions, but then made some search in vscode github project and elsewhere,  ended up finding  that this is a feature already built-in in vscode after v1.79 and provided the correct json for me to copy to user settings.   </p> <p>For example. Paste figures into the assets folder in the document directory (one that contains the markdown file) with the name of the image =  document name + time + '.png'</p> <pre><code>{\n    \"markdown.copyFiles.destination\": {\n            \"**/*.md\": \"${documentDirName}/assets/${documentBaseName}-${unixTime}.${fileExtName}\"\n        }\n}\n</code></pre>","tags":["AI assistant"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#search-the-url-of-a-file-in-github-20250501","title":"search the url of a file in github - 20250501","text":"<p>show me the link to the source code of the new polars streaming engine. this should be hosted in github</p> <p>G wins. C and X provide irrelevant links. D cannot search (functionality not working in the region where I am which is EU). </p>","tags":["AI assistant"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#writing-task-20250518","title":"Writing task - 20250518","text":"<p>when checking dataquality what are principles that are mutually exclusive so that I don't forget anaything. this project invovles many datasets forming a graph and I need to check indiviudal datasets as well as the way datasets are joined, transformed from one to another.</p> <p>I've gone through the drafting, criticizing, improving cycle a few times, with several llms doing part of the job. </p> <p>The topic is kinda common and all llms have seen articles, books, blogposts about it already, in one way or another. </p> <p>Both D and X followed the prompt and provided solid ideas. G raised some valid points questioning the prompt itself (like DQ checks are often interconnected). C revised the answer G gave that improved readability. </p> <p>My votes go to G and C. The result is here.</p>","tags":["AI assistant"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/","title":"Puzzling query optimization behaviour and dtypes shrinking in polars","text":"<p>The official blog<sup>1</sup> of <code>polars</code> shared a recent case study that's part counterintuitive, part educational. </p>","tags":["data engineering"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/#counterintuitive-bits","title":"Counterintuitive bits","text":"<p>The authors found that for large DataFrames (~400M rows), using lazy execution with the streaming engine for a massive query plan -- built via the <code>pipe</code> method --  was not the optimal approach. This is basically saying: the query engine, one of polars' key selling point, struggles to optimize giant plans effectively.</p> <p>The solution they found is to use lazy + streaming only on very expensive joins, and run  other parts of the query plan  eagerly. </p> Update on 2025-06-15 <p>The new streaming engine achieves some impressive benchmark results per official blog in May 2025, potentially changing the verdict of this case study if it were conducted now. </p>","tags":["data engineering"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/#shrink-dtype","title":"Shrink dtype","text":"<p>Feels obvious, but it's a solid reminder: why would you use int64 if all your values never go above a few millions? Use  <code>shrink_dtype</code> on expressions/series and <code>shrink_to_fit</code> on dataframe to downcast dtypes to just what's needed. </p> <p>Watch out when joining on downcast columns - dtypes need to match. </p> <ol> <li> <p>https://pola.rs/posts/case-metro-digital/ \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/","title":"Static site generator using fasthtml","text":"","tags":["dev tools"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/#genesis","title":"genesis","text":"<p>Currently I use mkdocs (with the <code>material</code> theme) for both blog and documentation. </p> <p>The thing I like the most about material for mkdocs is that it is easy to get started. Copy mkdocs.yml from your favorite projects which uses material for mkdocs (polars, uv, fastapi in my case) and start writing your md files. Essentially a no code solution for writing static contents.</p> <p>The ease of use comes at a price though. It may be hard to tweak one little thing you want which is not supported in the yaml config file or/and mkdocs plugin. To do that, you have to dig into the template, the partials and learn Jinja2 template language if you haven't already. It's too much.</p> <p>Practically all SSG out there seem to use the same components: a markdown parser, a template system with Jinja2-like language and html/css files. </p> <p>It strikes me that fasthtml can do all that with just python. I am not interested in something achieving feature parity with mkdocs, hugo, jekyll, but rather a minimal system that I have full control and maximum customization for SSG use case. </p>","tags":["dev tools"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/#archetecture","title":"archetecture","text":"<p>Write a function that chains fasttags leaving as variable the things I want to inject into the function from a parsed markdown file (metadata, content etc). This function is the template. All styles and scripts are embedded into the function.  Markdown files can be rendered with mistletoe (which come with monsterui,  fasthtml's UI components library).  </p> <p>for example</p> <pre><code>import yaml\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom fasthtml.common import *\nfrom monsterui.all import * # include render_md\n\n\n@dataclass\nclass Page:\n    \"\"\"from markdown to html\"\"\"\n    path: Path\n    content: str\n    meta: dict\n\n    @classmethod\n    def from_file(cls, path: Path):\n        \"\"\"Load a page from a file, parsing front matter and content.\"\"\"\n        text = path.read_text()\n        if \"---\" in text:\n            meta_str, content = text.split(\"---\", 2)[1:]\n            meta = yaml.safe_load(meta_str)\n        else:\n            meta, content = {}, text\n        return cls(path, content, meta)\n\n    def render(self) -&gt; str:\n        \"\"\"Plug page data into a template.\"\"\"        \n        content = render_md(self.content)\n        title=  self.meta.get(\"title\", \"Untitled\"),\n        return base_template(title, content, self.meta)\n\ndef base_template(title, content, meta, hdrs=Theme.blue.headers(daisy=True, highlightjs=True)) -&gt; str:\n    \"\"\"Core layout template using MonsterUI components\"\"\"\n    return to_xml(Html(\n            Head(\n                Title(title),\n                Meta(charset=\"utf-8\"),\n                Meta(name=\"viewport\", content=\"width=device-width, initial-scale=1.0\"),\n                *hdrs  # Theme is a list\n            ),\n            Body(\n                Container(\n                    NavBar(\n                        A(\"Home\", href=\"/\"),\n                        A(\"Blog\", href=\"/blog\"),\n                        brand=H2(\"My SSG\"),\n                    ),\n                    Main(\n                        Article(\n                            Header(\n                                H1(title),\n                                Div(\n                                    Small(meta.get('date',None)),\n                                    cls=TextPresets.muted_sm\n                                ) if meta.get('date') else None,\n                            ),\n                            Div(Safe(content)),  # Markdown content\n                        ),\n                        cls=\"max-w-3xl mx-auto\"\n                    ),\n                    Footer(\n                        P(f\"\u00a9 {datetime.now().year} My Static Site\", cls=TextPresets.muted_sm),\n                        cls=\"mt-8 text-center\")))))\n</code></pre> <p>Now I can do this to generate one file (or write a loop to handle a directory of md docs). </p> <pre><code>Path(\"index.html\").write_text(Page.from_file(Path('README.md')).render())\n</code></pre>","tags":["dev tools"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/","title":"A dimensional approach to data quality principles","text":"<p>courtesy: Gemini, ChatGPT, DeepSeek, Grok</p> <p>Data quality checks often overlap\u2014rather than forcing each principle into its own silo, it\u2019s more practical to group them into logical \u201cdimensions.\u201d This approach helps teams understand where issues live, how they relate, and how to fix them.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#i-intrinsic-data-quality-characteristics-of-the-data","title":"I. Intrinsic Data Quality (Characteristics of the Data)","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-accuracy","title":"1. Accuracy","text":"<p>What it is: How well data reflects the real world. Why it matters: Bad accuracy means bad decisions. Example: Customer address matches actual GPS coordinates.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-completeness","title":"2. Completeness","text":"<p>What it is: All required data is present. - Schema completeness: All expected columns exist. - Column completeness (density): % of non-null values in a column. - Population completeness: All expected rows exist. Example: Every customer record has an <code>email_address</code>.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-consistency","title":"3. Consistency","text":"<p>What it is: No contradictions within or across datasets. - Intra-record: Fields inside one record don\u2019t clash (e.g., age vs. date_of_birth). - Inter-record: Same entity looks the same over time (e.g., name spelling). - Cross-dataset: Shared fields use the same units and formats (e.g., revenue in USD). Example: Product codes use the same format in sales, inventory, and marketing.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-timeliness","title":"4. Timeliness","text":"<p>What it is: Data is fresh enough for its purpose. - Currency: How recent it is. - Frequency: How often it\u2019s updated. - Punctuality: It arrives when you expect it. Example: Stock prices updated every second for trading dashboards.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#5-uniqueness","title":"5. Uniqueness","text":"<p>What it is: No duplicates of the same real-world entity. Example: Each customer has exactly one <code>customer_id</code>.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#6-validity","title":"6. Validity","text":"<p>What it is: Data follows rules, formats, and logical constraints. - Syntactic: Correct type/format (e.g., dates as YYYY\u2011MM\u2011DD). - Semantic: Meets business logic (e.g., order_date \u2264 ship_date). - Referential integrity: Foreign keys point to real parent records. Example: Emails match a regex pattern; <code>order_status</code> is one of \u201cPending,\u201d \u201cShipped,\u201d \u201cDelivered.\u201d</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#ii-data-management-operational-excellence","title":"II. Data Management &amp; Operational Excellence","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-transformation-integrity","title":"1. Transformation Integrity","text":"<p>What it is: ETL/ELT logic is correct and preserves quality. Example: <code>profit = revenue \u2212 cost</code> is implemented correctly downstream.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-metadata-lineage","title":"2. Metadata &amp; Lineage","text":"<p>What it is: You know what each field means and where data came from. - Schemas match docs. - Business rules documented. - Data dictionaries/glossaries exist. - Lineage tracking end-to-end. Example: A data catalog shows original source files, transformations, and load targets.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-performance-scalability","title":"3. Performance &amp; Scalability","text":"<p>What it is: Systems handle current workloads and growth. Example: Indexed tables for fast queries; pipelines auto-scale on spikes.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-resilience-error-handling","title":"4. Resilience &amp; Error Handling","text":"<p>What it is: Systems detect, retry, and recover from failures. Example: ETL jobs retry on transient errors; backups restore lost data.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#5-security-privacy","title":"5. Security &amp; Privacy","text":"<p>What it is: Data is protected and handled per regulations. Example: PII is encrypted in transit and at rest; RBAC enforces least privilege.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#iii-governance-risk-compliance","title":"III. Governance, Risk &amp; Compliance","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-stewardship-ownership","title":"1. Stewardship &amp; Ownership","text":"<p>What it is: Clear accountability for each data domain. Example: Finance owns transaction data; Marketing owns campaign metrics.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-regulatory-compliance","title":"2. Regulatory Compliance","text":"<p>What it is: Processes meet GDPR, HIPAA, SOX, etc. Example: Customer opt\u2011outs are enforced in all systems.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-auditability-reproducibility","title":"3. Auditability &amp; Reproducibility","text":"<p>What it is: You can trace every change and rerun analyses. Example: Versioned datasets and code; audit logs for data changes.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-lifecycle-management","title":"4. Lifecycle Management","text":"<p>What it is: Data is created, stored, archived, and deleted per policy. Example: Transaction logs archived after 7 years, purged at 10.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#iv-user-centric-value-realization","title":"IV. User-Centric &amp; Value Realization","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-relevance","title":"1. Relevance","text":"<p>What it is: Data fits the users\u2019 needs and use cases. Example: Sales dashboard shows KPIs the team actually uses.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-usability-interpretability","title":"2. Usability &amp; Interpretability","text":"<p>What it is: Data is easy to find, understand, and use. - Clarity: Human\u2011readable names and definitions. - Accessibility: Delivered in formats users can work with. - Context: Code tables, descriptions, and examples provided. Example: Dashboards with clear labels and linked data definitions.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#v-continuous-improvement-monitoring","title":"V. Continuous Improvement &amp; Monitoring","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-monitoring-measurement","title":"1. Monitoring &amp; Measurement","text":"<p>What it is: Automated checks track data quality over time. Example: Daily jobs that flag spikes in null rates or invalid formats.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-issue-management-remediation","title":"2. Issue Management &amp; Remediation","text":"<p>What it is: Process to log, prioritize, and fix data issues. Example: Tickets in a bug tracker for data errors, complete with severity levels and SLAs.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#putting-it-all-together-a-sample-workflow","title":"Putting It All Together: A Sample Workflow","text":"<p>Define requirements &amp; metrics </p> <ul> <li>Talk to users: What does \u201cfit for purpose\u201d look like?  </li> <li>Set thresholds for accuracy, completeness, timeliness, etc.</li> </ul> <p>Profile &amp; assess data </p> <ul> <li>Run accuracy checks, null counts, format validators, duplication scans.</li> </ul> <p>Evaluate operations </p> <ul> <li>Audit ETL logic, metadata, system health, security controls.</li> </ul> <p>Review governance </p> <ul> <li>Confirm ownership, compliance controls, audit trails, retention policies.</li> </ul> <p>Establish monitoring &amp; fixes </p> <ul> <li>Schedule automated checks and ticketing for quick resolution.</li> </ul> <p>By viewing data quality through these dimensions, you get a holistic framework that\u2019s both comprehensive and practical\u2014nothing slips through the cracks, and every team knows where they fit in.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/","title":"Intro to Snowpark API","text":"<p>Polars users might find the Snowpark/PySpark API verbose. But the high-level abstractions roughly map: <code>snowpark.Column</code> is like <code>polars.Expr</code>, and <code>snowpark.DataFrame</code> is like <code>polars.LazyFrame</code>.</p> <p>Caveat: the list of methods on <code>Column</code> is much smaller than on <code>Expr</code>, and there\u2019s no clear namespace separating methods by dtype. If you don\u2019t see a method for your use case, check the gigantic list under <code>snowpark.functions</code>.</p> <p>Let\u2019s explore.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#first-imports","title":"First, imports","text":"<pre><code>from snowflake import snowpark as sp\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#column-class","title":"<code>Column</code> class","text":"<p>Accessible via <code>F.col</code>, e.g. <code>lf.select(F.col(\"A\"))</code>.</p> <p>A few notable methods on <code>Column</code> instances:</p> <ul> <li><code>cast</code></li> <li><code>name</code> (alias)</li> <li><code>asc</code> / <code>desc</code></li> <li><code>is_null</code></li> <li><code>is_in</code></li> <li><code>over</code></li> <li><code>substr</code></li> </ul> <p>Unlike Polars, there\u2019s no method namespace separation by dtype. Still, <code>Column</code> feels a lot like <code>polars.Expr</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#f-namespace-functions","title":"<code>F</code> namespace (functions)","text":"<p>The <code>F</code> namespace provides many ways to manipulate <code>Column</code>s.</p> <p>Note: all functions return a <code>Column</code>, not just <code>F.col</code>, e.g.</p> <ul> <li><code>F.concat</code></li> <li><code>F.contains</code></li> <li><code>F.array_size</code></li> <li><code>F.year</code></li> <li><code>F.when(...).otherwise(...)</code></li> <li><code>F.explode</code> and <code>F.flatten</code></li> </ul> <p>Again, no subnamespaces by dtype \u2014 everything is dumped into <code>functions</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#window-class","title":"<code>Window</code> class","text":"<p>Provides a bunch of class methods:</p> <ul> <li><code>Window.partition_by</code></li> <li><code>Window.order_by</code></li> <li><code>Window.range_between</code> (time or int)</li> </ul> <p>And constants like:</p> <ul> <li><code>Window.CURRENT_ROW</code></li> </ul> <p>used as an argument to the <code>over</code> method.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#t-types","title":"<code>T</code> (types)","text":"<p>For type hints and casting, use:</p> <ul> <li><code>T.ArrayType</code></li> <li><code>T.DateType</code></li> <li><code>T.StructType</code></li> </ul> <p>Caveat: these are callables \u2014 instantiate them like <code>T.DateType()</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#dataframe-class","title":"<code>DataFrame</code> class","text":"<p>It\u2019s called a DataFrame, but really behaves like a LazyFrame \u2014 call <code>.collect()</code> to materialize.</p> <p>Notable methods (all return a new DataFrame):</p> <ul> <li><code>select</code></li> <li><code>filter</code></li> <li><code>pivot</code> / <code>unpivot</code></li> <li><code>join</code></li> <li><code>union</code></li> <li><code>with_columns</code> (takes <code>list[str]</code>, <code>list[Column]</code>)</li> <li><code>with_column</code></li> <li><code>distinct</code></li> <li><code>fillna</code></li> </ul> <p>and the <code>.columns</code> attribute.</p> <p>Note: after joining, columns with the same name need to be aliased explicitly to save/view the joined table. e.g. t1.col.alias(\"c1\"), t2.col.alias(\"c2\").  </p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#io","title":"IO","text":"<p>Ways to read data from snowflake:</p> <ul> <li><code>session.table(\"TABLE_NAME\")</code> </li> <li><code>session.sql(\"SELECT ...\")</code> </li> <li><code>session.read.option(\"infer_schema\", False).parquet(\"@mystage/test.parquet\")</code>  Multiple file formats are supported, simply replace parquet by the file extension e.g. csv</li> </ul> <p>Ways to push in-memory data to snowflake: </p> <ul> <li><code>session.create_dataframe(df: pd.DataFrame).write.format(...).save(...)</code></li> <li><code>session.create_dataframe(df: list[tuple|Row]).write.mode(\"overwrite\").save_as_table(\"my_table\", table_type=\"temporary\")</code></li> </ul>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#row-class","title":"<code>Row</code> class","text":"<p>You get a list of Rows after <code>collect()</code> The <code>.as_dict()</code> method on Row makes it easy to interoperate with Polars: just pass a list of dicts to construct a Polars DataFrame.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#testing","title":"Testing","text":"<ul> <li><code>sp.testing.assert_dataframe_equal</code></li> </ul>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#horizontal-ops-in-snowpark","title":"Horizontal ops in snowpark","text":"<p>Horizontal ops such as horizontal any (think <code>np.any(..., axis=1)</code>) can be achieved with a chain of logical OR e.g.  <code>(... | ... | ... )</code> or by using <code>F.when().otherwise()</code>. When the number of conditions/columns increases, I would like to use something similar to polars </p> <pre><code>import polars as pl\nlf.select(pl.any_horizontal(col_names)) # col_names: list[str]\n</code></pre> <p>or the general purpose reduce/fold function for horizontal ops in polars. </p> <pre><code>lf.select(pl.reduce(lambda a,b:a|b, exprs = pl.col(col_names)))\n</code></pre> <p>In snowpark there is no any_horizontal, nor reduce function.  But one can use python <code>functools.reduce</code>.</p> <pre><code>from functools import reduce\n\nany_expr = reduce(lambda a, b: a | b, map(F.col, col_names))\nlf.select(any_expr)\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#exception","title":"Exception","text":"<p>The most common is <code>sp.exceptions.SnowparkSQLException</code>, corresponding to 13xx SQL error.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#null-handling","title":"NULL handling","text":"<p>In aggregate functions like count, sum etc, NULL values are ignored. If all values in a column are NULL, the aggregate function acting on the column returns NULL.</p> <p>In comparison logic (&gt;, &lt;, &lt;&gt;, =, &gt;=, &lt;=) the result of NULL vs any value is NULL. Therefore in filters, NULL needs to be explicitly catched by the clause <code>A IS NULL</code> or <code>A IS NOT NULL</code>, for otherwise the filter <code>where A &lt; 1</code> excludes all the records where A is NULL.    Several functions are bound to NULL handling e.g. coalesce, ifnull, nullif, equalnull.</p>","tags":["data engineering"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/","title":"Repurposing gitHub for mathematical research","text":"<p>courtesy: Claude</p> <p>Marin project triggers my interest to repurpose github as a tool for open and reproducible mathematical research. Below is an example of how this can work, mostly one-shot generated by Claude.  </p>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#basic-repository-structure","title":"Basic Repository Structure","text":"<pre><code>conjecture-name/\n\u251c\u2500\u2500 README.md         # Overview, current status, and roadmap\n\u251c\u2500\u2500 conjecture.md     # Formal statement and background\n\u251c\u2500\u2500 proofs/           # Your proof attempts and progress\n\u251c\u2500\u2500 simulations/      # Code for experimental verification\n\u2514\u2500\u2500 notes/            # Working thoughts and observations\n</code></pre>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#simple-workflow","title":"Simple Workflow","text":"","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#1-setup-problem-definition","title":"1. Setup &amp; Problem Definition","text":"<ul> <li>Create the repository with a clear README explaining your goal</li> <li>Write <code>conjecture.md</code> with the formal statement, notation, and known related results</li> <li>Set up an initial project board with basic columns: \"Ideas\", \"In Progress\", \"Needs Review\", \"Completed\"</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#2-working-process-using-issues","title":"2. Working Process Using Issues","text":"<ul> <li>Create issues for specific approaches: \"Approach via induction\", \"Probabilistic method attempt\"</li> <li>Create issues for subproblems: \"Prove special case when n=2\", \"Find counterexample for condition X\"</li> <li>Use issue descriptions to outline your thinking process</li> <li>Reference mathematical literature with links or DOIs</li> <li>Close issues when an approach proves unsuccessful (with notes on why)</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#3-exploration-via-branches","title":"3. Exploration via Branches","text":"<ul> <li>Create a new branch for each significant approach (<code>inductive-approach</code>, <code>computational-verification</code>)</li> <li>Commit frequently with descriptive messages that explain your mathematical thinking</li> <li>Use commit messages to document insights: \"Realized lemma fails when x approaches infinity\"</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#4-documentation-using-pull-requests","title":"4. Documentation Using Pull Requests","text":"<p>When you make meaningful progress: - Open a PR from your working branch to main - Write a detailed description of your findings - Self-review your work by reading through changes with fresh eyes - Merge when you're confident in the correctness</p>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#5-simulation-verification","title":"5. Simulation &amp; Verification","text":"<ul> <li>Place code in the <code>simulations/</code> directory</li> <li>Document computational experiments in PR descriptions</li> <li>Link simulation results to theoretical approaches</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#6-progress-tracking","title":"6. Progress Tracking","text":"<ul> <li>Use GitHub milestones to mark significant achievements (\"Special case proven\", \"Main lemma established\")</li> <li>Update the README regularly with current status</li> <li>Pin important issues that represent active lines of inquiry</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#key-github-features-to-leverage","title":"Key GitHub Features to Leverage","text":"","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#issues-use-as-mathematical-questions-approaches-or-subproblems","title":"Issues - Use as mathematical questions, approaches, or subproblems","text":"<ul> <li>Add labels like \"promising\", \"stuck\", \"counterexample\"</li> <li>Reference equations with LaTeX in Markdown: <code>$E = mc^2$</code></li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#project-board-visual-organization-of-your-research-flow","title":"Project Board - Visual organization of your research flow","text":"<ul> <li>Track which approaches are active</li> <li>See what needs verification</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#pr-process-even-as-a-solo-researcher-use-prs-as-checkpoints","title":"PR Process - Even as a solo researcher, use PRs as checkpoints","text":"<ul> <li>Force yourself to clearly articulate progress</li> <li>Create a record of major developments</li> <li>Serve as a self-review mechanism</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#discussions-for-longer-form-mathematical-exploration","title":"Discussions - For longer-form mathematical exploration","text":"<ul> <li>Work through complex ideas</li> <li>Record insights that don't fit elsewhere</li> </ul>","tags":["research"]},{"location":"2025/05/25/gpu-puzzles-annotated/","title":"GPU puzzles annotated","text":"<p>Modular's adaptation of CUDA puzzles (by Sasha Rush) in mojo, solved and annotated below.</p>","tags":["low latency programming"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-1","title":"Puzzle 1","text":"<p>Notes:</p> <ul> <li>be aware of host and device (CPU and GPU), sync and async</li> <li>enqueue operations in GPU stream, execute them async</li> </ul> <pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = SIZE\nalias dtype = DType.float32\n\nfn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):\n    i = thread_idx.x\n    out[i] = 10 + a[i]\n\ndef main():\n    with DeviceContext() as ctx:\n        print(ctx.api()) # cuda\n        out = ctx.enqueue_create_buffer[dtype](SIZE) # gpu async \n        out = out.enqueue_fill(0) # gpu async \n        a = ctx.enqueue_create_buffer[dtype](SIZE)\n        a = a.enqueue_fill(0)\n        with a.map_to_host() as a_host: # sync at mapping to ensure buffer is created\n            for i in range(SIZE):\n                a_host[i] = i\n\n        ctx.enqueue_function[add_10](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        ) # gpu async\n\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE)\n        expected = expected.enqueue_fill(0) # gpu async\n\n        ctx.synchronize() # code would fail if sync after modifying expected i.e. first modify then fill 0 at sync time. \n\n        for i in range(SIZE): \n            expected[i] = i + 10\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-2","title":"Puzzle 2","text":"<pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = SIZE\nalias dtype = DType.float32\n\nfn add(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    b: UnsafePointer[Scalar[dtype]],\n):\n    i = thread_idx.x # 1d thread index map on 1d array data \n    out[i] = a[i] + b[i]\n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0) # size of the input data is conveniently block_size\n        b = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0) \n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host, b.map_to_host() as b_host:\n            for i in range(SIZE):\n                a_host[i] = i\n                b_host[i] = i\n                expected[i] = a_host[i] + b_host[i]\n\n        ctx.enqueue_function[add](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            b.unsafe_ptr(),\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-3","title":"Puzzle 3","text":"<pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (8, 1) # more threads than data size\nalias dtype = DType.float32\n\nfn add_10_guard(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    i = thread_idx.x\n    if i&lt;size: # avoid out of bounds \n        out[i] = a[i] + 10.0 \n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(0)\n\n        with a.map_to_host() as a_host:\n            for i in range(SIZE):\n                a_host[i] = i\n                expected[i] = i + 10\n\n        ctx.enqueue_function[add_10_guard](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-4","title":"Puzzle 4","text":"<p>with pointer</p> <pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 2\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (3, 3)\nalias dtype = DType.float32\n\n#  2d block, 1d input, 1d output\nfn add_10_2d(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    row = thread_idx.y\n    col = thread_idx.x\n    if row &lt; size and col&lt;size: # prevent out of bounds\n        out[row * size + col] = a[row * size + col] + 10\n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        expected = ctx.enqueue_create_host_buffer[dtype](\n            SIZE * SIZE\n        ).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host:\n            # row-major\n            for i in range(SIZE):\n                for j in range(SIZE):\n                    a_host[i * SIZE + j] = i * SIZE + j\n                    expected[i * SIZE + j] = a_host[i * SIZE + j] + 10\n\n        ctx.enqueue_function[add_10_2d](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                for j in range(SIZE):\n                    assert_equal(out_host[i * SIZE + j], expected[i * SIZE + j])\n</code></pre> <p>with <code>LayoutTensor</code></p> <pre><code>from gpu import thread_idx, block_dim, block_idx\nfrom gpu.host import DeviceContext\nfrom layout import Layout, LayoutTensor\nfrom testing import assert_equal\n\nalias SIZE = 2\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (3, 3)\nalias dtype = DType.float32\nalias layout = Layout.row_major(SIZE, SIZE)\n\n# 2d block, 2d input, 2d output\nfn add_10_2d(\n    out: LayoutTensor[mut=True, dtype, layout],\n    a: LayoutTensor[mut=True, dtype, layout],\n    size: Int,\n):\n    row = thread_idx.y\n    col = thread_idx.x\n    if row &lt; size and col &lt; size: \n        out[row, col] = a[row, col] + 10 # index arithmetic is handled by LayoutTensor\n\n\ndef main():\n    with DeviceContext() as ctx:\n        out_buf = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        out_tensor = LayoutTensor[mut=True, dtype, layout](out_buf.unsafe_ptr()) # NOT created with ctx\n        print(\"out shape:\", out_tensor.shape[0](), \"x\", out_tensor.shape[1]())\n\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n\n        a = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host:\n            for i in range(SIZE * SIZE):\n                a_host[i] = i\n                expected[i] = a_host[i] + 10\n\n        a_tensor = LayoutTensor[mut=True, dtype, layout](a.unsafe_ptr())\n\n        ctx.enqueue_function[add_10_2d](\n            out_tensor,\n            a_tensor,\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out_buf.map_to_host() as out_buf_host:\n            print(\"out:\", out_buf_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE * SIZE):\n                assert_equal(out_buf_host[i], expected[i])\n</code></pre>","tags":["low latency programming"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/","title":"Working with large datasets in Snowflake","text":"<p>This post covers how to work with large datasets in Snowflake e.g. 100+ GB, billions of rows. You can't really do this kind of stuff on your personal laptop (mine has 6 cores and 16GB RAM), so most people turn to the cloud for this workload.</p> <p>We'll go over some practical tips, using the Snowpark API version 1.32 to illustrate.</p> <pre><code>from snowflake import snowpark as sp\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#choose-the-right-warehouse","title":"Choose the Right Warehouse","text":"<p>There\u2019s no one-size-fits-all. Larger warehouses cost more, so don\u2019t scale up unless you actually need the extra horsepower. Match the warehouse size to the workload. Start small and scale up only if performance is a problem.</p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#cache-the-results","title":"Cache the Results","text":"<p>Snowpark DataFrames are lazily evaluated. That means if you reuse part of a query in multiple branches, Snowflake will recompute that shared part unless you cache it explicitly.</p> <p>Example:</p> <pre><code>a = session.table(\"...\")\nquery = a.group_by(\"A\").agg(...)  # simulate expensive intermediate step, and this line won't execute anything yet due to lazy evaluation\n\n# branching out\nres1 = query.filter(...).collect()\nres2 = query.select(...).collect()\n</code></pre> <p>Both <code>res1</code> and <code>res2</code> will trigger the same expensive computation. To avoid that, materialize the intermediate result:</p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#temp-table-session-scoped","title":"Temp Table (session scoped)","text":"<pre><code>query.write.mode(\"overwrite\").save_as_table(\"temp_table\", table_type=\"temporary\")\nlf = session.table(\"temp_table\")\n</code></pre> <p>The result is materialized and named so that it can be queried from SQL.  Use this if you want more control e.g. how to cache. </p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#temp-dataframesession-scoped","title":"Temp DataFrame(session scoped)","text":"<pre><code>c = query.cache_result()\n</code></pre> <p>The result is again materialized but no explict table name. Suitable for pure snowpark routines. Use this over if you want simplicity.  </p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#materialized-view-persistent-auto-refreshed-costs-money","title":"Materialized View (persistent, auto-refreshed, costs money)","text":"<pre><code>session.sql(\"\"\"\n    create materialized view my_view as\n    select id, count(*)\n    from my_table\n    group by id\n\"\"\").collect()\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#parquet-file-in-stage","title":"Parquet File (in stage)","text":"<pre><code>path = f\"{session.get_session_stage()}/f.parquet\"\na.group_by(\"A\").agg(...).write.format(\"parquet\").save(path)\nlf = sp.DataFrameReader.parquet(path)\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#predicate-and-projection-pushdown","title":"Predicate and Projection Pushdown","text":"<p>\"Predicate\" = filter. \"Projection\" = column selection.</p> <p>Snowflake tries to optimize automatically, but it only knows what you tell it\u2014and lazy evaluation doesn't help. If you're building up a complex query, be explicit:</p> <ul> <li>Filter early, especially before joins</li> <li>Drop columns you don\u2019t need</li> <li>Use <code>DataFrame.explain()</code> to inspect the query plan</li> </ul>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#monitoring","title":"Monitoring","text":"<p>Use <code>QUERY_HISTORY</code> to monitor performance. Look at:</p> <ul> <li><code>bytes_scanned</code></li> <li><code>partitions_scanned</code></li> </ul> <p>Lower is better. If you're scanning too much data, it's a sign that pushdowns or clustering could be improved.</p>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/","title":"duckdb basics","text":"","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#in-memory","title":"in-memory","text":"<p>duckdb can query files and dataframes</p> <pre><code>import duckdb\nimport polars as pl\n\nduckdb.sql(\"SELECT * FROM 'example.csv'\")     # directly query a CSV file\nduckdb.sql(\"SELECT * FROM 'example.parquet'\") # directly query a Parquet file\nduckdb.sql(\"SELECT * FROM 'example.json'\")    # directly query a JSON file\n\npolars_df = pl.DataFrame({\"a\": [42]})\nduckdb.sql(\"SELECT * FROM polars_df\")\n</code></pre> <p>What's returned is called Relation. duckdb can turn Relation back to dataframes/disk.</p> <pre><code>duckdb.sql(\"SELECT 42\").fetchall()   # Python objects\nduckdb.sql(\"SELECT 42\").df()         # Pandas DataFrame\nduckdb.sql(\"SELECT 42\").pl()         # Polars DataFrame\nduckdb.sql(\"SELECT 42\").arrow()      # Arrow Table\nduckdb.sql(\"SELECT 42\").fetchnumpy() # NumPy Arrays\n\nduckdb.sql(\"SELECT 42\").write_parquet(\"out.parquet\") # Write to a Parquet file\nduckdb.sql(\"SELECT 42\").write_csv(\"out.csv\")         # Write to a CSV file\nduckdb.sql(\"COPY (SELECT 42) TO 'out.parquet'\")      # Copy to a Parquet file\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#connection","title":"connection","text":"<pre><code>import duckdb\n\nwith duckdb.connect(\"file.db\") as con:\n    con.sql(\"CREATE TABLE test (i INTEGER)\")\n    con.sql(\"INSERT INTO test VALUES (42)\")\n    con.table(\"test\").show()\n    # the context manager closes the connection automatically\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#extensions","title":"extensions","text":"<p>to interoperate with sqlite </p> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.install_extension(\"sqlite\")\ncon.load_extension(\"sqlite\")\n</code></pre> <p>core extensions e.g. ducklake, azure, excel ...</p>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#example","title":"example","text":"<p>in this example, we load csv files into a duckdb file, copy a table from an existing sqlite file into it using the  sqlite extension, then copy all tables in the duckdb file into a new sqlite file. </p> <p>the goal of this script is to use duckdb's schema inference to harmonize csv date format for sqlite to consume. The reason for the conversion to sqlite is that other parts of the codebase assumes sqlite db in the backend.   </p> <pre><code># script to load CSV files into DuckDB and convert to SQLite; use duckdb's schema inference.\n\nimport os\nimport glob\nimport re\nimport sqlite3\nimport duckdb\n\noriginal = \"og.db\" # assume there is a table called table_metadata in the db\n\ndef parse_filename(filename_string):\n    \"\"\"\n    remove leading numbers and dashes, replace non-alphanumeric characters by underscores.\n    \"\"\"\n    parsed_name = re.sub(r'^\\d+-', '', filename_string)\n    cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '_', parsed_name)\n    return cleaned_name\n\ndef load_csv_files_as_separate_tables(csv_folder_path, db_file_path):\n    \"\"\"\n    Loads each CSV file from a specified folder into its own separate table\n    in a DuckDB database file. The table name will be derived from the CSV filename.\n\n    Args:\n        csv_folder_path (str): The path to the folder containing the CSV files.\n        db_file_path (str): The path where the DuckDB database file will be created/stored.\n    \"\"\"\n    # Connect to DuckDB. If the file doesn't exist, it will be created.\n    with duckdb.connect(database=db_file_path, read_only=False) as con:\n        csv_files = glob.glob(os.path.join(csv_folder_path,'*.csv'))\n        if not csv_files: return\n        for csv_path in csv_files:\n            base_filename = os.path.splitext(os.path.basename(csv_path))[0]\n            table_name = parse_filename(base_filename)            \n            # auto_detect=TRUE is very helpful for inferring schema\n            con.execute(f\"\"\"\n                CREATE OR REPLACE TABLE \"{table_name}\" AS\n                SELECT * FROM read_csv('{csv_path}', auto_detect=TRUE);\n            \"\"\")\n\n\ndef convert_duckdb_to_sqlite(duckdb_file, sqlite_file):\n    \"\"\"\n    Converts a DuckDB database to a SQLite database by copying all tables.\n\n    Args:\n        duckdb_file (str): Path to the existing DuckDB database file.\n        sqlite_file (str): Path where the new SQLite database file will be created.\n    \"\"\"\n    con_duck = duckdb.connect(database=duckdb_file)\n    con_duck.execute(\"INSTALL sqlite; LOAD sqlite;\")\n    con_duck.execute(f\"ATTACH '{sqlite_file}' AS new_sqlite_db (TYPE sqlite);\") # empty \n    con_duck.execute(f\"ATTACH '{original}' AS original (TYPE sqlite);\") # contains table_metadata\n    con_duck.sql(\"\"\"CREATE OR REPLACE TABLE table_metadata as (select * from original.table_metadata)\"\"\")\n    duckdb_tables = con_duck.execute(\"SHOW TABLES;\").fetchall() # should see table_metadata in it\n\n    for table_name_tuple in duckdb_tables:\n        table_name = table_name_tuple[0]\n        con_duck.execute(f\"CREATE TABLE new_sqlite_db.\\\"{table_name}\\\" AS SELECT * FROM \\\"{table_name}\\\";\")\n\n    con_duck.close()\n    print(f\"Successfully converted '{duckdb_file}' to '{sqlite_file}'.\")\n\nif __name__ == \"__main__\":\n\n    csv_folder_path = \"./raw_datasets\"  \n    db_file_path = \"000_duck.db\"  \n    sqlite_file_path = \"001_sqlite.db\"\n\n    load_csv_files_as_separate_tables(csv_folder_path, db_file_path)\n    convert_duckdb_to_sqlite(db_file_path, sqlite_file_path)\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/","title":"SQLite: the absolute basics","text":"","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#routines","title":"Routines","text":"<p>create </p> <pre><code>CREATE TABLE t (\n    id INTEGER PRIMARY KEY,\n    title TEXT NOT NULL\n);\n\nINSERT INTO t (id, title) VALUES (1, 'hi'), (2, 'world');\n</code></pre> <p>read</p> <pre><code>SELECT * FROM t WHERE id=2;\n</code></pre> <p>update</p> <pre><code>ALTER TABLE t ADD COLUMN score REAL;\nUPDATE t SET score = 5.0 WHERE id = 1;\n</code></pre> <p>delete</p> <pre><code>DELETE FROM t WHERE id = 1 AND title = 'hi';\nDROP TABLE t;\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#basics","title":"Basics","text":"<ul> <li>limited storage classes (dtypes): <code>null</code>, <code>text</code>, <code>integer</code>, <code>real</code>, <code>blob</code></li> <li><code>create table (...) strict</code> here strict is sqlite specific because its default dtype behaviour is quite the opposite of strict.   </li> <li>constraints: <code>primary key</code>, <code>not null</code>, <code>unique</code>, <code>default</code>, <code>references</code>, <code>check</code> (e.g. price REAL CHECK(price &gt; 0))</li> <li>PRAGMA pragma_name; (to query a setting)</li> <li>PRAGMA pragma_name = value; (to set a setting)</li> <li>ACID compliant: atomicity, consistency, isolation, durability<ul> <li>PRAGMA foreign_key = on; # for integrity. </li> <li>PRAGMA journal_mode= WAL; # for concurrency</li> <li>PRAGMA synchronous = NORMAL;</li> </ul> </li> </ul>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#examples","title":"Examples","text":"<p>Mostly standard SQL</p>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#create-strict-table-and-index","title":"Create strict table and index","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,\n    username TEXT NOT NULL UNIQUE,\n    email TEXT UNIQUE,\n    password_hash TEXT,\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n) STRICT;\n\nCREATE INDEX idx_users_email ON users (email);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#compare-primary-key-and-unique","title":"Compare primary key and unique","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,           -- The unique, non-null, main identifier for each user\n    username TEXT NOT NULL UNIQUE,    -- Unique, but could theoretically be NULL if not specified NOT NULL\n    email TEXT UNIQUE,                -- Unique, but allows multiple NULL emails if some users don't provide one\n    phone_number TEXT UNIQUE,         -- Another unique attribute, allows NULL\n    last_login_ip TEXT\n);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#set-foreign-key","title":"Set foreign key","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,           -- Parent table's Primary Key\n    username TEXT NOT NULL UNIQUE,\n    email TEXT UNIQUE\n);\nCREATE TABLE posts (\n    id INTEGER PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT,\n    user_id INTEGER NOT NULL REFERENCES users (id),  -- defines the Foreign Key column\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#parametrized-query","title":"Parametrized query","text":"<p>sqlite3 is a python interface for the SQLite C library.</p> <pre><code>import sqlite3\n\nwith sqlite3.connect(\"my.db\") as con:\n    username = request.args.get('username')\n    cursor = con.cursor()\n    cursor.execute(\"SELECT * FROM users WHERE username = ?\", (username,))\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/15/basel-framework/","title":"Basel Framework","text":"<p>courtesy: Gemini 2.5 Flash</p> <p>The Basel Framework, developed by the Basel Committee on Banking Supervision (BCBS), provides international standards for banking regulation. It is built upon three pillars:</p> <ul> <li>Pillar 1: Minimum Capital Requirements</li> <li>Pillar 2: Supervisory Review Process</li> <li>Pillar 3: Market Discipline</li> </ul> <p>Here's a breakdown of the differences between Pillar 1 and Pillar 2:</p> <p>Pillar 1: Minimum Capital Requirements</p> <ul> <li>Focus: This pillar sets out the minimum amount of capital that banks must hold to cover specific, quantifiable risks.</li> <li>Risks Covered: Primarily covers:<ul> <li>Credit Risk: The risk of loss due to a borrower's failure to repay a loan or meet contractual obligations.</li> <li>Market Risk: The risk of losses in on- and off-balance-sheet positions arising from movements in market prices (e.g., interest rates, exchange rates, equity prices, commodity prices).</li> <li>Operational Risk: The risk of loss resulting from inadequate or failed internal processes, people, and systems, or from external events (e.g., fraud, system failures, legal risks).</li> </ul> </li> <li>Calculation: Banks can use either standardized approaches (prescribed by regulators) or, if approved, internal models (developed by the bank itself) to calculate their risk-weighted assets (RWAs) for these risks. The minimum capital requirement is typically a percentage (e.g., 8%) of these RWAs.</li> <li> <p>Nature: More rules-based and quantitative. It provides a standardized baseline for capital adequacy across banks.</p> </li> <li> <p>Primarily set out in the Capital Requirements Regulation (CRR). The CRR (Regulation (EU) No 575/2013, and its subsequent amendments like CRR II and CRR III) is a directly applicable EU law that lays down the detailed rules for banks' capital requirements, including the calculation of Risk-Weighted Assets (RWAs) for credit, market, and operational risks. It's a regulation because it's uniformly applied across all EU member states without needing national transposition.</p> </li> </ul> <p>Pillar 2: Supervisory Review Process</p> <ul> <li>Focus: This pillar addresses risks not fully captured by Pillar 1 and ensures that banks have adequate capital to support all material risks in their business, as well as robust risk management processes. It's about a more holistic and qualitative assessment of a bank's risk profile.</li> <li>Risks Covered:<ul> <li>Includes risks already covered in Pillar 1, but with a deeper assessment of how well the bank manages them (e.g., concentration risk within credit risk).</li> <li>Crucially, it also covers other material risks not fully or adequately captured by Pillar 1, such as:<ul> <li>Interest Rate Risk in the Banking Book (IRRBB)</li> <li>Liquidity Risk</li> <li>Strategic Risk</li> <li>Reputational Risk</li> <li>Business Model Risk</li> <li>Pension Risk</li> <li>Environmental, Social, and Governance (ESG) risks (increasingly relevant)</li> </ul> </li> </ul> </li> <li>Process: This involves:<ul> <li>Internal Capital Adequacy Assessment Process (ICAAP): Banks are required to conduct their own comprehensive assessment of their risks and determine the appropriate level of capital needed to cover them, along with a strategy for maintaining that capital.</li> <li>Supervisory Review and Evaluation Process (SREP): Supervisors review the bank's ICAAP and its overall risk profile. Based on this review, supervisors can require banks to hold additional capital above the Pillar 1 minimum (known as Pillar 2 capital requirements or add-ons), address deficiencies in their risk management, or take other supervisory actions.</li> </ul> </li> <li>Nature: More principles-based, qualitative, and tailored to the individual bank's specific circumstances. It allows for supervisory judgment and flexibility.</li> <li>Primarily set out in the Capital Requirements Directive (CRD). The CRD (Directive 2013/36/EU, and its subsequent amendments like CRD V) is an EU directive, which means member states must transpose its provisions into their national laws. The CRD provides the legal basis for the supervisory review process (SREP), including the requirement for banks to conduct an Internal Capital Adequacy Assessment Process (ICAAP) and for supervisors to assess these and potentially impose additional capital requirements (Pillar 2 Requirements - P2R) or guidance (Pillar 2 Guidance - P2G).</li> </ul> <p>Key Differences Summarized:</p> Feature Pillar 1: Minimum Capital Requirements Pillar 2: Supervisory Review Process Primary Goal To set a minimum, quantitative capital floor for specific, common risks. To ensure holistic capital adequacy and sound risk management. Scope of Risks Credit, Market, and Operational Risks (quantifiable). All material risks (including those not fully captured by Pillar 1). Approach Rules-based, standardized (or internal model-based but still prescriptive). Principles-based, judgmental, and bank-specific. Tool/Process Calculation of Risk-Weighted Assets (RWAs) and minimum capital ratios. Internal Capital Adequacy Assessment Process (ICAAP) and Supervisory Review and Evaluation Process (SREP). Output Minimum capital ratios. Bank-specific capital requirements (P2R), and supervisory guidance (P2G) on top of Pillar 1, and requirements for improved risk management. Enforcement Binding minimum capital ratios. Binding Pillar 2 requirements (P2R) and non-binding guidance (P2G). <p>In essence, Pillar 1 provides the foundational minimum capital rules, while Pillar 2 acts as a crucial complement, ensuring that banks not only meet these minimums but also proactively manage all their risks and hold sufficient capital for their specific risk profile, under the oversight of supervisors.</p>","tags":["regulations"]},{"location":"2025/06/17/exploring-a-large-database/","title":"Exploring a large database","text":"<p>Big organizations typically have their data stored in one or multiple database.  Sometimes, analytics team find themselves buried in the jungle of tables/views with non consistent naming conventions,  therefore hard to build a holistic view about where to find which data etc.  </p> <p>In this post we explore a few tricks to ease the initial phase of understanding a large database. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#having-applications-in-mind","title":"Having applications in mind","text":"<p>Very often in business, database are the backend of some frontend applications with GUI that one can interact with. Therefore, it is not a bad idea to use the frontend application first hand to see what's possible, the meaning of a column, how it relates to other columns etc.</p> <p>This can be a back-and-forth process, while doing analytics work, check the GUI to see where are the column names appear in the application, gradually completing a good mental picture of the 'geography' of the tables.  </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#learn-the-data-model-from-an-architect","title":"Learn the data model from an architect","text":"<p>Data architect is responsible for creating data model: how the business is broken into concepts (stored as tables) and how they relate (via foreign keys). If talking to an architect is an option, it is the fastest way of understaning the data model. </p> <p>Next we explore how to reverse engineer the data model. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#visualizations","title":"Visualizations","text":"<p>Some DB has their native visualization tools, ER generators (e.g. Oracle SQL developer). Not a bad idea to take advantage of that. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#use-metadata-tables","title":"Use metadata tables","text":"<p>Almost all database systems have a kind of information schema which contains tables that describe the metadata of the database such as the existing schemas, tables, views, columns etc. </p> <p>In Oracle it's <code>ALL_TABLES</code> <code>ALL_VIEWS</code> <code>ALL_TAB_COLUMNS</code> (not a schema per se, rather individual tables that start with <code>ALL_</code> prefix).  In Postgres/DuckDB/Snowflake it's the <code>information_schema</code>  schema, with tables such as <code>tables</code>, <code>columns</code> in it. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#foreign-keys","title":"Foreign keys","text":"<p>For obvious reasons, it is important to know these keys for the tables of interest. In Oracle, use <code>ALL_CONSTRAINTS</code> table. In Snowflake/DuckDB/Postgres, use <code>information_schema.REFERENTIAL_CONSTRAINTS</code>.</p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#describe-a-table","title":"Describe a table","text":"<p>Literally <code>DESCRIBE _NAME_</code> to show the column names and types. </p>","tags":["data engineering"]},{"location":"2025/07/07/random-forest-scaling-mini-benchmark/","title":"Random forest scaling mini-benchmark","text":"<p>My goal in this minibench is to see how random forest models scale with the sample size/feature size.</p> <p>I run the script at the end of this post on my Intel i5-10400 6 cores + 8GB RAM machine. I cannot scale sample size any larger i.e. 400k rows with this hardware. </p> <p>Software side, I have scikit-learn=1.7 and numpy=2.2.5 running on python3.12</p> <p>Here is the result </p> <p></p> <p>Two observations</p> <ul> <li>it is easier to scale the feature number than to scale the sample size</li> <li>there is some sort of crossover regime change in that 2x sample size requires 3x or more training time at larger sample size, something that is not happening when the sample size is small. </li> </ul> <pre><code>import time\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n\ndef generate_data(n_samples, n_features):\n    print(f\"Generating data: {n_samples} samples, {n_features} features...\")\n    X = np.random.randint(2, size=(n_samples, n_features))\n    y = np.random.randint(2, size=n_samples)\n    return X, y\n\ndef benchmark_training(X, y):\n    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n    start_time = time.time()\n    model.fit(X, y)\n    end_time = time.time()\n    return end_time - start_time\n\ndef run_benchmarks():\n    results = []\n\n    print(\"-\" * 50)\n    print(\"Benchmark 1: Increasing Number of Samples (Rows)\")\n    print(\"-\" * 50)\n    n_features_fixed = 50\n    sample_sizes = [1000, 5000, 10000, 50000, 100000, 200000, 300000]\n\n    for n_samples in sample_sizes:\n        try:\n            X, y = generate_data(n_samples, n_features_fixed)\n            training_time = benchmark_training(X, y)\n            results.append({\n                \"Test\": \"Increasing Samples\",\n                \"Samples\": n_samples,\n                \"Features\": n_features_fixed,\n                \"Bits\": n_samples * n_features_fixed,\n                \"Training Time (s)\": round(training_time, 4)\n            })\n            del X, y\n        except MemoryError:\n            print(f\"MemoryError at {n_samples} samples. Stopping this benchmark.\")\n            break\n\n    print(\"\\n\" + \"-\" * 50)\n    print(\"Benchmark 2: Increasing Number of Features (Columns)\")\n    print(\"-\" * 50)\n    n_samples_fixed = 10000\n    feature_sizes = [10, 50, 100, 500, 1000, 2000]\n\n    for n_features in feature_sizes:\n        X, y = generate_data(n_samples_fixed, n_features)\n        training_time = benchmark_training(X, y)\n        results.append({\n            \"Test\": \"Increasing Features\",\n            \"Samples\": n_samples_fixed,\n            \"Features\": n_features,\n            \"Bits\": n_samples_fixed * n_features,\n            \"Training Time (s)\": round(training_time, 4)\n        })\n        del X, y\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Benchmark Results Summary\")\n    print(\"=\" * 50)\n    df_results = pd.DataFrame(results)\n    print(df_results.to_string(index=False))\n\n\nif __name__ == \"__main__\":\n    run_benchmarks()\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/10/logging-in-python/","title":"Logging in python","text":"<p>The basic setup </p> <ol> <li>create a logger with a name</li> <li>define a handler </li> <li>set handler level</li> <li>set handler  format</li> <li>add handler to logger</li> <li>if needed, repeat 2-5 to define another handler and add it to logger</li> </ol> <p>We consolidate the above in a reusable function. Its functionality is only slightly more than minimal, but probably sufficient for 99% of a data person's use case. </p> <pre><code>import logging\n\ndef setup_logger(\n    name: str,\n    level=logging.DEBUG,\n    log_to_file=False,\n    filename=\"app.log\",\n    file_level=None  # optional file-specific level\n):\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n    # Clear any existing handlers (optional but helps in Jupyter)\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    # Console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    # Optional file handler with separate level\n    if log_to_file:\n        fh = logging.FileHandler(filename)\n        fh.setLevel(file_level if file_level else level)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n</code></pre> <p>Usage</p> <pre><code>logger = setup_logger(\"mylog\", level=10, log_to_file=True, file_level=30) # debug=10, warning=30\nlogger.info(\"30&gt;info=20&gt;= 10, go to console but NOT file\")\nlogger.error(\"error=40 &gt;= 30, go to console and file\")\n</code></pre>","tags":["dev tools"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/","title":"Hierarchical clustering with 36 LOC","text":"<p>Clustering is the action of dividing samples into subgroups. </p> <p>Hierarchical clustering is a bottom up method that gradually merge individual samples into bigger groups until all samples are merged into one group. The criterion for the merge is distance-based. For example, the single linkage algorithm merges clusters that are closest to each other where the distance between clusters is measured using Euclidean distance between sets. </p> <p>The abundant choice of distance functions lead to a zoo of HC algorithms, but the idea is the same. </p> <p>For example we have 5 samples indexed by 1, ..., 5. Each sample is a cluster of its own in the beginning. Let's say 1 and 3 are the closest among all pairs. We merge them and obtain clusters {1,3}, {2}, {4}, {5}. Then we compute the inter-cluster distance and, say, find the closest ones to be {1,3} and {2}.  We merge them to obtain clusters {1,2,3}, {4}, {5}. We repeat this process until there is only one cluster {1,2,3,4,5}. </p> <p>It is up to the user of the algorihm to decide the number of clusters needed for the problem at hands. Say the user wants 2 clusters, then they can use the 2 clusters just before the last merge. If they want 3 clusters, they can stop before the one but last merge, etc etc.</p> <p>Hopefully the idea is clear. Let's see how to implement it. We need to keep track of two things (in fact three, hold on for a second for the explanation):</p> <ul> <li>current cluster structure (a partition if you wish)</li> <li>current inter-cluster distances</li> </ul> <p>For the first, we can use a hashmap <code>dict[int, set]</code> where the key is the index of cluster, and the value is the collection of samples indices beloging to that cluster. </p> <p>For the second, we can use a heap (priority queue) so that the retrieval of minimum distance and its update is fast. For faster computation of inter-cluster distance, it is actually beneficial to have a copy of the distances stored as a hashmap too (hence 3 things to maintain). </p> <p>In summary, here's the plan of attack. </p> <p>Initialization: </p> <ul> <li>define each individual as one cluster </li> <li>compute pairwise distances and store them in a heap and a hashmap</li> </ul> <p>Main loop:</p> <ol> <li>retrieve smallest distance</li> <li>merge: update clusters hashmap</li> <li>compute distances: update distances heap and hashmap</li> </ol> <p>Here is all the code. Also a few comments just below. </p> <pre><code>import math \nimport heapq\n\ndef euclid(u,v):\n    return math.sqrt(sum((ui-vi)**2 for ui,vi in zip(u,v)))\n\ndef init(arr):\n    \"\"\"space O(m^2)\"\"\"\n    m = len(arr)\n    clusters = {i:{i} for i in range(m)}\n    dmap = {(i,j): euclid(arr[i],arr[j])\n        for i in range(m-1)\n        for j in range(i+1,m)\n    }\n    dheap = [(d,i,j) for (i,j), d in dmap.items()]\n    heapq.heapify(dheap)\n    return clusters, dmap, dheap\n\ndef main(arr, low):\n    \"\"\"time O(m^2 log m)\"\"\"\n    clusters, dmap, dheap = init(arr)\n    res = []\n    while len(clusters)&gt;low:\n        # retrieve smallest\n        while True: \n            d, i, j = heapq.heappop(dheap)\n            if i in clusters and j in clusters:\n                break\n        res.append((d,i,j))\n        # update clusters: + 1 - 2\n        new_idx = max(clusters) + 1\n        clusters.update({new_idx: clusters[i].union(clusters[j])})\n        clusters.pop(i)\n        clusters.pop(j)\n        # compute distance\n        for k in clusters:\n            if k != new_idx:\n                # update dmap O(1)\n                dmap[(k,new_idx)] = newtok = min(\n                    dmap[tuple(sorted([i,k]))],  \n                    dmap[tuple(sorted([j,k]))]\n                )\n                # udpate dheap O(log m)\n                heapq.heappush(dheap,(newtok,k,new_idx))\n    return clusters, res\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#why-heap","title":"Why heap?","text":"<p>A heap is a complete binary tree (stored in a sequence by arithmetic mapping between nodes and index). In python <code>heapq</code> implements the min heap which means that the root is the smallest element and  all parent node is smaller than the children nodes. The retrival of smallest element is O(1) operation. </p> <p>A heap must be dynamically maintained with the insertion and deletion of elements. Since the height of a complete binary tree is O(log n) for a tree with n elements, the cost of maintain the min heap structure when inserting (push) new element to the heap or removing (pop) smallest element is O(log n) (because an update is basically a chain of swapping operations between parent and child node, upwards / downwards). </p> <p>The python module <code>heapq</code> is in the stdlib with a simple API. To pop an element from the root: </p> <pre><code>heapq.heappop(dheap)\n</code></pre> <p>To push a new element into the heap:</p> <pre><code>heapq.heappush(dheap, element)\n</code></pre> <p>At initialization, one can turn a list into a heap all at once with O(n) time instead of push one by one which would require O(n log n) time</p> <pre><code>heapq.heapify(my_list)\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#merge","title":"Merge","text":"<p>Notice that we mint a new index for the merged cluster, which becomes the hash key. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#inter-cluster-distance","title":"Inter-cluster distance","text":"<p>This is where different algorithms vary. What we have implemented is the single linkage algorithm.  </p> <pre><code>dmap[(k,new_idx)] = min(\n    dmap[tuple(sorted([i,k]))],  \n    dmap[tuple(sorted([j,k]))]\n)\n</code></pre> <p>The  complete  linkage algorithm amounts to take max instead of min in the above. Each choice has its own trade-off and focus. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#main-loop","title":"Main loop","text":"<p>The first while loop is simple: we merge one at a time until we reach <code>low</code> number of clusters.</p> <p>The inner while loop is necessary becasue when a merge happens, the old cluster index is removed from  <code>clusters</code> but they still exists in <code>dheap</code>. We keep removing them until we get both indices in the current <code>clusters</code> (as keys). </p> <p>The purpose of the for loop is to push distances of the new cluster with other clusters into <code>dheap</code>, as well as the <code>dmap</code>. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#demo","title":"Demo","text":"<p>Here is an interactive demo. You can activate the app mode by pressing (Ctrl+ .)  The visualization is generated one-shot by Gemini 2.5 pro. </p> <p></p> <p>The source is here in case the link is not valid any more when you read this post. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/","title":"Unpacking the n-1 in the t test","text":"<p>If you ask an AI (below Grok 3 think mode) to explain the degree of freedom in a t test, or open an introductory statistics textbook you'll get explanation like this </p> <p>https://grok.com/share/bGVnYWN5_e1fcd789-991d-4593-beba-99a78a8f71c1</p> <p>This explainer is OK for someone who gets introduced to the topic for the first time - this had been how I taught the topic for a first year statistics class.  But there are a lot of hand-waving which may not be deemed satisfactory for someone who likes precise statement. </p> <p>My goal in this post is to spell out all the  details. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#fun-fact","title":"Fun fact","text":"<p>I'll start with a fun fact that may seem off topic. I promise it is not.</p> <p>Let \\(X,Y\\) be independent standard Gaussian random variables. Consider the mean \\((X+Y)/2\\) and the distance to the mean \\(X - (X+Y)/2 = (X-Y)/2\\). THe interesting fact is that they are independent Gaussians (with variance \\(1/2\\)). </p> <p>In other word knowing the mean does not provide any information  about the distance from the mean to a particular sample.  The fact can be proved by showing that the covariance is zero, which implies independence under Gaussian distribution (compute the Laplace transform to prove the latter).  </p> <p>Ok, now let's push the example a bit by considering \\(n\\) iid Gaussian \\(X_1,...,X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Analogously, consider </p> \\[  G = (X_i - \\bar X)_{i= 1,...,n} \\] <p>where \\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Note that this is a Gaussian vector.  It is a degenerate one because this random vector lives in a hyperplane \\(\\{x\\in\\mathbb{R}^n: \\sum_i^n x_i = 0 \\}\\). Again we can show that each coordinate of this Gaussian vector is independent of \\(\\bar X\\) by checking the covariance because of Gaussianity. Therefore \\(G\\) and \\(\\bar X\\) are independent.</p> <p>Recall that the sample variance is the scaled squared 2-norm of the vector \\(G\\) </p> \\[ \\hat\\sigma^2 = \\frac{1}{n-1}\\|G\\|^2 \\] <p>where the normalization \\(n-1\\) ensures that \\(\\hat\\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\). </p> <p>As a direct consequence of the independence of \\(G\\) and \\(\\bar X\\), we get independence of \\(\\hat\\sigma^2\\) and \\(\\bar X\\).</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#relate-to-t-statistic","title":"Relate to t statistic","text":"<p>t statistic for iid Gaussians of hypothesised population mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\) is given by</p> \\[ t = \\frac{\\bar X - \\mu}{\\hat\\sigma/\\sqrt{n}} \\] <p>From our discussion in the previous section, we know that the numerator and the denominator are independent random variables. Since \\(\\bar X\\) is Gaussian, we know that t statistic is actually a mixture of Gaussian with random variance \\(\\sigma^2/\\hat\\sigma^2\\).</p> <p>We know Gaussian very well but what is the distriubtion of the sample variance?</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#setting-up-the-target","title":"Setting up the target","text":"<p>The goal is to show</p> \\[ \\hat\\sigma^2 = \\sigma^2 \\frac{\\chi^2_{n-1}}{n-1} \\] <p>where \\(\\chi^2_d\\) is the chi-square distribution with \\(d\\) degrees of freedom.</p> <p>Ah, degree of freedom ! Hold on, we are close.</p> <p>Recall that \\(\\chi^2_d\\) can be defined as the sum of independent standard Gaussian squared where the degree of freedom is the number of terms in the sum. </p> <p>Looking at \\(\\hat\\sigma^2\\), we do have a sum of Gaussian squared, but they are not indepedent (\\(G\\) is degenerate) and there are \\(n\\) terms in the sum. We need to find a way to somehow reformualte it as a sum of \\(n-1\\) independent Gaussian squares.</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#some-linear-algebra","title":"Some linear algebra","text":"<p>Notice that the distribution of \\(G\\) is invariant with respect to the value of \\(\\mu\\). We therefore can assume \\(\\mu=0\\). Our goal is to show </p> \\[ \\frac{\\|G\\|^2}{\\sigma^2} = \\chi^2_{n-1} \\] <p>We can write \\(G\\) in matrix form</p> \\[ G = A X \\] <p>where \\(A = I - (1/n)E\\) with \\(E\\) being \\(n \\times n\\) filled with 1 and \\(I\\) being the identity matrix.</p> <p>The matrix \\(A\\) is symmetric and has an interesting property</p> \\[ A^2 = A \\] <p>so it is an orthogonal projection. This property (aka idempotence) implies that all the eigenvalues are either 0 or 1. Indeed, if \\(Av = \\lambda v\\) for some non-zero vector \\(v\\), then \\(\\lambda v = Av = A^2v = A(Av) = \\lambda^2 v\\).  </p> <p>By spectral decomposition, there exists an orthogonal matrix \\(O\\) and diagomal matrix \\(\\Lambda\\) (filled with 0 and 1 on the diagonal because of idempotence) such that \\(A= O^T \\Lambda O\\), leading to </p> \\[ \\|G\\|^2 = \\langle AX, AX \\rangle = \\langle AX,X\\rangle = \\langle  \\Lambda OX, OX\\rangle = \\langle  \\Lambda X, X\\rangle \\] <p>where the last equality is distributional identity, using the fact that standard multivariate normal distribution is invariant under orthogonal transformation.  </p> <p>Therefore we have shown that  \\(\\|G\\|^2\\) can indeed be represented as the sum of independent mean-zero (variance \\(\\sigma^2\\)) Gaussian sqaured. The one last thing to check is the number of terms in the sum which corresponds to the number of ones in the spectrum of \\(A\\). This must be equal to the rank of \\(A\\) which is \\(n-1\\). CQFD.  </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#putting-all-together","title":"Putting all together","text":"<p>To conclude, we have</p> \\[ t = \\frac{N(0,1)}{\\sqrt{\\chi^2_{n-1} / (n-1)}} \\] <p>with independent numerator and denominator. We can now subscript \\(t\\) with parameter \\(n-1\\)  to indicate the degree of freedom on the right hand side of the equation. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#a-slight-generalisation-two-sample-t-test-with-equal-variance","title":"A slight generalisation: two sample t test with equal variance","text":"<p>Consider two sample t test. Given \\(\\{X_1,...,X_n\\}\\) and \\(\\{Y_1,...,Y_m\\}\\) two collections of Gaussian with same variance \\(\\sigma^2\\). </p> <p>The null hypothesis is that the population mean of two samples are the same. The t statistic is </p> \\[ t = \\frac{\\bar X - \\bar Y}{      (1/n + 1/m) \\hat\\sigma_p } \\] <p>where \\(\\hat\\sigma_p^2 = \\frac{(n-1)\\hat\\sigma^2_X + (m-1)\\hat\\sigma^2_Y}{n+m-2}\\), the pooled sample variance. The numerator is the squared norm of a Gaussian vector </p> \\[(X_1 - \\bar X, .... X_n-\\bar X, Y_1 - \\bar Y, ...,  Y_m - \\bar Y)\\] <p>which lives in a \\((n+m-2)\\)-dimensional subspace. One checkes readily that this vector can be represented as an idempotent transformation of the (\\(m+n\\))-variate standard Gaussian \\((X_1,...,X_n, Y_1,...,Y_m)\\). Arguing as before, the squared norm can be written, via spectral decomposition, as a sum of independent squared Gaussians (i.e. chisquare). The number of terms in the sum coincide with the rank of the idempotent transformation which is \\(m+n-2\\). We have thus \\(t = \\frac{N(0,1)}{      \\sqrt{\\chi^2_{m+n-2} / (m+n-2)}}\\). </p>","tags":["quant methods"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/","title":"Use code when Dataiku's UI gets in the way","text":"","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#feed-partition-numbers-with-code","title":"Feed partition numbers with code","text":"<p>To programmatically feed partition numbers, do</p> <pre><code>import dataiku as dk\np = dk.api_client().get_project(\"MYPROJECT\")\np.get_recipe(\"MYRECIPE\").run(partitions=...)\n</code></pre> <p>This is particularly useful when the UI cannot autocomplete the partition keys (e.g. discrete values), but it is trivial to  write a python function/SQL query to retrive them. Also possible is to use the grammer for partition identifiers:</p> <p>https://doc.dataiku.com/dss/latest/partitions/identifiers.html</p> <p>Another way of doing it is through Job object. There are more configs which can be useful. </p> <p>https://developer.dataiku.com/latest/concepts-and-examples/jobs.html https://developer.dataiku.com/latest/api-reference/python/projects.html#dataikuapi.dss.project.DSSProject.list_jobs</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#access-custom-variables","title":"Access custom variables","text":"<p>There are several possible ways of accessing variables. For me, the one in the dataiku namespace is best for the ease of use</p> <pre><code>import dataiku as dk\n\ndk.get_custom_variables() \n</code></pre> <p>Indeed, although one gets the project level variables like this </p> <pre><code>dk.api_client().project(\"MYPROJECT\").get_variables()\n</code></pre> <p>the returned object is a nested dictionary, with standard variables dict and local variables dict inside it. </p> <p>As far as I can tell, <code>dk.get_custom_variables</code> has everything in it, containing both standard and local variables, and it is a flat dictionary. I prefer this one over the project-level variables dict. </p> <p>Yet another kind of variables are runtime variables. If we call the below in a notebook we get <code>None</code>. It may come in handy in a SQL/Python recipe and can be used to refer to a column that is the partition key (be it in the input table or the ouput table). </p> <pre><code>dk.dku_flow_variables # dict\n</code></pre>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#sql-recipe-that-takes-as-input-a-partitioned-dataset","title":"SQL recipe that takes as input a partitioned dataset","text":"<p>if I want to partition the output on a column that is not necessarily the input partition key</p> <p></p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#python-recipe-that-takes-as-input-a-partitioned-dataset","title":"Python recipe that takes as input a partitioned dataset","text":"<p>more rules about partition variable substitution</p> <p>https://doc.dataiku.com/dss/latest/partitions/variables.html</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#partition-dependency-function","title":"Partition dependency function","text":"<p>Haven't use it in my projects but I can see it being flexible in specifying the dependency of input and output partitions. </p> <p>https://doc.dataiku.com/dss/latest/partitions/dependencies.html</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/05/vasicek-in-basel/","title":"Vasicek in Basel","text":"<p>be me be Vasicek chill Czech dude decide to model credit risk with Brownian motion invent one-factor Gaussian copula before it was cool \"default happens when latent variable crosses threshold\" everyone claps  </p> <p>fast-forward Basel Committee steals my idea slaps some capital requirement on it banks now simulating millions of standard normals get no royalties</p> <p>Merton out here pricing corporate debt with structural models I just make it rain closed-form expressions still gets all the fame  </p> <p>Moody\u2019s KMV be like \u201cthanks for the latent variable bro\u201d turn it into a product banks pay millions I get academic citation  </p> <p>still better than being Black or Scholes at least nobody\u2019s blowing up hedge funds with my model yet.  </p> <p>(prose courtesy ChatGPT)</p> <p>This post is not about Vasicek's life. It is about a legacy Vasicek left for the banking regulations.</p>","tags":["quant methods","regulations"]},{"location":"2025/08/05/vasicek-in-basel/#capital-requirement","title":"Capital requirement","text":"<p>The above formula figures in the Basel framework for regulatory capital computation. You see the Gaussian distribution function, the probability of default, the correlation of obligors's asset value with a common factor for all obligors such as economic situation.</p> <p>Where is this coming from?   </p> <p>Reference</p> <p>http://mx.nthu.edu.tw/~jtyang/Teaching/Risk_management/Papers/Models/Probability%20of%20Loss%20on%20Loan%20Portfolio.pdf</p> <p>https://www.bankofgreece.gr/MediaAttachments/Vasicek.pdf</p>","tags":["quant methods","regulations"]},{"location":"2025/08/05/vasicek-in-basel/#one-factor-model","title":"One factor model","text":"<p>Capital is required to cover unexpected loss. Mathematically, the expected loss just means the expectation of the loss of a portfolio of instruments. The regulatory unexpected loss can be roughly translated to the difference between the quantile of the loss distribution and the expected value of the loss.</p> <p>The one factor model postulates the following, the asset value of obligor \\(i\\) is the </p> \\[ A_i = \\sqrt{\\rho} X + \\sqrt{1-\\rho} \\epsilon_i \\] <p>where \\((X,\\epsilon_i, i=1,2,...)\\) is iid Gaussian standard normal, and \\(X\\) represents the common factor that influences the value of all assets. Notice that \\((A_i)\\) are standard Gaussian correlated with correlation \\(\\rho\\).  </p> <p>Default event occurs if the asset value falls below a certain threshold \\(B_i\\). The probability of default given \\(X\\) is therefore</p> \\[ P[A_i&lt; B_i|X] = \\Phi(\\frac{B_i -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}}) \\] <p>where \\(\\Phi\\) is the CDF of a standard Gaussian. </p> <p>In real life, the (unconditional) probability of default may come from an internal model (say a logistic regression on obligor characteristics), denoted by \\((p_i)\\)</p> \\[ p_i = P[A_i&lt;B_i] \\] <p>namely </p> \\[ B_i  = \\Phi^{-1}(p_i). \\]","tags":["quant methods","regulations"]},{"location":"2025/08/05/vasicek-in-basel/#law-of-large-numbers","title":"Law of large numbers","text":"<p>When the number of obligors in a portfolio is large, the law of large numbers kicks in,</p> \\[ P\\Big[\\frac{1}{n}\\sum_{i=1}^n I(A_i&lt;\\Phi^{-1}(p_i)) \\to  \\Phi(\\frac{B_i -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}})|X \\Big] =1 \\] <p>Assume that the loss incured by a default event is 1/n across all obligors in the portfolio, then the loss is </p> \\[ L = \\frac{1}{n}\\sum_{i=1}^n I(A_i&lt;\\Phi^{-1}(p_i)) \\approx  \\Phi(\\frac{\\Phi^{-1}(p_i) -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}}). \\]","tags":["quant methods","regulations"]},{"location":"2025/08/05/vasicek-in-basel/#quantile-of-the-loss","title":"Quantile of the loss","text":"<p>We show that </p> \\[ q_{L,\\alpha} \\approx \\Phi(\\frac{\\Phi^{-1}(p_i) - \\sqrt{\\rho} q_{X,1-\\alpha}}{\\sqrt{1-\\rho}}) \\] <p>where  \\(q_{D,\\alpha}\\) is the \\(\\alpha\\) quantile of random variable \\(D\\), therefore \\(q_{X,1-\\alpha} = \\Phi^{-1}(1-\\alpha)= -\\Phi^{-1}(\\alpha)\\). The Basel formula follows right off with \\(\\alpha = 0.999\\). </p> <p>Indeed, we are interested in the 0.999 quantile of \\(L\\) which is approximately a monotone function of the Gaussian \\(X\\). Using the approximation, we have </p> \\[ \\begin{align*} 0.999 = P[L\\le q] &amp; \\approx P[\\Phi(\\frac{\\Phi^{-1}(p_i) -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}})\\le q] \\\\ &amp; = P\\Big[X\\ge \\frac{\\Phi^{-1}(p_i) - \\sqrt{1-\\rho} \\Phi^{-1}(q)}{\\sqrt{\\rho}}\\Big] \\\\ &amp;= 1 - \\Phi( \\frac{\\Phi^{-1}(p_i) - \\sqrt{1-\\rho} \\Phi^{-1}(q)}{\\sqrt{\\rho}}) \\end{align*} \\] <p>Rearranging terms</p> \\[ \\Phi^{-1}(q) = \\frac{ \\Phi^{-1}(p_i) - \\sqrt{\\rho}\\Phi^{-1}(0.001)  } {\\sqrt{1-\\rho} }. \\] <p>Hence,</p> \\[ q = \\Phi(\\frac{\\Phi^{-1}(p_i)+\\sqrt{\\rho}\\Phi^{-1}(0.999)}{\\sqrt{1-\\rho}})   \\] <p>as desired. </p>","tags":["quant methods","regulations"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/","title":"Unpacking the k-1 in the chi-square test","text":"<p>This post is a continuation of my previous one about t-test. The aim, as before, is to spell out all the details about the degrees of freedom in the chi-square test. </p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#setting","title":"Setting","text":"<p>Let \\(\\mathbf{X} = (X_1, \\dots, X_k)\\) be a multinomial random vector with \\(k\\) categories, total count \\(n\\), and probabilities \\(\\mathbf{p} = (p_1, \\dots, p_k)\\). Pearson\u2019s chi-square test statistic is</p> \\[ \\chi^2 = \\sum_{i=1}^k \\frac{(X_i - n p_i)^2}{n p_i}. \\] <p>As \\(n \\to \\infty\\), the distribution of \\(\\chi^2\\) converges to a chi-square distribution with \\(k - 1\\) degrees of freedom.</p> <p>The goal of this post is to explain where the \\(k - 1\\) comes from.</p> <p>This post can be read independently of my previous one on the t-test, but a core part of the argument was already presented there, so reading that first might help.</p> <p>Core ideas:</p> <ul> <li>Represent the multinomial vector as a sum of i.i.d. random vectors (so normal approximation applies).</li> <li>The chi-square statistic becomes close to the squared norm of a Gaussian vector lying in a \\((k - 1)\\)-dimensional subspace.</li> <li>Spectral decomposition shows that this squared norm is distributed as a sum of \\(k - 1\\) independent standard normal squares.</li> </ul> <p>That last part is a generalization of the linear algebra trick we used in the t-test post.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#multivariate-central-limit-theorem","title":"Multivariate central limit theorem","text":"<p>Let \\(E = \\{e_1, \\dots, e_k\\}\\) be the canonical basis of \\(\\mathbb{R}^k\\) where each \\(e_i\\) has a 1 in the \\(i\\)-th position and 0 elsewhere.</p> <p>Let \\(I_i\\) be a random vector taking values in \\(E\\) such that</p> \\[ P[I_i = e_j] = p_j. \\] <p>Then</p> \\[ (X_1, \\dots, X_k) = \\sum_{i=1}^n I_i. \\] <p>The mean of this sum is \\(n \\mathbf{p}\\). Its covariance, normalized by \\(n\\), does not depend on \\(n\\), so we denote it by \\(\\Sigma = \\Sigma(\\mathbf{p})\\).</p> <p>By the multivariate central limit theorem:</p> \\[ \\frac{1}{\\sqrt{n}}\\left( \\sum_{i=1}^n I_i - n \\mathbf{p} \\right) \\to \\mathcal{N}(0, \\Sigma). \\] <p>Let \\(G\\) denote this limiting Gaussian vector. Then the chi-square statistic is asymptotically approximated by</p> \\[ \\sum_{i=1}^k \\frac{G_i^2}{p_i} = \\left\\| \\left( \\frac{G_1}{\\sqrt{p_1}}, \\dots, \\frac{G_k}{\\sqrt{p_k}} \\right) \\right\\|^2. \\] <p>We now want to show that this norm squared follows a chi-square distribution with \\(k - 1\\) degrees of freedom.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#checking-the-covariance","title":"Checking the covariance","text":"<p>We claim:</p> \\[ \\text{Cov}[X_i, X_j]/n = p_i \\delta_{ij} - p_i p_j. \\] <p>Hence, the Gaussian vector \\(H = (G_1 / \\sqrt{p_1}, \\dots, G_k / \\sqrt{p_k})\\) has covariance matrix</p> \\[ V_{ij} = \\delta_{ij} - p_j. \\] <p>So we\u2019ve shown that \\(\\chi^2\\) converges in distribution to \\(\\|H\\|^2\\). The rest of the argument is nearly identical to the t-test post, so I'll just sketch it here.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#linear-algebra-again","title":"Linear algebra again","text":"<p>Note that \\(V = V^2\\), so it is a projection matrix. Its eigenvalues are either 0 or 1.</p> <p>We can write \\(H = VZ\\), where \\(Z\\) is a standard Gaussian in \\(\\mathbb{R}^k\\). Then \\(\\|H\\|^2 = \\|VZ\\|^2\\) is the sum of squares of the projections of \\(Z\\) onto the image of \\(V\\).</p> <p>Spectral decomposition lets us write this as a sum of \\(r\\) independent standard normal squares, where \\(r = \\text{rank}(V)\\).</p> <p>We now check that \\(\\text{rank}(V) = k - 1\\).</p> <p>Recall:</p> \\[ \\Sigma = \\text{diag}(\\mathbf{p}) - \\mathbf{p} \\mathbf{p}^\\top. \\] <p>So if \\(\\Sigma x = 0\\), then necessarily \\(x_1 = x_2 = \\dots = x_k = \\sum_i x_i p_i\\). That is, the null space consists of constant vectors, hence it's one-dimensional.</p> <p>Therefore, \\(\\text{rank}(\\Sigma) = k - 1\\), and since \\(V\\) is derived from \\(\\Sigma\\), we also have \\(\\text{rank}(V) = k - 1\\).</p> <p>So \\(\\|H\\|^2\\) is the sum of \\(k - 1\\) independent standard normal squares, namely, chi-square distributed with \\(k - 1\\) degrees of freedom. CQFD.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#generalization-again","title":"Generalization again","text":"<p>Now consider contingency tables which are pivot tables of bivariate categorical data. We assume iid sequence \\(\\{(X_i,Y_i), i=1,...,n \\}\\). Both are categorical with \\(a\\) and \\(b\\) categories respectively. The data can be stored as a table of shape (n,2). Pivoting on the second column (Y's) with the first column (X's) as index, we get frequencies of each one of the \\(ab\\) combinations. The pivot table is of shape (a,b). In dataframe land, this is done as follows:</p> <pre><code>import polars as pl\nimport numpy as np\n\nx = pl.from_numpy(np.random.randint(0,2,size=(10,2)))\nx.pivot(on=\"column_1\",index=\"column_0\",values=\"column_0\",aggregate_function=\"len\")\n</code></pre> <p>The cells in the pivot table sum to n.</p> <p>It is again possible to express the frequencies as sum of \\(n\\) iid random matrices \\(A_1, \\dots, A_n\\). Each is of shape (a,b) matrix. If \\((X_1,Y_1)=(k,l)\\) then \\(A_{1,kl}=1\\) and 0 elsewhere. Again we have multinomial distribution with parameters \\((n, p_{ij})\\) with \\(p_{ij}=P[X_1=i, Y_1=j]\\). </p> <p>Except from a fancier way of indexing n values (using 2d array), the situation is exactly the same as before, so that when n get larger, we can approximate the statistic</p> \\[ C = \\sum_{1\\le i\\le a, 1\\le j\\le b} \\frac{(X_{ij}-n p_{ij})^2}{n p_{ij}} \\] <p>by a chisquare with \\(ab-1\\) degrees of freedom. </p> <p>In fact, a common setup for contingency tables is to assume independence between variables. In such case the matrix \\((p_{ij})\\) is rank 1 because \\(p_{ij}=r_i q_j\\) where \\(\\mathbf{r} = (r_1, \\dots, r_a)\\) and \\(\\mathbf{q} = (q_1, \\dots, q_b)\\) are marginal distributions of the pair \\((X_1,Y_1)\\).</p> <p>We claim that \\(C\\) is asymptotically chisquare with degrees of freedom \\((a-1)(b-1)\\). It suffices to show that the covariance matrix of \\((X_{ij})\\) (a projection as we have shown already), divided by \\(n\\),  is of rank \\((a-1)(b-1)\\). We can show this by investigating the null space of the covariance matrix (show it's of dimension \\(a+b-1\\)). We leave this as an exercise to the interested reader </p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/","title":"Monotonic binning with PAVA","text":"<p>In credit-risk modelling, a common requirement is to maintain monotonicity of the target variable with respect to risk drivers. Think PD modelling where the target is the probability of default and the risk drivers are customer characteristics and/or their financial situation.</p> <p>A useful technique for this is monotonic binning. The idea is to bin the continuous variable in some user-defined way (e.g., evenly-spaced breakpoints or quantile-based), then merge consecutive bins so that their default rates are monotonic.</p> <p>PAVA, or the pooled adjacent violators algorithm, is a classic method for solving optimization problems with monotonicity constraints, such as isotonic regression. In this post, we adapt its core idea to solve the monotonic-binning problem.</p> <p>We\u2019ve simplified the implementation as much as possible. What you see below is just the bare bones. Even though the code is short, it deserves some analysis for its complexity, which we\u2019ll get to next.</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#pava","title":"PAVA","text":"<p>We maintain two dynamic states:</p> <ul> <li>A list of pools that represents current clusters</li> <li>A list of values that stores the statistics (number of defaults and total in each cluster)</li> </ul> <p>We iteratively compare the current pool and the next one (based on their values):</p> <ul> <li>If the constraint is met, we advance by one index.</li> <li>If the constraint is violated, we merge the current and next pools and backtrack to check again. We keep merging and backtracking as needed until the constraint holds.</li> </ul> <p>That\u2019s it!</p> <pre><code>def pava(x, constraint=lambda a, b: a[0] / a[1] &lt;= b[0] / b[1]):\n    \"\"\"x : list of pairs of counts [(n_default, total), ...]\"\"\"\n    n = len(x)\n    active_pools = [[i] for i in range(n)]\n    active_values = list(np.asarray(x))\n\n    i = 0\n    while i &lt; len(active_pools) - 1:\n        if not constraint(active_values[i], active_values[i + 1]):\n            # Violation found, merge pools\n            active_pools[i].extend(active_pools[i + 1])\n            del active_pools[i + 1]\n            active_values[i] = active_values[i] + active_values[i + 1]\n            del active_values[i + 1]\n            # Step back to check for new violations with the merged pool\n            if i &gt; 0:\n                i -= 1\n        else:\n            i += 1\n    return active_pools\n</code></pre>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#complexity","title":"Complexity","text":"<p>Because the index can both advance and backtrack, it's not immediately obvious what the time complexity is.</p> <p>We can think of the process as alternating between two phases:</p> <ol> <li>A forward merge occurs, followed by backtracking and merging until monotonicity is restored.</li> <li>Or, if no merge is needed, the index keeps advancing by 1 until it encounters a violation that forces a merge.</li> </ol> <p>Index advancement happens at most n times, because merging doesn\u2019t affect how many positions remain ahead of the current index. Every time we advance, there are fewer spots left to advance into.</p> <p>Let \\(b_i\\) be the number of backtracks during the i-th forward-merge round. We can show:</p> \\[ \\sum_{i=1}^r b_i = r + \\sum_{i=1}^r (b_i - 1) \\le \\#\\{\\text{forward merges}\\} + \\#\\{\\text{backward merges}\\} \\le \\#\\{\\text{merges}\\} \\le n \\] <p>Since merges\u2014and thus backtracks\u2014occur at most n times and each merge is \\(O(1)\\), the overall complexity is \\(O(n)\\).</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#the-catch","title":"The catch","text":"<p>A sharp reader will notice that removing elements from a Python list (<code>del</code>) inside the loop is actually \\(O(n)\\), so the binning algorithm as written is \\(O(n^2)\\).</p> <p>To truly achieve \\(O(n)\\) complexity, you\u2019d need a data structure that supports \\(O(1)\\) element removal (like a linked list), which we\u2019re not using here for clarity.</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#bonus-stack-based-approach","title":"Bonus: stack-based approach","text":"<p>Since rewriting the function <code>pava</code> with a custom linked-list feels a bit obscure, I ask an AI (Gemini 2.5 pro) to find an alternative</p> <p>i feel that the code with linked link is harder to follow because of all the method calling swapping things around. is there a cleaner way?</p> <p>It\u2019s as short as the original but achieves \\(O(n)\\) without needing a linked list:</p> <pre><code>def pavai(x, constraint=lambda a, b: a[0] / a[1] &lt;= b[0] / b[1]):\n    \"\"\"\n    x: list of pairs of counts, e.g., [(numerator, denominator), ...]\n    \"\"\"\n    if not x:\n        return []\n\n    stack = []  # stores tuples of (value, indices) for each active pool\n\n    for i, val in enumerate(x):\n        current_value = np.asarray(val)\n        current_indices = [i]\n\n        # --- Backwards Merging ---\n        # While the stack isn\u2019t empty and the top pool violates the constraint\n        # with the current pool, merge them.\n        while stack and not constraint(stack[-1][0], current_value):\n            prev_value, prev_indices = stack.pop()\n            # Merge it into the current pool\n            current_value += prev_value\n            current_indices = prev_indices + current_indices\n\n        # Now that all violations are resolved, push the new pool onto the stack\n        stack.append((current_value, current_indices))\n\n    # --- Finalization: unpack the stack into the final list of pools ---\n    final_pools = [indices for value, indices in stack]\n\n    return final_pools\n</code></pre>","tags":["quant methods"]},{"location":"2025/08/28/a-patching-pattern/","title":"A patching pattern","text":"<p>When some functionality is not quite what you expected from a library, or you wanted to enhance it with some additonal features, a useful pattern is to patch them dynamically. </p> <p>Here are a few examples</p>","tags":["dev tools"]},{"location":"2025/08/28/a-patching-pattern/#example-1-add-a-lookup-method-to-dataframe","title":"Example 1: add a lookup method to DataFrame","text":"<p>As a cute utility function, we can add a lookup method to a DataFrame, which returns the column names that contains the queried string</p> <pre><code>import polars as pl\n\ndef lookup(self, name:str):\n    return [c for c in self.columns if name.lower() in c.lower()]\n\npl.DataFrame.lookup = lookup\n</code></pre> <p>and use it like so </p> <pre><code>df = pl.DataFrame({\"apple\":[1], \"pear\":[2]})\nassert df.lookup(\"app\") == [\"apple\"]\n</code></pre>","tags":["dev tools"]},{"location":"2025/08/28/a-patching-pattern/#example-2-change-the-formatting-of-ttestresult","title":"Example 2: change the formatting of TtestResult","text":"<p>We may want to display the result differently than the default e.g. with a theme or formatting.  We can overwrite the TtestResult class in the global scope, but here we do it with a context manager.    </p> <pre><code>import contextlib\nfrom scipy.stats._stats_py import TtestResult\n\n@contextlib.contextmanager\ndef add_html_repr_to_ttest():\n\n    original_repr_html = getattr(TtestResult, '_repr_html_', None)\n\n    def custom_repr_html(self):\n        ...\n        return ...\n\n    setattr(TtestResult, '_repr_html_', custom_repr_html)\n\n    try:\n        yield\n    finally:\n        if original_repr_html is None:\n            delattr(TtestResult, '_repr_html_')\n        else:\n            setattr(TtestResult, '_repr_html_', original_repr_html)\n</code></pre> <p>and use it like so</p> <pre><code>from scipy.stats import ttest_ind\n\nwith add_html_repr_to_ttest():\n    result = ttest_ind([1, 2, 3], [4, 5, 6])\n    display(result)  \n</code></pre>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/","title":"Cheatsheet for regular expression","text":"<p>Regex is a DSL with a very small vocabulary. It is so common that everyone has to deal with it, sooner or later. I have finally reached to the point that I want to know the exact rules instead of consulting an AI for even very simple regex patterns. So here we are, a cheatsheet.  </p> <p>In the most common cases, the goal is to retrieve a substring out of a given string that presents some pattern. Here are the tools at our disposal. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#anchors","title":"anchors","text":"<p>^ matches the start of the string and $ the end. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#characer-class","title":"characer class","text":"<p>Use square bracket [] inside which we put the character we want to match e.g. </p> <ul> <li>[abc] matches any of the three characters a,b,c</li> <li>[a-zA-Z] matches an alphabet lower/upper case</li> </ul>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#grouping","title":"grouping","text":"<p>Use round bracket () inside which we put the entire string we want to match e.g.</p> <p><pre><code>(awesome)\n</code></pre> matches exactly awesome</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#quantifiers","title":"quantifiers","text":"<p>To be composed with character class or grouping,</p> Quantifier Meaning * 0 or more + 1 or more ? 0 or 1 {n} Exactly n times {n,} n or more {n,m} Between n and m times <p>For example,</p> <p><pre><code>(go)+\n</code></pre>  would match go, gogo, gogogo etc</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#escape-with","title":"escape with <code>\\</code>","text":"<p>Quantifiers and backets are special characters that we may want to match too. To do this use backslash to escape like so </p> <pre><code>\\([0-9]?\n</code></pre> <p>to match an openning bracket followed by one or zero digits. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#shorthand","title":"shorthand","text":"Shorthand Meaning Matches \\d Digit [0-9] \\D Non-digit [^0-9] \\w Word character [a-zA-Z0-9_] \\W Non-word character [^a-zA-Z0-9_] \\s Whitespace character [ \\t\\r\\n\\f\\v] \\S Non-whitespace character [^ \\t\\r\\n\\f\\v] . Any character (except \\n)","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#or-with-negation-with","title":"OR with | ,   NEGATION with ^","text":"<p>(live|die) matches live, it also matches die. We already saw  the use of negation in the shorthand table.</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#example-match-the-endpoints-in-intervals","title":"Example: match the endpoints in intervals","text":"<p>We have strings like this</p> <pre><code>(-inf,1.1]\n(1.1,6]\n(6,inf]\n</code></pre> <p>We can retrieve the left endpoint like so  </p> <pre><code>import re\n\ns = \"(-inf, 1.1]\"\nmatch = re.match(r\"\\(([^,]+),\", s)\nif match:\n    print(match.group(1))\n</code></pre> <p>here <code>.group(1)</code> means we retrieve the first appearance of the grouping, which is what we want. </p> <p>In polars we can </p> <pre><code>import polars as pl\n\npattern = r\"\\(([^,]+),\"\npl.select(pl.col(\"interval\").str.extract(pattern))\n</code></pre> <p>where <code>.extract</code> method has the default <code>group=1</code>.</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#bonus-gpt4-split-pattern","title":"Bonus: GPT4 split pattern","text":"<p>Try this </p> <pre><code>GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n</code></pre> <p>This is used in the BPE algorithm as the initial step to split a large chunk of text into words, handing punctuation, spaces, unicode etc. </p> <p>Reference</p> <p>https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py</p>","tags":["dev tools"]},{"location":"2025/09/07/repurpose-hierarchical-clustering/","title":"Repurpose hierarchical clustering for feature engineering","text":"<p>A while ago, I wrote about hierarchical clustering with a focus on clean implementation of this classic method. </p> <p>In this short post, my goal is to repurpose it for consolidation of categorical features based on hypothesis testing. By consolidation I mean merge together values of a categorical variable which behave similarly with regard to the target variable (think default tag in credit risk modelling) in some statistical sense. </p> <p>The idea is simple: when we merge clusters in the bottom up fashion a la hierarchical clustering, we could use p-value as a \"distance\". Here distince is in quote because p-value should actually be viewed as a similarity measure: lower p-value would reject the null that two clusters have the same distribution, therefore suggesting a large distributional distance. On the other hand, higher p-value would suggest a low distributional distance. </p> <p>The most similar clusters would have the highest p-value, which we merge first. The clusering stops once p-values for all pairs of current clusters is lower than a pre-set threshold, say 0.05, indicating pairwise heteogeneity.</p> <p>Implementation is \\(\\varepsilon\\)-away from the original one. </p> <pre><code>import heapq\nimport numpy as np\nfrom scipy.stats import ks_2samp \n\ndef ks_test(x, y):\n    \"\"\"ks_2samp expects 1d arrays. OK if X is 1d to begin with.\n    we can implement a customized test depending on needs.  \n    \"\"\"\n    return ks_2samp(x, y).pvalue\n\ndef test_clustering(X, test_func=ks_test, alpha=0.05):\n    clusters = {i: [i] for i in range(len(X))}\n    heap = []\n    for i in clusters:\n        for j in clusters:\n            if i &lt; j:\n                p = test_func(clusters[i], clusters[j])\n                heapq.heappush(heap, (-p, i, j))\n    while heap:\n        negp, i, j = heapq.heappop(heap)\n        p = -negp\n        if i not in clusters or j not in clusters:\n            continue\n        if p &lt;= alpha:\n            break\n        clusters[i].extend(clusters[j])\n        del clusters[j]\n        for k in clusters:\n            if k != i:\n                p = test_func(X[clusters[i]], X[clusters[k]])\n                heapq.heappush(heap, (-p, i, k))\n    return list(clusters.values())\n</code></pre>","tags":["quant methods"]},{"location":"2025/10/31/jax-as-a-differential-calculus-tool/","title":"JAX as a differential calculus tool","text":"<p>Back in January, I wrote a tutorial about JAX, highlighting its power in high performance computing and its near-mathematical syntax. Now I show how to use JAX as a differential calculus tool for students and educators. </p>","tags":["low latency programming"]},{"location":"2025/10/31/jax-as-a-differential-calculus-tool/#taylor-expansion","title":"Taylor expansion","text":"<p>The goal is to approximate a differentiable function by a few terms in its Taylor expansion near a fixed point. The neat mathematical statement is </p> \\[ f(x) - f(x_0) = \\nabla f(x_0)(x-x_0) +  \\frac{1}{2}\\langle Hf(x_0) (x-x_0), (x-x_0)\\rangle + O(\\|x-x_0\\|^3) \\] <p>where \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) is sufficiently differentiable,  \\(\\nabla f\\) is the gradient of \\(f\\) and \\(Hf\\) is its Hessian matrix. </p> <p>Based on the expansion, we can use the linear term (first on the right) or the quadratic form (first two terms on the right) to approximate \\(f\\).  We can go further down the expansion too. In the code below we go down the quadratic route.  </p> <p>The key steps of the experiment</p> <ol> <li>define Ackley function</li> <li>define quadratic approximation of a generic function around an arbitrary point</li> <li>compute the diff between a function with its quadratic approximation</li> <li>evaluate the diff over a few scales.  </li> </ol> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\n\njax.config.update(\"jax_enable_x64\", True)\n\ndef f(x):\n    \"\"\"Ackley function\"\"\"\n    a = 20.0\n    b = 0.2\n    c = 2 * jnp.pi\n    d = x.size\n\n    sum_sq_term = -b * jnp.sqrt(jnp.sum(x**2) / d)\n    sum_cos_term = jnp.sum(jnp.cos(c * x)) / d\n\n    term1 = -a * jnp.exp(sum_sq_term)\n    term2 = -jnp.exp(sum_cos_term)\n\n    return term1 + term2 + a + jnp.exp(1.0)\n\ndef approx(f, xo):\n    df = jax.grad(f)\n    hf = jax.jacobian(df)\n    def Q(x):\n        return f(xo) + df(xo).dot(x-xo) +  0.5* hf(xo).dot(x-xo).dot(x-xo) \n    return Q\n\ndef diff(x,xo):\n    return approx(f,xo)(x) - f(x)\n\nd = 3\nxo, u = jr.normal(jr.key(0), (2,d))\nscales = jnp.logspace(-1,-5,5) # 1e-1, 1e-2, ... ,1e-5\nxs = xo + u * scales[:,None] # broadcast to (d,5)\n\nerrors = jax.vmap(diff, in_axes=(0,None))(xs, xo) # (5,) one for each scale\n\nprint(errors)\n</code></pre> <pre><code>[1.01747675e-03 1.03140167e-06 1.03225695e-09 1.03295150e-12\n 8.88178420e-16]\n</code></pre> <p>We observe that, roughly speaking, as we reduce the distance from x to xo by 10x we see an improvement of the approximation by 1000x, which  confirms the cubic term in Taylor expansion described above. </p>","tags":["low latency programming"]},{"location":"2025/10/31/jax-as-a-differential-calculus-tool/#chain-rule","title":"Chain rule","text":"<p>Consider \\(g:\\mathbb{R}^p\\to \\mathbb{R}^q, f:\\mathbb{R}^q \\to \\mathbb{R}^r\\) with sufficient differentiability. We have</p> \\[ J_{f\\circ g}(x) = J_f(g(x)) J_g(x) \\] <p>where \\(J_f, J_g\\) are the Jacobian matrices of \\(f,g\\) respectively. In the special case \\(q=1\\), we can write \\(J_f\\) as \\(\\nabla f\\) and can call <code>jax.grad</code> in place <code>jax.jacobian</code>, both producing a 1D array. </p> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\n\ndef g(x): \n    p = x.size\n    A = jr.normal(jr.key(1), (2,p))\n    return A.dot(x)\n\ndef f(x):\n    \"\"\"an elementwise activation function\"\"\"\n    return (1+jnp.exp(-x))**(-1)\n\ndef h(x):\n    return f(g(x))\n\nx = jr.normal(jr.key(99), 3)\n\nres = jnp.allclose(\n    jax.jacobian(h)(x), \n    jax.jacobian(f)(g(x)) @ jax.jacobian(g)(x)\n)\n\nprint(res)\n</code></pre> <pre><code>True\n</code></pre> <p>From the examples above you see how easy it is to use JAX for showcasing differential caculus results. We can imagine its use in solving differential equations or designing optimization algorithms. Check out JAX based projects <code>diffrax</code> and <code>optax</code> for those use case.</p>","tags":["low latency programming"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/","title":"A simple overview to EU banking regulations","text":"<p>courtesy of GPT5</p> <p>EU banking rules look messy because different bodies write different layers: global standards, EU legislation, technical rules, and supervisory practice. But once you stack them properly, the architecture becomes predictable.</p> <p>Here\u2019s the map you want in your head.</p> <p>Related post on the distinction between pillar 1 and pillar 2.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#1-basel-standards-the-global-blueprint","title":"1. Basel Standards: The Global Blueprint","text":"<p>Everything starts with the Basel Committee. Basel sets global rules for capital, liquidity, leverage, and risk management.</p> <p>Not law. Just the blueprint.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#2-crr-and-crd-the-eu-turns-basel-into-law","title":"2. CRR and CRD: The EU Turns Basel Into Law","text":"","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#crr-capital-requirements-regulation","title":"CRR \u2013 Capital Requirements Regulation","text":"<ul> <li>Directly applicable law.</li> <li>Covers the \u201cmath\u201d: RWAs, capital ratios, leverage, LCR/NSFR, large exposures, Pillar 3.</li> </ul>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#crd-capital-requirements-directive","title":"CRD \u2013 Capital Requirements Directive","text":"<ul> <li>Transposed by each member state.</li> <li>Covers governance, internal controls, fit &amp; proper, buffers, Pillar 2, supervisory powers.</li> </ul> <p>Together, CRR + CRD = EU\u2019s implementation of Basel III.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#3-eba-technical-details-and-uniformity","title":"3. EBA: Technical Details and Uniformity","text":"<p>CRR/CRD leave details open. The EBA tightens them with:</p> <ul> <li>RTS \u2013 binding technical standards</li> <li>ITS \u2013 binding templates/procedures</li> <li>Guidelines \u2013 supervisory expectations (\u201ccomply or explain\u201d)</li> </ul> <p>If CRR/CRD are the skeleton, EBA provides the exact measurement instructions.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#4-ecbssm-and-ncas-how-the-rules-are-enforced","title":"4. ECB/SSM and NCAs: How the Rules Are Enforced","text":"<p>Supervision happens here:</p> <ul> <li>ECB/SSM for significant institutions</li> <li>NCAs for smaller banks</li> </ul> <p>Their methodology is documented in:</p> <ul> <li>SSM Supervisory Manual</li> <li>SREP methodology booklets</li> <li>ECB ICAAP/ILAAP Guides</li> <li>Thematic reviews</li> </ul> <p>These are not laws; they\u2019re the supervisor\u2019s operating manual.</p> <p>This is where models get challenged, assumptions get questioned, and Pillar 2 capital gets set.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#5-where-specific-modelsrisks-sit-three-concrete-examples","title":"5. Where Specific Models/Risks Sit \u2014 Three Concrete Examples","text":"","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#example-1-irrbb-interest-rate-risk-in-the-banking-book","title":"\u2705 Example 1: IRRBB (Interest Rate Risk in the Banking Book)","text":"<p>IRRBB is Pillar 2.</p> <ul> <li>CRD: defines IRRBB as a Pillar 2 risk</li> <li>EBA Guidelines: modelling standards, shock scenarios, behavioural assumptions</li> <li>ECB SREP/ICAAP methodology: how supervisors score, challenge, and set P2R/P2G</li> </ul> <p>Placement: CRD \u2192 EBA Guidelines \u2192 ECB/SSM supervision (Outside CRR. Not a Pillar 1 capital formula.)</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#example-2-irb-models-internal-ratings-based-credit-risk-models","title":"\u2705 Example 2: IRB Models (Internal Ratings-Based credit risk models)","text":"<p>These are Pillar 1 internal models.</p> <ul> <li>CRR: legal basis and hard requirements</li> <li>EBA RTS/Guidelines: PD/LGD estimation, MoC, downturn LGD, validation</li> <li>ECB TRIM + ongoing model reviews: permissions, inspections, constraints, and monitoring</li> </ul> <p>Placement: CRR \u2192 EBA RTS/Guidelines \u2192 ECB TRIM/Supervisory review</p> <p>These directly impact regulatory capital.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#example-3-ifrs-9-expected-credit-loss-models","title":"\u2705 Example 3: IFRS 9 Expected Credit Loss Models","text":"<p>Different lane. IFRS 9 comes from accounting, not Basel.</p> <ul> <li>IASB \u2192 IFRS 9 standard: defines how lifetime PDs, scenarios, and ECL must be computed</li> <li>EU endorsement: makes IFRS 9 legally binding for reporting</li> <li>EBA/ECB expectations: supervisors check IFRS 9 model quality because it affects CET1, provisioning, stress tests, and ICAAP</li> </ul> <p>These are not Pillar 1 or Pillar 2 models in the CRR/CRD sense, but supervisors care because bad accounting leads to bad prudential numbers.</p> <p>Placement: IASB \u2192 EU endorsement \u2192 EBA/ECB supervisory expectations</p> <p>Parallel to prudential rules, but tightly connected.</p>","tags":["regulations"]},{"location":"2025/11/09/a-simple-overview-to-eu-banking-regulations/#6-the-whole-stack-in-one-line","title":"6. The Whole Stack in One Line","text":"<p>Basel \u2192 CRR/CRD \u2192 EBA RTS/ITS/Guidelines \u2192 ECB/NCAs (SREP, ICAAP, model reviews) and in parallel:</p> <p>IASB \u2192 EU-endorsed IFRS 9 \u2192 supervisory expectations</p> <p>Once this structure clicks, everything \u2014 IRRBB, IRB, IFRS 9 models, liquidity rules, buffers \u2014 falls neatly into place.</p>","tags":["regulations"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/","title":"A guide to convolution parameters in neural nets","text":"","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#origin","title":"Origin","text":"<p>Given a signal \\(f\\) and a kernel \\(g\\), say a probability density/mass function, the convolution \\(f*g\\) at point \\(x\\) measures  an \"average\" of \\(f\\) around the point \\(x\\), mathematically defined as </p> \\[ f*g = \\int f(x-y) g(y) \\mu(dy) \\] <p>Here \\(\\mu\\) is some reference measure on the ambient space, e.g. Lebesgue or counting measure on the Euclidean space. </p> <p>In deep learning, \\(g\\) is a learnable whereas in mathematics/engineering, the kernel is prescribed by the user. </p> <p>Different convolutions would invovle translations and scalings of the signal and the kernel.     </p>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#concepts-that-provide-a-granular-control","title":"Concepts that provide a granular control","text":"<p>There are quite a few knobs at our disposal.</p> <ul> <li>Stride is about how kernels slide.  <code>stride=k</code> means you would slide kernel <code>k</code> position at a time when you do the sum of product.</li> <li>Padding is about adding zeros to the boundary of the input. </li> <li>Input dilation is about adding zeros in between positions of the input, thereby inflating the input size by a factor. Input dilation is relevant when you up-sample a feature map.   </li> <li>Kernel dilation is about adding zeros in between positions of the kenrel, thereby expanding the reach of a kernel by a factor, while keeping the number of parameters unchanged. </li> </ul> <p>In the rest of the post, we see how to turn these knobs in <code>flax</code>, a neural network library built on top of <code>jax</code>.    </p>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#simplest-setup","title":"Simplest setup","text":"<pre><code>from flax import nnx\n\nrngs = nnx.Rngs(0)\nlayer = nnx.Conv(3, 4, (5,5), rngs=rngs)\n</code></pre> <p>We need to feed three positional arguments: the input channel, output channel, and the kernel size.  The length of the kernel size matches the number of spatial dimension of the convolution.  Here we use the word spatial in a loose sense. It can be 1d, 2d or more.  For instance, if each channel of the input represents the system's state at a timestamp and a  3d coordinate, the number of spatial dimension is 4 and so should provide a list of 4 integers for the kernel size.  </p> <p>Here we don't see the knobs because they take a sensible default.  </p> <pre><code>import jax.numpy as jnp\n\nlayer(jnp.ones((32,32, 3))).shape # (32,32,4) no batch dimension\nlayer(jnp.ones((1,32,32, 3))).shape # (1,32,32,4)\nlayer(jnp.ones((1,1,32,32, 3))).shape # (1,1,32,32,4)\n</code></pre> <p>The output shape tells a few things:</p> <ul> <li>it keeps the batch dimension as is. </li> <li>default padding is \"SAME\", refering to keep spatial dimensions the same.</li> <li>the last axis of input is the channel dimension, whose value should match the first parameter of the <code>nnx.Conv</code>.</li> </ul>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#padding-patterns","title":"Padding patterns","text":"<p>When we slide the kernel close to  the boundary, there is not enough token from the input to match the kernel length. Setting <code>padding=\"valid\"</code> respect this. The output spatial dimension is k-1 less where k is the kernel size. </p> <pre><code>layer = nnx.Conv(3,4,5, rngs=rngs, padding = \"valid\")\nlayer(jnp.ones((128, 100, 3))).shape  # (128, 96, 4)\n</code></pre> <p>We can provide padding parameter with a list, where each item of the list is either an integer (pad both sides the same way)  or a pair of integers (asymmetric). </p> <pre><code>layer = nnx.Conv(3,4,5, rngs=rngs, padding = [(4,0)])\nlayer(jnp.ones((100, 3))).shape  # (100, 4)\n</code></pre> <p>The level of control allows to implement the so-called causal convolution where the ouput at time i depends only on  the time i or before of the input sequence. </p> <p>There are other string choices of padding:  CIRCULAR identifies opposing boundaries the same way a torus is defined. REFLECT is like putting a mirror on the boudary so the value nearby is symmetric as is seen by the kernel. </p>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#kernel-dilation","title":"Kernel dilation","text":"<pre><code>layer = nnx.Conv(5,6, kernel_size= 7, rngs=rngs, padding=\"valid\", kernel_dilation = 2) \n\nlayer(jnp.ones((224,5))).shape    # (212,6)\n</code></pre> <p>To understand kernel dilation, some basic arithmetics are in place. Let's say dilation factor is d, and kernel size is k.  Then the dilation adds \\((k-1)(d-1)\\) zeros. The size of the dilated kernel is therefore \\(k+(k-1)(d-1)\\) and the  the output spatial dimension reduces by \\(k+(k-1)(d-1)-1 = (k-1)*d\\). </p>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#input-dilation","title":"Input dilation","text":"<pre><code>layer = nnx.Conv(5,6, kernel_size= 7, rngs=rngs, \n                input_dilation = 2,\n                padding = [(0,0)]  # str padding patterns not supported if input_dilation &gt; 1\n                )\n\nlayer(jnp.ones((224,5))).shape   # (441, 6)\n</code></pre> <p>Here dilation adds \\((T-1)(d-1)\\) zeros to the input sequence. The length of the dilated input is \\(T+(T-1)(d-1)\\) + pad.  The output spatial shape is T+(T-1)*(d-1) + pad - k + 1. </p>","tags":["dev tools"]},{"location":"2025/11/18/a-guide-to-convolution-parameters-in-neural-nets/#application-temporal-convolutional-network","title":"Application: Temporal Convolutional Network","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass TemporalBlock(nnx.Module):\n    \"\"\"\n    A single residual block for the TCN.\n    Structure: Input -&gt; (Dilated Conv -&gt; ReLU -&gt; Dropout) x2 -&gt; Add Residual -&gt; ReLU\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout, rngs):\n        # 'CAUSAL' is equivalent to [(dilation*(kernek_size-1),0)]\n        self.conv1 = nnx.Conv(\n            in_channels, out_channels, kernel_size, \n            strides=1, padding=\"CAUSAL\", kernel_dilation=dilation, rngs=rngs\n        )\n        self.conv2 = nnx.Conv(\n            out_channels, out_channels, kernel_size, \n            strides=1, padding=\"CAUSAL\", kernel_dilation=dilation, rngs=rngs\n        )\n        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n\n        # If input/output channels differ, we need a 1x1 conv to match dimensions for the residual add\n        self.downsample = (\n            nnx.Conv(in_channels, out_channels, kernel_size=1, rngs=rngs) \n            if in_channels != out_channels else None\n        )\n\n    def __call__(self, x):\n        residual = x\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        y = self.conv1(x)\n        y = nnx.relu(y)\n        y = self.dropout(y)\n        y = self.conv2(y)\n        y = nnx.relu(y)\n        y = self.dropout(y)\n\n        return nnx.relu(y + residual)\n\nclass TCN(nnx.Module):\n    def __init__(self, in_channels, num_channels, out_features, kernel_size=2, dropout=0.2, rngs=None):\n        \"\"\"\n        Args:\n            in_channels: Number of input features (C).\n            num_channels: List of hidden sizes for each layer (e.g. [25, 25, 25]).\n                          The length of this list determines the depth of the network before final decoder\n        \"\"\"\n        self.blocks = nnx.List()\n        num_levels = len(num_channels)\n\n        for i in range(num_levels):\n            dilation = 2 ** i  # 1, 2, 4, 8, ...\n            in_ch = in_channels if i == 0 else num_channels[i-1]\n            out_ch = num_channels[i]\n\n            block = TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout, rngs)\n            self.blocks.append(block)\n\n        self.decoder = nnx.Conv(num_channels[-1], out_features, kernel_size=1, rngs=rngs) # (B, T, last_hidden) -&gt; (B, T, 1)\n\n    def __call__(self, x):\n\n        x = nnx.Sequential(*self.blocks)(x)    \n        x = self.decoder(x)\n\n        return x\n</code></pre> <p>Usage:</p> <pre><code>model = TCN(5, [25,25,25], 1, 3, rngs=nnx.Rngs(1))\nB, T, C = 32, 1024, 5\nx_dummy = jnp.ones((B, T, C))\nmodel(x_dummy).shape   # (B, T, 1)\n</code></pre>","tags":["dev tools"]},{"location":"2025/11/20/high-performance-data-loading-with-grain/","title":"High performance data loading with grain","text":"<p>With the advent of Gemini 3 Pro and Nano Banana Pro, Google is clearly winning. </p> <p>At the same time, developper experience is getting better for people using Google's stack.  JAX is getting better at utilizing GPU; <code>flax.nnx</code> lets people write neural nets with similar syntax to pytorch. It might be a good time to keep up with this successful stack - from TPU, JAX, all the way to Gemini. </p> <p>Today I am going to talk about grain, a data loading library that works best with JAX.</p>","tags":["dev tools","low latency programming","data engineering"]},{"location":"2025/11/20/high-performance-data-loading-with-grain/#caveat-on-the-import","title":"Caveat on the import","text":"<p>At the time of writing, the version of grain is 0.2.14. We import like so:</p> <pre><code>import grain\n</code></pre> <p>Part of the official documentation uses a different path: <code>import grain.python as grain</code>.  Digging into the codebase shows that the latter exists for backwards compatibility. </p>","tags":["dev tools","low latency programming","data engineering"]},{"location":"2025/11/20/high-performance-data-loading-with-grain/#dataloader-api-is-verbose","title":"DataLoader API is verbose","text":"<p>Grain provides two APIs: the lower level functional API and the higher level imperative API. </p> <p>To use the imperative DataLoader API, you collect the data source, the transformations and the sampler to instantiate the <code>grain.DataLoader</code> class which returns an iterator that can be used in for loops.   </p> <pre><code>dataloader = grain.DataLoader(\n    data_source = data_source,\n    transformations = transformations,\n    sampler = sampler\n)\n</code></pre> <p>There are more to it, e.g. multiprocessing, prefetching, sharding etc but this is the gist of it.  I am not a fan of this because it is verbose and does not support data mixing.</p>","tags":["dev tools","low latency programming","data engineering"]},{"location":"2025/11/20/high-performance-data-loading-with-grain/#dataset-api-is-cooler","title":"Dataset API is cooler","text":"<p>To use the functional API, you basically point to the data source and start chaining a bunch of transformations to it. Lazy evaluation is used to avoid unnecessary computation. There are two main classes: <code>MapDataset</code> and <code>IterDataset</code>. Think of <code>MapDataset</code> as a sequence of elements because it supports indexing and len(), while <code>IterDataset</code> is an iterable of elements because it only supports iteration. </p> <p>Both are equipped with a bunch of methods to transform the data. e.g. </p> <ul> <li><code>map</code>: apply a function to each element</li> <li><code>random_map</code>: apply a function to each element with random arguments</li> <li><code>filter</code>: filter out elements that do not satisfy a condition</li> <li><code>batch</code>: batch elements</li> <li><code>seed</code>: set the random seed</li> <li><code>mix</code>: mix elements from multiple datasets</li> <li>...</li> </ul> <p>Methods that <code>MapDataset</code> have but <code>IterDataset</code> does NOT have are </p> <ul> <li><code>source</code>: create a dataset from a data source e.g. file, in memoery sequence, etc</li> <li><code>range</code>: create a range of elements</li> <li><code>repeat</code>: repeat the dataset</li> <li><code>shuffle</code>: this is global shuffle</li> <li><code>to_iter_dataset</code>: convert to <code>IterDataset</code></li> <li>...</li> </ul> <p>Methods that <code>IterDataset</code> have but <code>MapDataset</code> does NOT have are </p> <ul> <li><code>mp_prefetch</code>: prefetch the dataset</li> </ul> <p>In other words, the starting point has to be <code>MapDataset</code> and shuffling has to be done at the <code>MapDataset</code> level, while multiprocessing has to be done at the <code>IterDataset</code> level. To specify the multiprocessing options you need to cross the boundary from <code>MapDataset</code> to <code>IterDataset</code> with <code>to_iter_dataset</code> and then use <code>mp_prefetch</code>. Here is an example:</p> <pre><code>dataset = grain.MapDataset.range(100)\n    .shuffle(0)\n    .map(lambda elem:elem) # no op\n    .to_iter_dataset()\n    .batch(2, drop_remainder=True)\n    .mp_prefetch(\n        grain.multiprocessing.MultiprocessingOptions(\n            num_workers=4\n        )        \n    )\n</code></pre>","tags":["dev tools","low latency programming","data engineering"]},{"location":"2025/11/20/high-performance-data-loading-with-grain/#taking-advange-of-tpusgpus","title":"Taking advange of TPUs/GPUs","text":"<p>Google is generous enough to provide free instances of TPU and GPU on kaggle kernels. TPU has 224 CPU cores and  8 local TPU devices. GPU has 4 CPU cores and 2 local T4 devices. In both cases, there is only one host (single machine).  </p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec, NamedSharding\nimport grain\n\ndevices = jax.local_devices() # 8 TPUs or 2 GPUs\nmesh = Mesh(devices, axis_names=('data_axis',))\n\nGLOBAL_BATCH_SIZE = 128 * len(devices) \n\ndataset = (grain.MapDataset.range(10000)\n    .shuffle(0)\n    .repeat() # infinite\n    .map(lambda x: x) \n    .batch(GLOBAL_BATCH_SIZE) # &lt;--- ONE Big Batch, not list of batches\n    .to_iter_dataset()\n    .mp_prefetch(grain.multiprocessing.MultiprocessingOptions(num_workers=4))\n)\niterator = iter(dataset) # infinite\n\n# --- DEFINE SHARDING ---\ndata_sharding = NamedSharding(mesh, PartitionSpec('data_axis'))\n\n# --- JITTED STEP ---\n@jax.jit\ndef train_step(batch):\n    return jnp.sum(batch) * 2\n\n# --- TRAINING LOOP ---\nfor i in range(5): #  num of training steps here\n    host_batch = next(iterator)\n\n    device_batch = jax.device_put(host_batch, data_sharding)\n    loss = train_step(device_batch)\n\n    print(f\"Step {i}, Loss: {loss}, Device_batch_size:{device_batch.shape}, host_batch {host_batch.shape}\") # same shapes = GLOBAL_BATCH_SIZE\n    for i, shard in enumerate(device_batch.addressable_shards):\n        print(f\"Device {i} Physical Shape: {shard.data.shape}\") # see how each device has a shape 128\n</code></pre> <p>Reference</p> <p>https://developers.googleblog.com/en/building-high-performance-data-pipelines-with-grain-and-arrayrecord/</p> <p>https://google-grain.readthedocs.io/en/latest/grain.dataset.html#grain.MapDataset</p> <p>https://docs.cloud.google.com/tpu/docs/jax-ai-stack</p>","tags":["dev tools","low latency programming","data engineering"]},{"location":"2025/12/02/scalable-bootstrapping/","title":"Scalable bootstrapping","text":"<p>It is frustrating to see the out of memory error. It is equally frustrating to wait for hours before the computation finishes.  </p> <p>Sometimes these frustrations are inevitable, sometimes they are artificial. The goal of this post is to identify some cases  where we can avoid the frustration: run large scale bootstrap efficiently (no OOM and fast).    </p> <p>Consider the following situation: we resample with replacement a set of size one million from a set of the same size. We repeat 100k times.  If each invididual element takes 4 bytes (say Float32), we need 4e11 bytes, approximately 400GB to store all data in memory.  This is larger than RAM available on most consumer devices. Holding all these data in memory is not an option. </p> <p>However, it is rarely the case that we need to hold all these data. The purpose of bootstrap is to estimate the variability of  a handful of quantities across all replications (100k in our example). A reduction step would reduce that memory footprint of a  single replication to O(1). If we execute these reductions sequentially, we can avoid OOM. </p> <p>We use JAX to achieve this below. An important point here is that  <code>jax.lax.map</code> avoids using expensive python for loops, which makes the seqeuntial execution more efficient.  This runs almost instantly after the first compilation. </p> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\n\nkey = jr.key(0)\nseq = jr.choice(key, 2, (5000000,), p=jnp.array([95,5]))\n\ndef resample(key, seq):\n    return jr.choice(key, seq, shape=(len(seq),)).mean() # reduction op is the mean\n\n@jax.jit\ndef resample_all_sequential(keys, seq):\n    return jax.lax.map(lambda k: resample(k, seq), keys)\n\nresample_all_sequential(jr.split(key, 1000000), seq).shape \n</code></pre>","tags":["low latency programming","quant methods"]},{"location":"2025/12/02/scalable-bootstrapping/#side-note-why-not-vmap","title":"Side note: why not vmap?","text":"<p>In situation like bootstraping it is natural to use vmap and feed a large number of keys into the vmapped functions. This will not work because under the hood, JAX will allocate memoery for the whole data matrix even if we have a reduction at the end of each replication, which, as we mentioned before, is way too large. </p> <p>However, it is possible to take a hybrid approach. Use vmap to take 100 keys at a time, then splash a sequential map on top. I don't feel much of a difference for the scale I try. </p> <pre><code>@jax.jit\ndef resample_batched(keys, seq): \n    keys_reshaped = keys.reshape(-1, 100)  \n    return jax.lax.map(\n        jax.vmap(lambda k: resample(k, seq)),  # expect more than one key\n        keys_reshaped                          # a sequence of key groups, each group=100 keys\n    ).flatten()\n</code></pre>","tags":["low latency programming","quant methods"]},{"location":"2025/12/10/a-tale-of-convexity-in-credit-rating-assignment-problem/","title":"A tale of convexity in credit rating assignment problem","text":"<p>As I was exploring methodologies in credit rating assignment (say in the PD model),  an elegant approach caught my attention. Here is the formulation: </p> <p>given a collection of continuous scores and coresponding binary status (good or bad), find a partition of scores into bins that maximize the the information value (IV) aka the symmetrized KL divergence, subject to the constraints  </p> <ul> <li>monotonicity of the bad rates in the bins (e.g. default rate is increasing wrt score)  </li> <li>consecutive bins are statitically different (e.g. z test for binomials results in a p value &lt; 0.05)  </li> <li>upper and lower bounds for the final number of bins</li> </ul> <p>The IV is computed between the probability vectors \\((B_i/\\sum_j B_j)\\) and \\((G_i/\\sum_j G_j)\\), where \\(B_i\\) and \\(G_i\\) are the number of bad and good observations in bin \\(i\\).  </p> <p>A popular library for this task in the python ecosystem is <code>optbinning</code> which calls under the hood Google's OR-Tools. </p> <p>At first sight, this is a very hard combinatorial optimization problem. A first reduction of complexity is to pre-bin the scores  at a relatively granular level e.g. 100 bins by quantiles, then search for the optimal way of merging these pre-bins to achieve the best IV under constraints. This reduces the search space but the search problem is still a hard one.  </p> <p>A remarkable observation is the following </p> <p>merging two bins only decreases the IV</p> <p>This monotonicity makes the search problem tractable. If the IV in the current branch is lower than the best known IV value previously explored, this branch can be abandoned altogether. The exact implementation is still non trivial but luckily Google does it for us.  </p> <p>My main inerest here is to convince myself this remarkable property is true.  Mathematically, this is equivalent to the statement that </p> \\[ (x,y) \\mapsto (x-y) \\log \\frac{x}{y},   \\quad x, y&gt;0 \\] <p>is subadditive. To this end, we only need to show that  </p> \\[ g(x,y) = x \\log\\frac{x}{y} \\] <p>is subadditive. </p> <p>Note that \\(g\\) is homogeneous of degree 1, hence showing subadditivity of \\(g\\) is equivalent of showing the convexity of \\(g\\).  Indeed, if \\(g\\) were convex,</p> \\[ g(u + v) = 2 g((u+v)/2) \\le 2 (g(u)/2+g(v)/2) = g(u)+g(v).  \\] <p>Now it remains to show the convexity of \\(g\\). Here is the key lemma:</p> <p>Lemma</p> <p>Let \\(f:\\mathbb R^d-&gt;\\mathbb R\\) be a convext function and define \\(g:\\mathbb R^{d+1}\\to\\mathbb{R}\\) by </p> \\[ g(x,t) = t f(\\frac{x}{t}) \\quad t&gt;0. \\] <p>Then \\(g\\) is a convex function. </p> <p>We can apply the lemma with \\(f(x)=x\\log(x)\\) and note that \\(f\\) is a convex function. </p> <p>This lemma is striking. It allows us to claim convexity of multivariate functions from convexity of functions  on lower dimensional space. </p> <p>One can prove this lemma analytically but that does not offer much insight. A better explanation of why this lemma holds  is that the epigraph of \\(g\\) is the cone generated by the epigraph of \\(f\\). How? If we place ourselves at the origin and put the flat epigraph of \\(f\\) at height \\(t=1\\), we form a cone if we collect all the rays travelling from the origin through the  epigraph of \\(f\\). Note that \\(f(x)=g(x,1)\\) and sliding \\(t\\) up and down gives us a collection of scaled flat epigraph of \\(f\\),  the union of which is the cone.</p>","tags":["quant methods"]},{"location":"2025/12/14/hit-ecb-data-api/","title":"Hit ECB data API","text":"<p>ECB data portal is a wealth of macro data. Use cases include but not limited to scenario analysis, macro forecasting, and macro stress testing.</p> <p>The portal has a data API which allows users to programmatically access the data. To use it, users need to find the flow ref (like a data domain) and the key. </p> <p>This information can be found in the data portal UI. For example, the dataset \"MFIs lending margins on loans for house purchase, France, Monthly\" has the Series key: RAI.M.FR.LMGLHH.EUR.MIR.Z. The flow ref is RAI and the key is M.FR.LMGLHH.EUR.MIR.Z. </p> <p>We see the country code FR in the key. A nifty trick is to use the empty space (see code example below) to get all countries. One can also add another country using the OR operator. For example, M.FR+LU.LMGLHH.EUR.MIR.Z.</p> <p>Here is a utility function to fetch data:</p> <pre><code>import io\nimport requests\nimport polars as pl\n\ndef get_ecb_data(flow_ref, key, params=None):\n    base_url = \"https://data-api.ecb.europa.eu/service/data\"\n    request_url = f\"{base_url}/{flow_ref}/{key}\"\n\n    headers = {'Accept': 'text/csv'}\n    response = requests.get(request_url, params=params, headers=headers)\n    response.raise_for_status()\n\n    return pl.read_csv(io.BytesIO(response.content))\n\ndf = get_ecb_data(\n    flow_ref=\"RAI\", \n    key=\"M..LMGLHH.EUR.MIR.Z\", # use empty space to get all countries\n    params = {\n    'startPeriod': '2023-06',\n    'endPeriod': '2023-12',\n} # leave as None to get all time periods\n)\n</code></pre>","tags":["data engineering"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/","title":"A case study for AI assisted review of regulatory texts","text":"<p>This post records my attempt of using AI to help reviewing a large number of regulatory texts, in particular, in the context of risk model development and validation in a highly regulated environment. </p> <p>In such environment, model development and validation is the art of constraint optimization. Regulators and supervisors set the constraints, and institutions optimize their business metrics over the space of constraint solutions.</p> <p>Reading regulatory texts is a necessity in this context. Given the sheer amount of regulataions, directives, guidelines, technical standards, guides ..., it is hard (if not unrealistic) for anyone to read and remember all the contraints for the compliance of their risk mdoels.   </p> <p>My goal here is to lay out a workflow that uses AI to facilitate the process of retrieving the constraints, therefore allowing developers and validators spend more time on the interesting optimization part of their job. </p>","tags":["AI assistant","regulations"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/#keep-it-simple","title":"Keep it simple","text":"<p>For starters, just use the chat UI of your choice and a collection of prompts saved in a text file. When the workflow stabiliszes, it might worth developing an app tailored to the task and save a few clicks and copy paste, but not now. </p>","tags":["AI assistant","regulations"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/#initial-context","title":"Initial context","text":"<p>A good starting point is to hand over a 6-tuple to AI: </p> <ul> <li>jurisdiction: who rules</li> <li>risk type: broad category</li> <li>model component:  building block of the model e.g. parameter, sensitivity, scenario ... </li> <li>methodology: regulatory approach e.g. IRB, IMA ... </li> <li>exposure scope: which book/portfolio</li> <li>objective: usage</li> </ul> <p>For example, </p> <ul> <li>(EU, credit risk, PD, IRB, retail, pillar 1 capital)</li> <li>(EU, IRRBB, delta NII, SA, BB, pillar 2 SOT)</li> </ul> <p>narrows down the scope of search quite a bit. </p>","tags":["AI assistant","regulations"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/#break-down-constraints","title":"Break down constraints","text":"<p>Asking \"list all requirements for X per EBA\" and hoping to get a comprehensive full coverage is unrealistic. </p> <p>Regulatory requirements may be viewed as constraints on the space of parameter estimators. To help AI achieving full coverage, we can design orthogonal dimensions so that ideally every requirement can be described with one and only one dimension. </p> <p>Here is an attempt</p> <ol> <li>data standards: data quality, grouping, inclusion/exclusion, proxy data</li> <li>quantifications: formula, floor/cap, conservatism, assumption</li> <li>performance monitoring: backtesting, discriminatory power, stability, frequency</li> <li>governance: roles/responsabilities, processes</li> <li>documentation and reporting: templates/what should be included</li> </ol>","tags":["AI assistant","regulations"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/#high-level-process","title":"High level process","text":"<p>The idea is to retreive one bucket of requirements at a time. Then stress test it to verify completeness of the list and catch missing items. The test phase can be done via search for clarifications, construct counterexamples, JST role play etc. We can be creative here.  </p>","tags":["AI assistant","regulations"]},{"location":"2025/12/30/a-case-study-for-ai-assisted-review-of-regulatory-texts/#prompt-template","title":"Prompt template","text":"<p>Context: I am analyzing the following regulatory model:</p> <ul> <li>jurisdiction: EU (CRR)</li> <li>risk type: credit risk</li> <li>model component: PD</li> <li>methodology: IRB Advanced</li> <li>exposure scope: retail</li> <li>objective: pillar 1 capital</li> </ul> <p>Prompt 1. retrieval</p> <p>List CRR/EBA/ECB requirements specifically for the category: [INSERT CATEGORY HERE]. Cite article or paragraph. </p> <ul> <li>If Category = data standards: Look for length of history, data quality, grouping, inclusion/exclusion, proxy data</li> <li>If Category = quantifications: Look for formula, floor/cap, conservatism, assumption</li> <li>If Category = performance monitoring: Look for backtesting, discriminatory power, stability, frequency</li> <li>If Category = governance: Look for roles/responsabilities, processes</li> <li>If Category = documentation and reporting: Look for templates, inclusion of topics</li> </ul> <p>Prompt 2. clarifications</p> <p>\"Review the text for {Component}. Create a table with two columns: Explicit Rules: (Must do X). Implicit Expectations: (Should consider Y). Highlight any requirement where the regulation is vague (uses words like 'adequate', 'sufficient', 'appropriate') and explain how a strict regulator interprets that vagueness in the context of {Objective}.\"</p> <p>Prompt 3. devil's advocate</p> <p>\"I am proposing a simplified approach for {Component} to save costs. The Proposal: [Insert a 1-sentence logic, e.g., 'I will use 3 years of data instead of 5 because the market structure changed.'] Your Task: Act as a strict regulator (e.g., ECB/PRA). Even if this sounds logical from a business perspective, find the specific {Category} regulation that explicitly forbids this. Quote the article and explain why my 'reasonable' logic is non-compliant.\"</p> <p>Prompt 4. hidden constraint</p> <p>\"I have satisfied all the explicit mathematical formulas in the Methodology category. However, look at the Governance and Data categories. Are there any 'qualitative overrides' or 'human judgment' requirements that effectively force me to alter the mathematical output? Example: Does the regulation require a 'margin of conservatism' or a 'human overlay' that makes the pure math insufficient?\"</p> <p>Prompt 5. role play</p> <p>\"Adopt the persona of a lead investigator from the {Jurisdiction}. You are conducting an on-site inspection of my {Risk Stripe} model. Based on the regulatory texts you retrieved regarding {Category}, ask me the single most difficult question you would pose to the Head of Modeling to expose a weakness. Wait for my answer, and then evaluate if my defense holds up against the regulation.\"</p> <p>Prompt 6. timeliness</p> <p>\"Would any recent {Jurisdiction} updates (e.g., CRR III, Basel IV, or recent EBA/PRA Consultation Papers) invalidate or modify the requirements we just discussed for {Component}? Specifically, check for: Transitional Arrangements: Are we currently in a phase-in period where two rules apply? New RTS/GL: Are there recent Regulatory Technical Standards that clarify a previously vague article? Sunset Clauses: Has the permission to use this specific {Methodology} been withdrawn for this {Exposure Scope} (e.g., removal of Advanced IRB for certain asset classes)?\"</p>","tags":["AI assistant","regulations"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/","title":"A guide to preparing tax form in Luxembourg","text":"<p>It is this time of the year where I cannot delay any longer doing tax for the previous year.  This post is for my futre self, but it might be helpful to others similar to our situation. Let me know if there are inaccuracies. </p>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#a-simplified-mental-model","title":"A simplified mental model","text":"<p>The taxable income is the outcome of substracting some spending from most earning. The nuances lie in \"some\" and \"most\" of this sentence.  Doing tax optimally is the process of identifying all earnings that can be exempted and all spendings that can be deducted in the equation.   </p> <p>This simplied view relates taxable income to cashflows in and out of the household. It is not perfect because there are deductables not related to spending e.g. abattement conjoint (if both tax payers work), the 540 euro floor for frais d'obtention (even if you don't spend any). The case of AC is particularly annoying because it's really not about spending. But luckily it claims to be  automaticaly handled in the online tax form so tax payer can forget about it in their calculation.  </p> <p>Once the taxable income is calculated, one applies the marginal tax rate for that amount, then removes the tax credits to obtain the final owed tax. </p>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#benefit-of-class-2","title":"Benefit of class 2","text":"<p>Although a single tax class R is about to be legislated, the old class 2 will continue to exist for 20 years.  Let's briefly discuss what makes Luxembourg a family friendly place in relation to class 2.</p> <p>Let \\(R\\) denote the marginal tax rate function for an invidiual (class 1). This is a piecewise constant and non-deceasing function. Let \\(T\\) denote the tax function excluding credits i.e. \\(T(x) = \\int_0^x R(s) ds\\). It's a convex function because its derivative is non-decreasing. </p> <p>class 2's tax function excluding credits is given by </p> \\[ T_2(x) = 2 T(\\frac{x}{2}) \\] <p>where \\(x\\) is the taxable income of the household. By convexity, we have</p> \\[T_2(x+y) \\le T(x)+T(y)\\] <p>therefore, there is 0 benefits for a couple to pay tax separately. </p>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#earning","title":"Earning","text":"<p>The starting point is the gross income aggregated across multiple categories. Then removing all that can be exempted, such as </p> <ul> <li>overtime pay </li> <li>night sunday holiday preminums.</li> <li>bonus (part) </li> <li>meal voucher (part)</li> <li>relocation allowance (part)</li> <li>settlement after losing a job (cap)</li> <li>capital gain after holding more than 6 month  </li> </ul> <p>Article 115 of LIR is all about exemptions.   </p>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#spending","title":"Spending","text":"<p>Deductables are documented in multiple articles in LIR.  Social security in article 110.  Special expenses in article 111, 111bis. Extraordinary expenses in article 127, 127 bis. Professional expenses in article 107, 105. </p> <ul> <li>social security  </li> <li>commuting (cap)  </li> <li>frais d'obtention (floor)  </li> <li>second pillar pension (cap)  </li> <li>donation (?)</li> <li> <p>special expenses</p> <ul> <li>insurance premiums (cap)  </li> <li>debit interests of loans (cap)  </li> <li>third pillar pension plan (cap)  </li> <li>home savings plan (cap)  </li> <li>annuity payment (?)  </li> <li>personal contribution (?)  </li> </ul> </li> <li> <p>extraordinary expenses: unavoidable and reduces ability to pay tax  </p> <ul> <li>medical care not covered by CNS (condition)  </li> <li>dependency (cap)       </li> <li>disability (?)      </li> <li>case by case (?)  </li> </ul> </li> </ul>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#financial-investment","title":"Financial investment","text":"<p>In the case of financial investment the situation is a bit nuanced. If the funds pays officially a divident, then it cannot be exempted even if the funds offer the option of reinvesting dividend into the funds. </p> <p>At the time of selling a financial investment, capital gain must not be exempted if the tax payer holds the funds for less than 6 months.</p>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#memo-where-to-look-in-the-lir","title":"Memo \u2014 where to look in the LIR","text":"<p>Courtesy chatGPT and Gemini for this section. </p> <p>Purpose: a one-page map so you can jump to the right article when you need to check whether an item is deductible, exempt, subject to withholding, eligible for credit, etc.</p> <ul> <li>TITRE I \u2014 Imp\u00f4t sur le revenu des personnes physiques \u2014 Art. 1 \u2192 Art. 157 (approx.). All rules for individuals: definition of taxable income, categories (salaires, pensions, revenus mobiliers, loyers, b\u00e9n\u00e9fices commerciaux/agricoles/lib\u00e9rales), deductions, exemptions, credits, withholding, rules for non-r\u00e9sidents. ([Impots Directs][1])</li> <li>TITRE II \u2014 Imp\u00f4t sur le revenu des collectivit\u00e9s (soci\u00e9t\u00e9s, etc.) \u2014 Art. 158 \u2192 Art. 174 (rules that apply to corporate entities and the overlap with Title I where indicated). ([Impots Directs][1])</li> <li>TITRE III \u2014 Dispositions additionnelles et transitoires \u2014 Art. 175 \u2192 Art. 188 (misc transitional/technical provisions). ([Impots Directs][1])</li> </ul>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#chapter-level-pointers-inside-title-i-the-most-relevant-for-personal-deductionsexemptions","title":"Chapter-level pointers inside Title I (the most relevant for personal deductions/exemptions)","text":"<ul> <li> <p>Chapitre I \u2014 Disposition g\u00e9n\u00e9rale \u2014 Art. 1 (formal definitions / scope). </p> </li> <li> <p>Chapitre II \u2014 Personnes soumises \u00e0 l\u2019imp\u00f4t \u2014 Art. 2 (resident / non-resident definitions). </p> </li> <li> <p>Chapitre III \u2014 Imposition collective \u2014 Art. 3 \u2192 5 (married couples, joint assessment). </p> </li> <li> <p>Chapitre IV \u2014 Revenu imposable \u2014 Art. 6 \u2192 114 (sections &amp; subsections). This is the central chapter: it defines income categories (commercial, agricole, lib\u00e9rale, salaires, pensions, revenus mobiliers, loyers, revenus divers) and contains the rules about how to compute taxable income (principles, valuation, amortissement, d\u00e9duction des d\u00e9penses d\u2019obtention, etc.). If you\u2019re asking \u201cis X deductible?\u201d start here. </p> </li> <li> <p>Section I \u2014 G\u00e9n\u00e9ralit\u00e9s \u2014 Art. 6 \u2192 13.</p> </li> <li>1\u1d49\u02b3 sous-section \u2014 B\u00e9n\u00e9fice commercial \u2014 Art. 14 \u2192 55 (mode de d\u00e9termination, amortissements, etc.).</li> <li>2\u1d49 sous-section \u2014 B\u00e9n\u00e9fice agricole / forestier \u2014 Art. 61 \u2192 90 (\u2248).</li> <li>3\u1d49 sous-section \u2014 Professions lib\u00e9rales \u2014 Art. 91 \u2192 94.</li> <li>4\u1d49 \u2014 Salaires \u2014 Art. 95 \u2192 95a.</li> <li>5\u1d49 \u2014 Pensions / rentes \u2014 Art. 96 \u2192 96a.</li> <li>6\u1d49 \u2014 Revenus mobiliers \u2014 Art. 97 \u2192 ...</li> <li>7\u1d49 \u2014 Location (revenus fonciers) \u2014 Art. 98 \u2192 ...</li> <li>Section III \u2014 Dispositions communes sur frais d\u2019obtention, recettes, d\u00e9penses \u2014 Art. 103 \u2192 108 (frais d\u2019obtention, recettes et d\u00e9penses li\u00e9es aux cat\u00e9gories 4\u21928 de l\u2019art.10). </li> <li> <p>Section IV \u2014 D\u00e9penses sp\u00e9ciales \u2014 Art. 109 \u2192 114 (list of special deductible expenses, plafonds, conditions). Check here for personal deductible items (assurances, int\u00e9r\u00eats, frais de garde, etc.). </p> </li> <li> <p>Chapitre V \u2014 Exemptions \u2014 Art. 115 (main general exemptions; then details elsewhere). If you suspect an amount is tax-exempt (insurance payouts, certain capital receipts, etc.), check Art.115 and the following enumerated items. </p> </li> <li> <p>Chapitre VI \u2014 D\u00e9claration / \u00e9tablissement de l\u2019imp\u00f4t \u2014 Art. 116 \u2192 117 (procedural: filing, forms). </p> </li> <li> <p>Chapitre VII \u2014 Calcul de l\u2019imp\u00f4t &amp; mod\u00e9rations pour enfants \u2014 Art. 118 \u2192 ~123bis (allowances, child reductions). Use this for family/child tax moderation. </p> </li> <li> <p>Chapitre VIII \u2014 Recouvrement de l\u2019imp\u00f4t \u2014 Art. 135 \u2192 155 (advances, retenue \u00e0 la source on salaries and capital, payment procedures, tax credits like CIS, CI-CO2, etc.). Withholding / tax credits live here (e.g. retenue d\u2019imp\u00f4t salaries Art.136\u2192145; retenue sur revenus de capitaux Art.146\u2192151; cr\u00e9dits d\u2019imp\u00f4t Art.152\u2192154ter etc.). </p> </li> <li> <p>Chapitre IX \u2014 Dispositions particuli\u00e8res non-r\u00e9sidents \u2014 Art. 156 \u2192 157ter (non-resident taxation rules). </p> </li> </ul>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#cheat-sheet-topic-where-to-look-first","title":"Cheat-sheet: topic \u2192 where to look first","text":"<ul> <li>\u201cIs this type of receipt a taxable income category?\u201d \u2192 Chap. IV (Revenu imposable), start at the subsection for that category (salaires Art.95; pensions Art.96; revenus mobiliers Art.97; loyers Art.98; b\u00e9n\u00e9fices commerciaux Art.14+). </li> <li>\u201cIs it deductible as a business expense / amortissement?\u201d \u2192 Chap. IV, subsections for the relevant business type and Art.22\u201334 for valuation / amortissements; Section III (Art.103\u2013108) for frais d\u2019obtention. </li> <li>\u201cPersonal deductible expenses (insurance, alimony, childcare, interest on loans, pension contributions)?\u201d \u2192 Section IV \u2014 D\u00e9penses sp\u00e9ciales (Art.109\u2013114), then cross-check with relevant specific articles. </li> <li>\u201cExemptions (what is explicitly tax-exempt)?\u201d \u2192 Chap. V \u2014 Art.115 (and linked enumerations later in the text). </li> <li>\u201cTax credits (child credit, minimum salary credit, CO\u2082, parental, etc.)?\u201d \u2192 Chap. VIII / Art.152 \u2192 Art.154ter (and nearby articles for variants). ([Impots Directs][1])</li> <li>\u201cWithholding at source on salary?\u201d \u2192 Chap. VIII, Section II \u2014 Art.136 \u2192 145. </li> <li>\u201cWithholding on capital income (dividends, interest)?\u201d \u2192 Chap. VIII, Section III \u2014 Art.146 \u2192 151. </li> <li>\u201cHow do I file / deadlines / assessment?\u201d \u2192 Chap. VI \u2014 Art.116\u2013117 (declaration rules). </li> <li>\u201cNon-resident special rules (partnerships, source taxation)?\u201d \u2192 Chap. IX \u2014 Art.156\u2192157ter. </li> </ul>","tags":["regulations","life hack"]},{"location":"2025/12/23/a-guide-to-preparing-tax-form-in-luxembourg/#quick-tip-for-speed","title":"Quick tip for speed","text":"<ol> <li>If it\u2019s about whether something is taxable (category) \u2192 open Chap. IV and go to the subsection for that category.</li> <li>If it\u2019s about whether it\u2019s deductible \u2192 check Art.103\u2013108 (frais d\u2019obtention) and Art.109\u2013114 (d\u00e9penses sp\u00e9ciales), and the amortissement rules (Art.29\u219234).</li> <li>If it\u2019s about exemption \u2192 jump to Art.115 first, then follow the cross-references.</li> <li>If it\u2019s about withholding or credits \u2192 use Chap. VIII (Art.136\u2192155; credits around Art.152\u2013154). </li> </ol>","tags":["regulations","life hack"]},{"location":"2025/12/31/2026-resolution/","title":"2026 Resolution","text":"","tags":["life hack"]},{"location":"2025/12/31/2026-resolution/#outlive-plan-per-attias-book","title":"outlive plan per Attia's book","text":"Day Protocol Intensity / Metric \u201cOutlive\u201d Rationale Mon Strength (Lower) 1.5 \u00d7 Bodyweight Squat (Target) Leg strength is the #1 predictor of stability in old age. Tue Zone 2 Run 45\u201360 min @ ~7:15 min/km Builds mitochondrial density to prevent Type 2 Diabetes. Wed Strength (Upper) 2 min Dead Hang / Max Pushups Grip strength is a highly correlated proxy for brain health. Thu Zone 2 Run 45\u201360 min @ ~7:15 min/km Trains your body to oxidize fat rather than just glucose. Fri VO\u2082 Max (4\u00d74) 4 min Hard / 4 min Easy (\u00d74) Moving from 25th to 95th percentile = ~5\u00d7 mortality risk reduction. Sat Long Ruck / Run 90 min Zone 2 (with 10\u201315 kg pack) Rucking builds \u201cChassis\u201d (bone density) and \u201cEngine\u201d simultaneously. Sun Stability 20 min DNS / Balance Drills Prevents the \u201cFinal Fall\u201d: 50% of 80-year-olds die within a year of a hip fracture. <p>high protein + omega3 EPA/DHA + 7.5 hours sleep</p> <p>keep a diary here</p>","tags":["life hack"]},{"location":"2026/02/03/design-an-irb-model-validation-library/","title":"Design an IRB model validation library","text":"","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#what-is-the-status-quo","title":"What is the status quo","text":"<p>A common workflow for validation of risk models is to write a bunch of functions for the relevant statistical tests (often offloading to scipy/statsmodels/sklearn/arch), save them in a py file, write an orchestration notebook that reads the data, runs a collection of functions and save the charts/spreadsheets somewhere. </p> <p>Possible complaints about this workflow include:</p> <ul> <li>passing around many dataframes from functions to functions can be painful</li> <li>having to create many intermediate dataframes in memory can feel repetitive and ressource consuming</li> <li>hard to see what are being tested at a glance</li> </ul>","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#move-from-calling-functions-to-building-a-report-object","title":"Move from calling functions to building a report object","text":"<p>The core design is to create a dataframe interface which allows users to specify the tests in one go. To be more precise, the library</p> <ul> <li>registers a new namespace <code>irb</code> on the <code>polars.LazyFrame</code> class;</li> <li>provides a unifided configuration interface for all PD, LGD, CCF models by calling <code>polars.LazyFrame.irb.configure(id_col=\"obligor_id\",score_col=\"score\", ...)</code>, which in turn creates an empty <code>Report</code> object;</li> <li>offers a fluent builder API for the <code>Report</code> class, allowing users to chain <code>.check_X().check_Y()</code>;</li> <li>The <code>Report</code> object is just a queue of checks which are not executed until the user calls <code>.show()</code> (pretty html report for notebook) or <code>.run()</code> (for manual inspection of specific tables/charts in the report). </li> </ul> <p>The choice of <code>polars</code> over <code>pandas</code> fits the lazy execution philosophy here. But obviously this does not prevent pandas users from using it. Here is an example:  </p> <pre><code>import pandas as pd\nimport polars as pl\n\ndf: pd.DataFrame\nlf = pl.from_pandas(df).lazy()\nreport = lf.irb.configure(...)\n(\n    report\n    .check_x()\n    .check_y()\n    .add_samples(...)\n    .check_representativeness(versus=SAMPLE, variables= ...)\n    .show()\n)\n</code></pre>","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#comments-on-the-use-of-ai-coding-tools","title":"Comments on the use of AI coding tools","text":"<p>We are in an era where LLMs write most of the code. In this instance, I mostly outlined the API design; reported bugs; insisted on being minimalist. I don't know how fast are we heading into an era where humans are not necessary even for design. </p> <p>Personally, I don't feel comfortable with generating too many lines of code in one go becaues the assumptions made by LLM along the way can be misaligned with my intent. Also since I still read all the code LLM generates, I prefer let AI generate a small chunk of code at a time to keep my sanity. It's better for detecing early the divergence from my intent/design choice which are not provided in the initial prompt. Perhaps this is just a matter of personal preference.</p>","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#a-few-implementation-details","title":"A few implementation details","text":"","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#class-diagram","title":"class diagram","text":"","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#register-a-new-namespace","title":"register a new namespace","text":"<pre><code>@pl.api.register_lazyframe_namespace(\"irb\")\nclass IRBAccessor:\n    def __init__(self, lf: pl.LazyFrame):\n        self._lf = lf\n\n    def configure(self, **kwargs) -&gt; Report:\n        return Report(self._lf, IRBConfig(**kwargs))\n</code></pre>","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#unified-config-for-pd-lgd-ccf","title":"unified config for PD LGD CCF","text":"<pre><code>@dataclass(frozen=True)\nclass IRBConfig:\n    # Metadata\n    id_col: str | None = None  # obligor or facility\n    date_col: str | None = None\n\n    # PD\n    default_col: str | None = None\n    score_col: str | None = None\n    grade_col: str | None = None\n    pd_col: str | None = None\n\n    # LGD\n    ...\n</code></pre> <p>User calls  <code>df.configure(score_col=\"score\", default_col=\"default\")</code> to overwrite the default values (None).</p>","tags":["dev tools","AI assistant"]},{"location":"2026/02/03/design-an-irb-model-validation-library/#report-class","title":"Report class","text":"<p>The builder always returns a new Report object for immutability. It is cheap to create them because a lazyframe is just a query plan + reference to data source, and the checks are essentially callables. For instance, below <code>RepresentativenessCheck</code> is a wrapper around <code>psi</code> function which computes the PSI between two categorical columns.    </p> <pre><code>class Report:\n    def __init__(\n        self,\n        lf: pl.LazyFrame,\n        config: IRBConfig,\n        checks: list[Check] = None,\n        samples: dict[str, pl.LazyFrame] = None\n    ):\n        self._lf = lf\n        self._config = config\n        self._checks = checks or []\n        self._samples = samples or {}\n\n    def add_samples(self, **samples: pl.LazyFrame) -&gt; Self:\n        \"\"\"Returns a new Report containing the merged samples.\"\"\"\n        return Report(\n            self._lf,\n            self._config,\n            self._checks,\n            {**self._samples, **samples}\n        )\n\n    def check_representativeness(self, versus: str, variables: list[str]) -&gt; Self:\n        check = RepresentativenessCheck(\n            target_lf=self._lf,\n            baseline_lf=self._samples[versus],\n            baseline_name=versus,\n            variables=variables\n        )\n\n        return Report(\n            self._lf,\n            self._config,\n            self._checks + [check],\n            self._samples\n        )\n</code></pre>","tags":["dev tools","AI assistant"]},{"location":"archive/2026/","title":"2026","text":""},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/til/","title":"TIL","text":""},{"location":"category/tutorial/","title":"Tutorial","text":""},{"location":"category/analysis/","title":"Analysis","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"page/3/","title":"Blog","text":""},{"location":"page/4/","title":"Blog","text":""},{"location":"page/5/","title":"Blog","text":""},{"location":"page/6/","title":"Blog","text":""},{"location":"page/7/","title":"Blog","text":""},{"location":"page/8/","title":"Blog","text":""},{"location":"page/9/","title":"Blog","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2025/page/3/","title":"2025","text":""},{"location":"archive/2025/page/4/","title":"2025","text":""},{"location":"archive/2025/page/5/","title":"2025","text":""},{"location":"archive/2025/page/6/","title":"2025","text":""},{"location":"archive/2025/page/7/","title":"2025","text":""},{"location":"archive/2025/page/8/","title":"2025","text":""},{"location":"category/analysis/page/2/","title":"Analysis","text":""},{"location":"category/analysis/page/3/","title":"Analysis","text":""},{"location":"category/til/page/2/","title":"TIL","text":""},{"location":"category/til/page/3/","title":"TIL","text":""},{"location":"category/til/page/4/","title":"TIL","text":""},{"location":"category/til/page/5/","title":"TIL","text":""},{"location":"category/tutorial/page/2/","title":"Tutorial","text":""}]}