{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":"<p>subscribe via RSS</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:basel-framework","title":"Basel framework","text":"<ul> <li>            Basel Framework          </li> <li>            Vasicek in Basel          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            LLM eval for my personal usage          </li> </ul>"},{"location":"tags/#tag:data-engineering","title":"data engineering","text":"<ul> <li>            A dimensional approach to data quality principles          </li> <li>            Essentials of polars for pandas experts          </li> <li>            Exploring a large database          </li> <li>            Generate test dataframe with polars (powered by hypothesis)          </li> <li>            Intro to Snowpark API          </li> <li>            Polars streaming tricks          </li> <li>            Puzzling query optimization behaviour and dtypes shrinking in polars          </li> <li>            SQLite: the absolute basics          </li> <li>            Use code when Dataiku's UI gets in the way          </li> <li>            Working with large datasets in Snowflake          </li> <li>            duckdb basics          </li> </ul>"},{"location":"tags/#tag:dev-tools","title":"dev tools","text":"<ul> <li>            A patching pattern          </li> <li>            Build a mortgate calculator with fasthtml          </li> <li>            Cheatsheet for regular expression          </li> <li>            Fork, merge and PR          </li> <li>            Logging in python          </li> <li>            Mini-tutorial for python packaging, release and publish          </li> <li>            Static site generator using fasthtml          </li> <li>            Technical writing with material for mkdocs          </li> <li>            Use code when Dataiku's UI gets in the way          </li> </ul>"},{"location":"tags/#tag:low-latency-programming","title":"low latency programming","text":"<ul> <li>            GPU puzzles annotated          </li> <li>            Intro to JAX          </li> <li>            JAX as a differential calculus tool          </li> </ul>"},{"location":"tags/#tag:mojo","title":"mojo","text":"<ul> <li>            GPU puzzles annotated          </li> </ul>"},{"location":"tags/#tag:quant-methods","title":"quant methods","text":"<ul> <li>            Hierarchical clustering with 36 LOC          </li> <li>            Monotonic binning with PAVA          </li> <li>            Random forest scaling mini-benchmark          </li> <li>            Repurpose hierarchical clustering for feature engineering          </li> <li>            The Cox survival guide: hazards, models and time          </li> <li>            Unpacking the k-1 in the chi-square test          </li> <li>            Unpacking the n-1 in the t test          </li> <li>            Vasicek in Basel          </li> </ul>"},{"location":"tags/#tag:research","title":"research","text":"<ul> <li>            Repurposing gitHub for mathematical research          </li> </ul>"},{"location":"2024/12/18/mkdocs-intro/","title":"Technical writing with material for mkdocs","text":"<p>Abstract</p> <p>Material for mkdocs is a documentation framework adopted by many popular python libraries e.g. this, this and this. The plugins and markdown extensions make writing rich technical content delightful e.g. maths, code, docstring parsers, and more. The main apeal of the project is that \"it's just markdown\" (with a bit of extended syntax). This tutorial provides a step by step intro to setting up a blog website (such as this one!) with this framework.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#credit","title":"Credit","text":"<p>template by material team</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#first-steps-to-follow","title":"First steps to follow","text":"<ol> <li><code>pip install mkdocs-material</code></li> <li>initialize docs locally with <code>mkdocs new .</code></li> <li>create empty remote repo and push local to remote</li> <li>[github] add CI config file as per material docs</li> <li>[github] set publish branch to <code>gh-pages</code></li> </ol> <p>After these steps, a site is published and CI should start working. New edits of docs in the main branch would trigger rebuild of the site, i.e. focus on the writing and let robots build the site automatically.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#customize-the-theme","title":"Customize the theme","text":"<p>In <code>mkdocs.yml</code>, config the theme, then customize it e.g. add dark/light toggle</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-blog-plugin","title":"Add blog plugin","text":"<p>Without any setting, the plugin creates a directory structure (<code>/docs/blog/posts</code>).</p> <p>Setting the following should be straightforward</p> <ol> <li>blog_toc</li> <li>archive_date_format</li> <li>categories_allowed</li> <li>pagination_per_page</li> </ol> <p>The <code>post_slugify</code> setting makes use of python markdown extension package, which is a dependency of <code>material</code> and installed with it.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-tags-plugin","title":"Add tags plugin","text":"<p>Add <code>tags</code> plugin in <code>mkdocs.yml</code> and create a file <code>tags.md</code> in <code>docs</code>.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#add-rss-plugin","title":"Add rss plugin","text":"<p>rss is a third party plugin which requires installation.</p> <p>It is important that have the site_name, site_description and site_url settings configured. The RSS plugin makes use of this information to construct the feed, so make sure you have configured them.</p> <ul> <li><code>pip install mkdocs-rss-plugin</code></li> <li>add rss plugin in  <code>mkdocs.yml</code></li> <li>add the required package also in ci: run <code>pip install mkdocs-rss-plugin</code></li> </ul>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#extra","title":"Extra","text":"<p>such as social media links in the footer</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#markdown-extension-maths","title":"Markdown extension: maths","text":"<p>Add markdown extension for writing maths</p> <ul> <li>add <code>arithmatex</code> extension in <code>mkdocs.yml</code></li> <li>add <code>mathjax.js</code> to extra_javascript (create a js file in <code>docs/js</code>, define macros as needed)</li> </ul> <p>One can use <code>katex</code> instead per documentation.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#markdown-extension-code-block","title":"Markdown extension: code block","text":"<p>Add necessary markdown extension for writing code.</p> <ul> <li>highlight</li> <li>inlinehilite</li> <li>snippets</li> <li>superfences</li> </ul>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#nav","title":"Nav","text":"<p>Rename nav sidebar and/or turn it into tabs.</p> <p>Add to <code>features</code> in <code>theme</code></p> <ul> <li>navigation.tabs : tabs</li> <li>navigation.indexes : index attached to sections (overview page)</li> </ul> <p>Also add <code>nav</code> section to be explicit what to include in the sidebar/tabs.</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#author","title":"Author","text":"<p>Add author metadata <code>docs/blog/.author.yml</code> and use it in all posts (by specifying <code>authors</code> in the header of posts).</p>","tags":["dev tools"]},{"location":"2024/12/18/mkdocs-intro/#metadata-of-posts","title":"Metadata of posts","text":"<p>Include metadata in the header of posts:</p> <ul> <li>date (enough)</li> <li>authors</li> <li>tags</li> <li>categories</li> <li>slug (if want to customize)</li> <li>readtime</li> </ul>","tags":["dev tools"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/","title":"Essentials of <code>polars</code> for <code>pandas</code> experts","text":"<p>Abstract</p> <p><code>pandas</code> is a standard tool for every data professional, although it does not scale well in production. Yet, being a standard is a strategic position to be, as libraries coming to solve the scale issue tend to meet data professionals where they are, by mimicing the <code>pandas</code> API (think: <code>dask</code>, <code>pyspark.pandas</code>).</p> <p><code>polars</code> is a new-ish tool that is probably replacing <code>pandas</code> at the time of writing. The goal of this post is to introduce the kind of mindset change needed to fully exploit <code>polars</code> in production.</p> <p>What does a dataframe library do? A few things came to mind.</p> <ul> <li>merge/join</li> <li>group by</li> <li>aggregate</li> <li>windows function</li> <li>rolling windows</li> <li>etc</li> </ul> <p>There are a plethora of dataframe libraries doing all these things. Yet <code>polars</code> seems to me a clear winner in the game of \"finding the successor of <code>pandas</code>\". </p> Update Feb 2025 <p><code>polars</code> may well replace <code>pyspark</code> with their annoucement  of the cloud offering for vertical and horizontal scaling. Indeed, <code>polars</code> solves the same scaling problem as <code>pyspark</code> does and its API is very close to <code>pyspark.sql</code> although their implementations are very different.</p> <p>Born in 2020, <code>polars</code> released its version 1.0 in mid-2024, officially marking its production readiness. A popular saying about <code>polars</code> is that people \"came for the speed, stay for the syntax\". This pronounces two ways in which <code>polars</code> is awesome</p> <ul> <li>fast</li> <li>elegant syntax: both intuitve and expressive.</li> </ul> <p>The goal of this tutorial is to present some basic concetps for effective use of the library. But first, let's briefly mention</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#why-polars-is-fast-in-a-nutshell","title":"Why polars is fast in a nutshell","text":"<p>The speed is achieved by lazy execution, query optimizer, vectorized query engine, parallelism. It would be too arrogant of me to claim that I can explain all these terms with complete precision.  The interested reader is invited to watch talks<sup>1</sup> <sup>2</sup> <sup>3</sup> by the creator of the library Ritchie Vink for details.</p> <p>In a nutshell,</p> <ul> <li>Lazy execution builds the computation graph of data transformations without loading any data into memory. Think of this as composition of functions where no input is required (the schema of the input must be known though).</li> <li>Query optimizer optimizes user's query, making them more efficient. The laziness leaves the room for optimising user's query e.g. changing the order of certain operations, fusing them, and all sorts of smart tricks that can boost the computation efficiency before any data is loaded. This is similar to  what machine learnig compilers would do (think <code>torch.compile</code> and <code>jax.jit</code>).</li> <li>Vectorized query engine leverages columnar memory format (<code>arrow</code>) and hardware optimizations (e.g. SIMD) for array manipulations.</li> <li>Parallelism is a paradigm to distribute computation workloads effectively across all the CPU/GPU cores.</li> </ul>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#eager-vs-lazy","title":"Eager vs Lazy","text":"<p>Eager is the \"opposite\" of lazy. In eager mode, data is loaded into memory before the first operation, and all operations are executed sequentially, one after another as user's query. This can be both wasteful and inefficient.</p> <p><code>pandas</code> only operates in eager mode, while <code>polars</code> operates in eager/lazy as per user's needs. The lazy API of <code>polars</code> is almost identical to the eager one so there is little mental overhead to users.</p> <p>Lazy is good for speed, but this does not mean that Eager is useless. Data professionals rarely build data pipelines in one go, rahter, they build iteratively, one step at a time. This is where eager mode shines.</p> Example <p>Think about this query in the diagram where the input <code>in</code> is processed in some ways to achieve the result <code>out</code>.</p> <p><pre><code>graph LR\n    A[in] --&gt; B[result1];\n    A --&gt; C[result2];\n    A --&gt; D[result3];\n    C --&gt; D;\n    B --&gt; E[out];\n    D --&gt; E;</code></pre> With the eager API, result2, result1 must be computed fully and stored in memory, then result3, before producing the output. In developement, it indeed makes sense to produce all these intermediate results and stored them in memory to check the accuracy of the results. However, it can be the case that the <code>out</code> is what really matters in production, and the intermediate results are just implementation details. Lazy execution would be the way to go in such cases. With the lazy API, the query is optimized, vectorized, multi-threaded so the execution can be ~10x faster.</p> <p>Concretely, the data structure to operate on with the eager API is called <code>DataFrame</code>, and for the lazy API it is called <code>LazyFrame</code>.</p> <p>There is one caveat/gotcha when working with lazy API. One might use blindly all data operations that are available in the eager API and surprised that they are not availabe in the lazy API.  To understand why this happens, it is crucial to know that <code>LazyFrame</code> must be agnostic of the data values and it must be aware of the schema. The schema of the output of <code>pivot</code> operation can NOT be determined if the data values are not known (how many columns are there?). It is the same story with <code>transpose</code>.</p> <p>Once the query reaches a point where lazy API cannot do what the user wants, it is time to switch to the eager API, do that operation eagerly, and switch back to lazy for the speed.</p> <p>In code, the pattern looks like this</p> <pre><code>lf.collect().pivot(...).lazy().other_lazy_operations()\n</code></pre> <p>Here <code>.collect()</code> would turn a <code>LazyFrame</code> into <code>DataFrame</code> (so <code>lf</code> is materialized/stored in memory). After the <code>pivot</code> operation, <code>.lazy()</code> would turn the dataframe back to <code>LazyFrame</code>.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#expression","title":"Expression","text":"<p><code>polars</code> offers a beloved Expression API. In essence, expressions are functions that associate an ouput array to an input array (think <code>numpy</code> operators). As operators, they can/should be isolated from data (Frame/Series). Expression can be appiled to an existing Expression to obtain a new Expression in the sense of function composition. This allows users to build complicated queries by composing building blocks offered in the Expression API.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#syntax-difference","title":"Syntax difference","text":"<p>The migration guide from official docs serves as a good summary<sup>4</sup>.</p>","tags":["data engineering"]},{"location":"2024/12/21/essentials-of-polars-for-pandas-experts/#go-further","title":"Go further","text":"<p>To make full advantage of <code>polars</code>, using lazy execution whenever possible and getting familiar with Expression are probably sufficient for most of the everyday data processing jobs.</p> <p>To go further, the official User Guide is the best source, which is being revised and improved actively (~ end of 2024).</p> <ol> <li> <p>https://www.youtube.com/watch?v=GOOYbl3cqlc&amp;ab_channel=PyConLithuania \u21a9</p> </li> <li> <p>https://www.youtube.com/watch?v=ubqF0yGyphU&amp;t=3556s&amp;ab_channel=SuperDataScience%3AML%26AIPodcastwithJonKrohn \u21a9</p> </li> <li> <p>https://www.youtube.com/watch?v=yYAVrVMGaMY&amp;ab_channel=PyData \u21a9</p> </li> <li> <p>https://docs.pola.rs/user-guide/migration/pandas/#key-syntax-differences \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/","title":"Mini-tutorial for python packaging, release and publish","text":"<p>Abstract</p> <p>This mini-tutorial is a beginner's cheatsheet to python packaging. Check Python packaing user guide for an authoritative guidance on the topic.  </p>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#classical-way","title":"Classical way","text":"<p>It is good practice to setup an isolated and clean environment e.g. with standard library <code>venv</code>. After that, </p> <ul> <li> <p>install packages for building wheels and source distributions:</p> <pre><code>pip install wheel build\n</code></pre> </li> <li> <p>create <code>setup.py</code> where one can specify the requirements and meta-data:  </p> <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name='my_package',\n    version='0.1.0',\n    packages=find_packages(),\n    install_requires=[\n        'numpy',\n        'pandas',\n    ],\n    # Optional: Add more metadata\n)\n</code></pre> </li> <li> <p>actually create wheels and source distribution:</p> <pre><code>python -m build\n</code></pre> </li> </ul>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#alternatively-with-uv","title":"Alternatively, with <code>uv</code>","text":"<p><code>uv</code> is a modern python dev tool, see features and  install guide.</p> <p>It is compliant with PEP 517 <sup>1</sup>, PEP 518 <sup>2</sup>. </p> <p>Use the project interface of <code>uv</code> to init project and add dependencies. </p> <pre><code>uv init\nuv add DEPENDENCIES\n</code></pre> <p>then </p> <pre><code>uv build\n</code></pre>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#create-a-tag-and-release","title":"Create a tag and release","text":"<pre><code>git tag -a v0.1.0 -m 'release message'\ngit push --tags\n</code></pre> <p>This will create a tag named \"v0.1.0\" with the message \"release message\". The distribuion files will be displayed as assets for the tag.</p>","tags":["dev tools"]},{"location":"2025/01/03/mini-tutorial-for-python-packaging-release-and-publish/#publish","title":"Publish","text":"<p>Publish to pypi index requires the developer to setup an account and get the API token. Using <code>uv</code>, run this</p> <pre><code>uv publish --token TOKEN\n</code></pre> <p>with the dev's API token in place of TOKEN. </p> <p>Useful</p> <p>https://pydevtools.com/handbook/</p> <ol> <li> <p>why <code>pyproject.toml</code>? https://peps.python.org/pep-0518/ \u21a9</p> </li> <li> <p>understand build frontend, build backend. https://peps.python.org/pep-0517 \u21a9</p> </li> </ol>","tags":["dev tools"]},{"location":"2025/01/12/intro-to-jax/","title":"Intro to JAX","text":"<p>Abstract</p> <p>Mutiple packages came to solve the speed issue of <code>python</code> in scientific computing. The de facto standard is of course <code>numpy</code>. Think also <code>numba</code>, <code>jax</code>, <code>torch</code> or even a new language  that go beyond what <code>numpy</code> offers.  This post is a quick intro to my personal preference as of Jan 2025. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#pytree","title":"PyTree","text":"<p>At the center of JAX is undoubtedly <code>jax.Array</code>.  A nested container of these objects is called a PyTree, e.g. lists of dicts of Array. The Array's are the leaves of the PyTree. It is possible to register a custom Pytree class.</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#numpy-operations","title":"Numpy operations","text":"<p><code>jax.numpy</code> module is easy to use for <code>numpy</code> users because they share near identical API, intentionally. Operations in <code>jax.numpy</code> are built on top of the XLA compiler primitives for high performance numerical computations. </p> <p>There is the distinction between abstract structure and concete value of arrays in jax. A <code>ShapeDtypeStruct</code> object captures dtype and shape, whereas <code>Array</code> carries the concrete values too.</p> <p>The caveat of using <code>jax.Array</code> is that they are immutable. The \"mutation\" syntax is different from <code>numpy</code> and actually generates new arrays instead of mutate them in place, in constrast to <code>numpy</code>. </p> <pre><code>import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\n\ny = x.at[1:].set(0)\nprint(y)\n\nz = x.at[1:].add(1)\nprint(z)\n</code></pre> <pre><code>[1 0 0]\n[1 3 4]\n</code></pre> <p>Broadcasting is arguably an elegant design of the numpy array API. It works the same for jax arrays. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#vmap","title":"<code>vmap</code>","text":"<p><code>vmap</code> vectorizes a function by adding a batch dimension at the leading axis to all the leave nodes of a function's arguments. In other words, vmap is like <code>np.stack</code> individual results of a sequence of function calls. We use <code>vmap</code> to avoid manual batching, manual stacking etc.  </p> <p>Since the arguments of the function at hand can be an arbitrary PyTree, the axis 0 will be the batch dimension for all arrays in each  PyTree, and the shape of the supplied arrays embedded in the PyTrees should be compatible with that.</p> <p>More control over the operating axes of <code>vmap</code> is possible. Below  <code>in_axes=(None,0)</code> adds batch dimension at the leading axis for the  second positional argument of the function only. If we do not specify <code>in_axes</code> (defaults to 0 or equivalently (0,0)), the vectorized function <code>vmap(f)</code> would add batch dimension to all leave nodes of the input PyTree's. This behaviour is not compatible with the inputs in our example, because beta.shape is (5,) and x.shape is (2,5), so their leading dimensions are not the same.</p> <pre><code>import jax\nimport jax.numpy as jnp\n\ndef li(beta, x):\n    \"\"\"linear predictor with multiple regressors\"\"\"\n    return beta.dot(x)\n\nbeta = jnp.arange(5)\n\njax.vmap(li, in_axes=(None,0))(\n    beta, \n    jr.normal(jr.key(1), (2,5))\n)\n</code></pre> <p>Additionally, we can specify <code>out_axes</code> along which we stack the results, to use our <code>np.stack</code> analogy again. But in our example above since the output is 1D array, we can specify nothing other than <code>out_axes=0</code>, which is the default already.  </p> <p>More details about vmap here</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#random-numbers","title":"Random numbers","text":"<p>Be aware that JAX follows the functional programming paradigm. This implies explicit key handling for samplers. Samplers can be composed with <code>vmap</code> to achieve vectorization across all parameters. </p> <p>For example,  <code>random.t</code> has two parameters key and df, one can supply one array of keys to generate a collection of t distributed random variables with the same degree of freedom like so </p> <pre><code>jax.vmap(random.t, in_axes=(0,None))(keys:Array, df=2)\n</code></pre> <p>or generate a collection each with a different degree of freedom like so </p> <pre><code># identical\njax.vmap(random.t)(keys:jax.Array, df:jax.Array)\njax.vmap(random.t, in_axes=0)(keys:jax.Array, df:jax.Array)\njax.vmap(random.t, in_axes=(0,0))(keys:jax.Array, df:jax.Array)\n</code></pre> <p>Here is an example of working with PRNG keys. The warning from the official docs is NEVER reuse a key. Split them.  </p> <pre><code>import jax\nimport jax.random as jr\nimport jax.numpy as jnp\n\nkey = jr.key(42)\nout1 = jr.normal(key, (10,))\nout2 = jax.vmap(jr.normal)(jr.split(key, 10)) # vmap adds batch dimension at leading axis by default\nout3 = jax.vmap(jr.t)(\n    key = jr.split(jr.key(21),10),\n    df = jnp.arange(10)+1 # degree of freedom of 10 t-distributions\n)\n\nprint(out1)\nprint(out2)\nprint(out3)\n</code></pre> <pre><code>[-0.02830462  0.46713185  0.29570296  0.15354592 -0.12403282  0.21692315\n -1.440879    0.7558599   0.52140963  0.9101704 ]\n[ 0.07592554  0.60576403  0.4323065  -0.2818947   0.6549178  -0.2166012\n -0.25440374  0.2886397   0.14384735 -1.3462586 ]\n[ 0.6342837   0.6981538   1.959329    0.35705897  0.95073795 -2.2646627\n  0.93203527  0.50947154  1.1138752  -0.03552625]\n</code></pre> <p>Again we provide <code>df</code> of type Array because JAX expects Array for vectorized arguments. Providing list (pytree!) of the same size wouldn't work. </p> <p>See details in the official docs. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#jit-if-you-can","title":"<code>jit</code> if you can","text":"<p>In eager mode, JAX transformations/opertors run sequentially one at a time. With <code>jit</code> compilation, the computation graph of a jax program is optimized (e.g. rearrange, fuse transformations) by XLA compiler so that it runs faster.</p> <p>The idea of <code>jit</code> compilation is to run/compile the program in python once, and cache the compiled program for repetitive evaluations. Compilation introduces overhead. So what we want is that inputs of the same dtype and shape would not trigger re-compilation. jax achieves this by tracing the dtype and shape of all operands in a computational graph, and optimize this abstract structure without having to know the exact values.  </p> <p>The approach has some implications. JAX transformations such as vmap must be agnostic of the values of the inputs, and they must know the shape and dtype of the inputs and outputs to comply with the XLA's requirement of being compile-time static. </p> <p>One should <code>jit</code> everything for speed but this is not always possible. Consider this.</p> <pre><code># NOT WORKING!\nfrom jax import jit\n\n@jit\ndef f(x):\n    if x &gt; 0:\n        return x\n    else:\n        return jnp.stack([x,x])\n\ntry: f(3)\nexcept Exception as e: print(e)\n</code></pre> <p>This function is not <code>jit</code>'able because at compile time the compiler cannot get the shape of the ouput without knowking the concete value of <code>x</code>. Of course one can get around the <code>if</code> statement with <code>jnp.where</code> which create branches in the computation graph. Conveniently, ALL <code>jax.numpy</code> operations are jittable.</p> <p>More broadly, none of the python control flows is jit compatible.  Consider this function </p> <pre><code>def xfibonacci(x, n):\n\n    a, b = 0, 1\n    for _ in range(1, n):\n        a, b = b, a + b\n\n    return x*b\n</code></pre> <p>We cannot jit compile this function because the number of iterations cannot be deduced from the shape and dtype of the input <code>n</code>. Jax needs to know the exact number of operations to include in the computational graph before compilation. </p> <p>There are two possible fix.  First, we can use <code>static_argnums</code> to make a positional argument static at  compile time i.e. treating <code>n</code> as constant. Effectively the loop is unrolled at compile time. </p> <pre><code>from jax import jit\nfrom functools import partial\n\n@partial(jit,static_argnums=1)\ndef xfibonacci(x, n):\n\n    a, b = 0, 1\n    for _ in range(1, n):\n        a, b = b, a + b\n\n    return x*b\n</code></pre> <p>Or we can use a JAX control flow primitive. </p> <pre><code>@jax.jit\ndef xfibonnacci(x, n):\n\n    def body_fun(i,val):\n        a,b = val\n        return b, a+b\n\n    _, out  = fori_loop(1, n, body_fun, (0,1)) \n\n    return x*out\n</code></pre> <p>For while loops, the number of iterations can be dependent of the input values, but we can still use the JAX while loop primitive. Consider Newton's algorithm for the square root:  </p> <pre><code>def newton(N, tol=1e-6):\n    x = N/2.0\n    error = tol + 1.\n    while error&gt;tol:\n        x_next = 0.5*(x+N/x)\n        error, x = abs(x - x_next), x_next\n    return x\n</code></pre> <p>We can rewrite this function using JAX control flow primitive <code>jax.lax.while_loop</code>. We need to supply the primitive with condition function and body function.   </p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.lax import while_loop\n\n@jax.jit\ndef newton_sqrt(N, tol=1e-6):\n\n    def cond_fun(val):\n        x, e = val\n        return e &gt; tol\n\n    def body_fun(val):\n        x, _ = val\n        x_next = 0.5 * (x + N / x)\n        e = abs(x - x_next)\n        return x_next, e\n\n    init = N / 2.0, tol + 1.0 # Ensure the loop runs at least once\n\n    final_estimate, _ = while_loop(cond_fun, body_fun, init)\n    return final_estimate\n</code></pre> <p>Details here. More on control flow operators: check this page.</p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#grad-grad","title":"<code>grad</code> <code>grad</code>!","text":"<p>When you can compose grad and jacobian to get Hessian, you know automatic differentiation of <code>jax</code> is done right.   Define your function and grad your way out with respect to any variable you are interested. Compose it with <code>jit</code> for performance. Some obvious caveats:</p> <ul> <li>functions must be scalar-valued (there is <code>jax.jacobian</code> for vector valued funcs)</li> <li>inputs must be continous (e.g. float)</li> <li>functions must be differentiable (indexing, argmax etc are not ok)</li> </ul>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#simple-profilingtesting","title":"Simple profiling/testing","text":"<p>The common pitfall of profiling jax is the unawareness of the async dispatch feature. This is the feature that allows JAX programs to run subsequent code before some heavy computation finishes. </p> <p>For instance, below we have a heavy matrix multiplication to run first, then a light query of array shape. In reality we first see the shape printed, then the result. The reason is  </p> <ol> <li>jax does not wait for <code>x.dot(x)</code> to finish before executing the print statment;</li> <li>even though out is not completed, its shape and dtype are known and passed to the print statement.    </li> </ol> <pre><code>import jax.random as jr\n\nx = jr.normal(jr.key(1), (10000,10000))\n\ndef fun(x):\n    out = x.dot(x)\n    print(out.shape, out.dtype)\n    return out\n\nfun(x)\n</code></pre> <p>Therefore, we should block main process until the computation before measuring time like so </p> <pre><code>fun(x).block_until_ready()\n</code></pre> <p>There is no testing module in <code>jax</code>.  Use <code>np.testing.assert_allclose</code> to check the results.  </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#type-hints","title":"Type hints","text":"<p>Use <code>jax.typing.ArrayLike</code> for array input and <code>jax.Array</code> for array output. </p>","tags":["low latency programming"]},{"location":"2025/01/12/intro-to-jax/#reference","title":"Reference","text":"<p>https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods https://jax.readthedocs.io/en/latest/working-with-pytrees.html#example-of-jax-tree-map-with-ml-model-parameters https://jax.readthedocs.io/en/latest/working-with-pytrees.html#custom-pytree-nodes https://jax.readthedocs.io/en/latest/stateful-computations.html#simple-worked-example-linear-regression</p>","tags":["low latency programming"]},{"location":"2025/02/15/survive/","title":"The Cox survival guide: hazards, models and time","text":"<p>Abstract</p> <p>This note is an overview of survival analysis with a focus on the Cox proportional hazards model, a key tool for modelling time-to-event data. We define hazard functions, address right-censoring, and derive estimators like Nelson-Aalen and Kaplan-Meier. The Cox model's semiparametric form and partial likelihood estimation are discussed, along with evaluation metrics such as concordance probability.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#what-is-hazard","title":"What is hazard?","text":"<p>The cumulative hazard function for a time-to-event (e.g. death in medical trials) distribution can be defined in the most generality using the Lebesgue-Stieltjes integral, which accommodates both continuous and discrete distributions and more.</p> <p>For a survival time \\( T \\) with survival function \\( S(t) = P(T &gt; t) \\), the cumulative hazard function \\( H(t) \\) is defined as:</p> \\[ H(t) = \\int_0^t \\frac{dF(s)}{S(s^-)}, \\] <p>where \\( F(s) = P(T \\leq s) \\) and \\( S(s^-) = \\lim_{u \\uparrow s} S(u) = P[T\\ge s] \\) is the left-continuous survival function. Here the integral is a Lebesgue-Stieltjes integral (TLDR this makes sense because F is monotone thus of finite variation).</p> <p>Continuous Distributions:</p> <p>If \\( T \\) is a continuous random variable with density \\(f\\), then \\( S(s^-) = S(s) \\) and the cumulative hazard reduces to</p> \\[ \\begin{align} H(t) &amp;= \\int_0^t h(s) ds \\notag\\\\ h(t) &amp;= \\frac{f(t)}{S(t)} \\label{e-hazard-func} \\end{align} \\] <p>Notice that \\(f(t)=-S'(t)\\) so by \\(\\eqref{e-hazard-func}\\) the survival function satisfies the ODE</p> \\[h(t) = - S'(t)/S(t)\\] <p>with boundary values \\(S(0)=1, S(\\infty)=0\\). One concludes that \\( S(t) = \\exp(-H(t)) \\).  Given an estimator of \\(H\\) or \\(h\\), one can estimate \\(S\\) through this relation.</p> <p>Discrete Distributions:</p> <p>If \\( T \\) is discrete with support \\(\\{ t_1, t_2, \\dots \\}\\), the cumulative hazard is a sum over discrete hazards:</p> \\[ H(t) = \\sum_{t_j \\leq t} h(t_j), \\quad \\text{where } h(t_j) =  \\frac{F(t_j)-F(t_j^-)}{P[T\\ge t_j]} =  P[T = t_j \\mid T \\geq t_j]. \\] <p>We can and will assume that \\(t_1&lt;t_2&lt;t_3\\) and so on. Any \\(t\\ge t_1\\) there exists a unique \\(j\\) such that \\(t_j\\le t&lt;t_{j+1}\\). For \\(t\\ge t_1\\), we have</p> \\[ \\begin{align*} S(t) &amp;= P[T&gt;t] = P[T&gt;t_i, \\forall i = 1, ..., j] \\\\ &amp; = P[T\\ge t_1] P[T&gt;t_1|T\\ge t_1] P[T&gt;t_2 | T&gt;t_1] ... P[T&gt;t_j|T&gt;t_{j-1}] \\end{align*} \\] <p>where we used the fact that \\(P[T\\ge t_1]=1\\). Notice that \\(\\{T&gt;t_{i-1}\\}= \\{T\\ge t_i\\}\\), hence</p> \\[ S(t) = \\prod_{t_j \\leq t} \\left(1 - h(t_j)\\right) \\] <p>Note that \\( H(t) \\neq -\\log S(t) \\) here; instead, \\( -\\log S(t) = \\sum_{t_j \\leq t} -\\log(1 - h(t_j)) \\), which differs from \\( H(t) \\).</p> <p>Again given an estimator of \\(h\\) (note this \\(h\\) is different from the \\(h\\) in the continuous case), one can estimator \\(S\\) through this relation.</p> <p>Other cases:</p> <p>For mixed distributions with both continuous and discrete components, \\( H(t) \\) combines integrals over continuous regions and sums over discrete jumps. But it can get more interseting, think Cantor's function (devil's staircase), how can we  express the hazard differently?</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#right-cencoring","title":"Right-cencoring","text":"<p>In medical applications, it is often necessary to incorporate a censoring distribution on top of the time-to-event distribution. Let \\(C\\) denote a nonnegative random variable and assume that the observation is \\(Y = \\min(T,C)\\) and \\(\\Delta = I(T\\le C)\\). The indicator evaluates to 1 if the event happens no later than the censoring time (eg dead at time \\(Y\\)), and 0 otherwise (eg alive but left the trial at time \\(Y\\)). Typically one assume independence between \\(T\\) and \\(C\\). What we really cares about is the time to event distribution which is not observed directly. How to estimate the hazard (hence the survival function from discussion above) in such case?</p> <p>It is a nice exercise to show that the independence assumption yields (see note<sup>1</sup> for derivation) </p> \\[ H_T(t) = \\int_0^t \\frac{dF_{Y,1}(u)}{S_Y(u^-)} \\] <p>where \\(H_T\\) is the cumulative hazard function of \\(T\\) and \\(F_{Y,1}(u)=P[Y\\le u, \\Delta=1]\\). Now the whole thing is estimable by replacing all terms by their empirical counterpart, which is</p> \\[ \\hat H_T(t) = \\sum_{i=1}^n \\frac{\\Delta_i I(Y_i\\le t)}{\\sum_{j} I(Y_j\\ge Y_i)}. \\] <p>Simple observations about the function \\(\\hat H_T\\):</p> <ol> <li>piecewise constant</li> <li>right continuous</li> <li>it jumps and only jumps at \\(T_i\\) when \\(\\Delta_i=1\\) (equivalently \\(T_i=Y_i\\))</li> <li>those \\(T_i\\) with \\(\\Delta_i=1\\) do not have to be distinct, it is possible that \\(T_i=T_j, \\Delta_i=\\Delta_j=1\\) for some \\(i\\neq j\\).</li> </ol> <p>Let \\(\\{t_1, ...,t_m\\} = \\mbox{set}(\\{T_i: \\Delta_i=1\\})\\). We conclude that \\(\\hat H\\)</p> <ul> <li>jumps at all \\(t_i\\)</li> <li>with magnitude \\(D_i/N_i\\) where \\(D_i = \\sum_j I(T_j=t_i, \\Delta_j = 1)\\) is the number of observed events at time \\(t_i\\), and \\(N_i=\\sum_{j}I(Y_j\\ge t_i)\\) is the number of invididuals at risk just before \\(t_i\\)</li> </ul> <p>Summing over these ratios gives an equivalent expression</p> \\[ \\hat H_T(t) = \\sum_{i:t_i\\le t} \\frac{D_i}{N_i}. \\] <p>Let's recap: we estimate the hazard of \\(T\\) using empirical distribution \\(Y\\) and empirical conditional distribution of \\(Y\\) given \\(\\Delta=1\\), then using the general relationship between hazard and survival function to get an estimate of the survival function of \\(T\\) so we come full circle!</p> <p>We have thus two estimators for the survival function of \\(T\\).</p> \\[ \\begin{align*} \\hat S_T^{\\rm{NA}}(t) &amp;= \\exp( - \\hat H_T(t)) \\\\ \\hat S_T^{\\rm{KM}}(t) &amp;= \\prod_{t_i\\le t} (1 - \\frac{D_i}{N_i}) \\end{align*} \\] <p>where KM stands for Kaplan-Meier and NA for Nelson-Aalen.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#cox-proportional-hazard-model","title":"Cox proportional hazard model","text":"<p>So far we have discussed observation of time-to-event data (censored or not) without covariate effect. Cox PH model incorporates the covariate effect into the modelling of hazard.</p> <p>Fun fact</p> <p>Google scholar reported 63k citations [data as of 11 Feb 2025] of the paper in 1972 by Cox <sup>5</sup> on his nowadays called \"proportional hazard model\". Some says that this is the 2nd most cited papers in science, but that is probably not true any more given that the deep learning industry has exploded the number of publications e.g. the transformer paper <sup>6</sup> has 152k citations now, which was published in 2017.</p> <p>Anyhow, Cox proportional hazard model remains the de facto standard model in survival analysis. It assumes a semiparametric form for the hazard (assuming the hazard measure absolutely continuous)</p> \\[ h_{T|X}(t|x) = h_0(t) \\exp(\\beta\\cdot x) \\quad t&gt;0, \\, x\\in \\RR^p. \\] <p>It is proportional with regard to the change of covariate \\(x\\) for each invididual i.e. \\(\\frac{h(t|x)}{h(t|x+\\delta)}\\) is constant over time. This is a strong assumption yet quite flexible because \\(h_0\\) is an arbitrary function and it allows for (parametric) covariate effect.</p> <p>Estimation and hypothesis testing of \\(\\beta\\) is obviously of utmost importance. This is done by maximizating partial likelihood (likelihood conditioning on the occurence time of the observed events, due to PH assumption, we have only \\(\\beta\\) in the partial likelihood). The exposition of some lecture notes<sup>2</sup> is very clear so I will not waste tokens on it. See also the notes mentioned before<sup>1</sup> for a slightly different perspective.</p>","tags":["quant methods"]},{"location":"2025/02/15/survive/#evaluating-models","title":"Evaluating models","text":"<p>Having estimated the beta parameters in the Cox PH model, we can rank the riskiness of individuals using  \\(g(x)= x\\cdot\\hat\\beta\\) which assigns the same ranking to individuals as with the hazard model at any point in time,  thanks to the PH assumption and the monotonicity of exponential function. </p> <p>A higher risk score translates to a shorter time to event, while a lower risk score translates to a longer time to event.  A natural evaluation metric is the quality of this ranking. One widely used measure of this type is the concordance probability.  The idea is to capture the probability that  a pair of independent samples \\((T_1,C_1,X_1), (T_2,C_2,X_2)\\) are ordered correctly by the risk scores \\(g(X_1)\\) and \\(g(X_2)\\).  To formulate this precisely, introduce for \\(i\\in \\{1,2\\}\\) the events</p> \\[ \\begin{align*} E_i &amp;= \\{T_i&lt;\\max(T_1,T_2)\\} \\\\ F_i &amp;= \\{g(X_i)&gt;\\min(g(X_1),g(X_2))\\} \\end{align*} \\] <p>Clearly \\(E_1\\cup E_2 = \\{T_1\\neq T_2\\}\\). </p> <p>The concordance probability is </p> \\[ \\begin{equation} \\label{e-cpop} P[ \\cup_{i=1}^2 (E_i\\cap F_i) | E_1\\cup E_2 ] = \\frac{ P[F_1 \\cap E_1] + P[F_2 \\cap E_2]}{P[E_1]+P[E_2]} \\end{equation} \\] <p>Note however that \\(T_1,T_2\\) are not always observable. To relate with the observable \\(Y\\) and \\(\\Delta\\) defined earlier, observe that </p> \\[ \\begin{align*} E_1 =&amp; \\{T_1&lt;T_2\\} \\\\ =&amp;\\{\\Delta_1=1, \\Delta_2=1, Y_1&lt;Y_2\\} \\\\ &amp;\\cup \\{\\Delta_1=1, \\Delta_2=0, Y_1&lt;T_2\\} \\\\  &amp;\\cup \\{\\Delta_1=0, \\Delta_2=1, T_1&lt;Y_2\\} \\\\ &amp;\\cup \\{\\Delta_1=0, \\Delta_2=0, T_1&lt;T_2\\} \\\\ \\supset&amp; \\{\\Delta_1 = 1, Y_1&lt;Y_2\\} =:D_1 \\end{align*}  \\] <p>and similarly \\(E_2\\supset \\{\\Delta_2 = 1, Y_1&gt;Y_2\\}=: D_2\\).</p> <p>The standard C-index of Harrell is not a consistent estimator of \\(\\eqref{e-cpop}\\), but rather one that estimates analogous  probability with \\(D_i\\) in place of \\(E_i\\). More concretely, it is the proportion of concordant pairs with the smaller observation being the event time (\\(\\Delta=1\\))  amongst all pairs with  with smaller one being the event time (aka the comparable pairs). One way to address the inconsistency is the CPE method<sup>3</sup> which is robost with  regard to how censoring is performed. Yet another proposal is the C-index IPCW<sup>4</sup> which is a weighted version of C-index using some consistent estimate of the survival function of the censoring distribution in the weights. </p> <ol> <li> <p>https://faculty.washington.edu/yenchic/short_note/note_hazard.pdf \u21a9\u21a9</p> </li> <li> <p>https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture28.pdf \u21a9</p> </li> <li> <p>https://www.mskcc.org/sites/default/files/node/2246/documents/cpe.pdf \u21a9</p> </li> <li> <p>https://biostats.bepress.com/cgi/viewcontent.cgi?referer=&amp;httpsredir=1&amp;article=1108&amp;context=harvardbiostat \u21a9</p> </li> <li> <p>https://www.medicine.mcgill.ca/epidemiology/hanley/c626/cox_jrssB_1972_hi_res.pdf \u21a9</p> </li> <li> <p>https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf \u21a9</p> </li> </ol>","tags":["quant methods"]},{"location":"2025/04/05/fork-merge-and-pr/","title":"Fork, merge and PR","text":"<p>This is a common workflow: </p> <ul> <li>fork a repo (<code>gh repo fork</code> with the gh cli fork the repo interactively)  </li> <li>work on some code then <code>git add</code>, <code>git commit</code> </li> <li>fetch upstream (<code>git remote add upstream REPO</code> then <code>git fetch upstream</code>)  </li> <li>merge upstream (<code>git merge</code> say <code>upstream/main</code>)   </li> <li>push back to the fork (<code>git push</code>)  </li> <li>ask for a pull request (<code>gh pr create</code> interactively)   </li> </ul> <p>Unfortunately the merge step often result in conflicts, and manual intervention is necessary. Fortunately, editors like vscode offer GUI to help visualize the process, making this step less of a pain (after messing up once/twice to understand the GUI).  </p> <p>Alternative workflow (after fetch upstream)</p> <ul> <li>... same until fetch</li> <li>rebase instead of merge: <code>git rebase upstream/main</code>  (then resolve conflicts, if any, commit by commit)</li> <li><code>git push --force-with-lease</code></li> <li>... PR as before</li> </ul> <p>To avoid fetch and rebase everytime, use </p> <pre><code>git config --global pull.rebase true &amp;&amp; git pull upstream main\n</code></pre>","tags":["dev tools"]},{"location":"2025/04/05/fork-merge-and-pr/#how-to-undo-changes","title":"How to undo changes","text":"<p>Conceptually, we may think of git as managing three copies of the same project, they are called </p> <ul> <li>working directory  </li> <li>index/staging area</li> <li>HEAD</li> </ul> <p>Initially they are all the same thing. After some edits, the working directory changes but the index and HEAD remain untouched.</p> <p>Once we run <code>git add</code>, the edits are synced to index so working directory and the index are the same thing. </p> <p>Once we run <code>git commit</code>, the edits are synced to the HEAD and we are once again in a state of all being equal. </p> <p>At any point in time of this process, we can undo changes. </p> <ul> <li><code>git restore .</code> removes modifications in the current directory that are not yet staged. </li> <li>Once staged (after <code>git add</code>), we can drop these changes using <code>git restore --staged .</code> </li> <li>Once commit, we have all three copies the same thing, but we can still drop the changes by restore the previous version of the HEAD (or any other version before) <code>git reset --soft HEAD~1</code> </li> </ul>","tags":["dev tools"]},{"location":"2025/04/05/fork-merge-and-pr/#what-happens-during-the-fetch-and-merge-process","title":"What happens during the fetch and merge process","text":"<p>Again conceptually, we may think of remote branch yet another copy of the same project. When we run <code>git fetch</code>, we sync the remote branch with the latest content in the remote repository.  Assuming we fetch remote content before edits, the working directory is same as index and HEAD, <code>git merge</code> would compare remote with local and merge them. It's similar if you decide to rebase. </p>","tags":["dev tools"]},{"location":"2025/04/06/polars-streaming-tricks/","title":"Polars streaming tricks","text":"<p>Background</p> <p>Polars' lazy execution model (see my previous post) allows query optimizer to \"rewrite\" the query to avoid loading all data into memory such as predicate/project pushdown, constant folding etc. However, the default execusion engine of the optimized query is still in-memory, i.e. loading all the data required in the optimized query into memory, therefore can be problematic if the required data is too large.  The streaming engine is an alternative.    </p> <p><code>polars</code> offers a convenient streaming capability via the <code>LazyFrame.collect(streaming=True)</code> method. Under the hood, this processes the large dataset in chunks, process them, cache the intermediate results and so on. </p> <p>A key limitation of this approach is that the final result (and possibly some intermediate results) must still fit entirely within the machine's RAM. When the final result itself is too large for RAM but all the intermediate results fit into RAM, <code>polars</code> provides the sink_* methods as an alternative. These methods allow to write the output directly to disk. Here is an example from the official documentation: <pre><code>lf = pl.scan_csv(\"my_dataset/*.csv\").filter(pl.all().is_not_null())\n\nlf.sink_parquet(\n    pl.PartitionMaxSize(\n        \"my_table_{part}.parquet\",  # {part} is replaced by partition number\n        max_size=512_000           # bytes\n    )\n)\n</code></pre> This code processes the LazyFrame and stores the results as a sequence of Parquet files on disk, ensuring no single file exceeds the max_size. Because the output is written incrementally to disk, this method effectively removes the RAM limitation for the final result size.</p> <p>One can use this pattern to break a complicated query into sequences of subquery-sink routines, which would remove the RAM constraint at the cost of longer query running time (round trips to disc is slower than operating in RAM).   </p> <p>Github issues tracker for the new streaming engine <sup>1</sup> can be used before a dedicated page on streaming engine functionalities/roadmap. In particular it is in the roadmap that the new streaming engine will support automatically writing to disc when intermediate results are too large, so the afore-mentioned pattern can be performed without user figuring out how to break queries themselves. </p> <ol> <li> <p>https://github.com/pola-rs/polars/issues/20947 \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/04/13/mortgate/","title":"Build a mortgate calculator with fasthtml","text":"<p>There are app frameworks out there for domain experts to easily get started with app buiding, e.g. <code>streamlit</code> for data scientists. </p> <p>A possible downside of these approaches is the lack of flexibility and scalability. <code>fasthtml</code> is a new python framework that is flexible and,  from what I read, pretty scalable too. It provides a unified python interface that maps 1-to-1 to HTTP, html, and htmx for interactivity. This guarantees flexibility without unnecessary abstractions and minimizes client side logic. Also the code often looks pretty neat, therefore temping for new users to get started by copy-pasting some basic examples from their repo. </p> <p>That said, if a new user does not have any knowledge of the web (eg http, how anchor/form tags work, client-server communication, htmx attributes), there are concepts to learn before they can make sense of the framework. But these concepts are worth learning anyway for someone building a web app, so the time invested in the learning is not wasted.  </p> <p>Here is a toy example.</p> <p></p> <p>The whole app is one python file with 20 lines of code (see below). Features highlighted: </p> <ul> <li>html tags are python object (title, input, button, div, details).</li> <li>router and function names map to  url and http methods (put, get ...).</li> <li>htmx for (server centric) interactivity, avoiding writing too much javascript (hx_include, hx_put) </li> <li>by default pico css is used for styling (it can be swapped by tailwind via <code>monsterui</code>, or customized css)</li> </ul> <pre><code>from fasthtml.common import *\n\napp, rt = fast_app(pico=True) # default True\n\n@rt(\"/\")\ndef get():\n    return Titled(\"mortgate calculator\", \n        Grid(\n            Input(type=\"number\", placeholder=\"Loan Amount\", id=\"P\"),\n            Input(type=\"number\", placeholder=\"Interest rate (per cent)\", id=\"r\"),\n            Input(type=\"number\", placeholder=\"Loan Term (months)\", id=\"n\"),\n            Button(\"Calculate\", target_id=\"result\", hx_put=\"/calculate\", hx_include=\"#P,#r,#n\"),),\n        Div(id=\"result\")\n    )\n\n@rt(\"/calculate\")\ndef put(P:float,r:float,n:int):\n    r= r/ 100 / 12\n    m = P*r*(1+r)**n / ((1+r)**n - 1)\n    return Details(\n        Summary(H3(f\"monthly payment {m:.2f}\")), \n        Ul(Li(f\"interest payment {P*r:.2f}\"), Li(f\"principle payment {m-P*r:.2f}\")),\n        open=True)\n\nserve()\n</code></pre> <p>No form element is used because any html element is allowed (via htmx) to trigger HTTP requests. The payload picks up the values of the ids mentioned in <code>hx_include</code> attribute (equivalent to using a form which wraps the inputs and button). </p>","tags":["dev tools"]},{"location":"2025/04/20/df-gen-polars/","title":"Generate test dataframe with <code>polars</code> (powered by <code>hypothesis</code>)","text":"<p>The sub-module <code>polars.testing.parametric</code> provide tools for generating fake data for testing purposes. Here is an example showing what can be done with just <code>dataframes</code> and <code>column</code> functions in this module <pre><code>import polars as pl\nfrom polars.testing.parametric import dataframes, column\n\ndef generate(size=5):\n    return dataframes(\n    [\n        column(\"id\", dtype=pl.UInt16, unique=True, allow_null=False), \n        column(\"value\", dtype=pl.Int16, allow_null=True), \n        column(\"cat\", dtype =pl.Enum(\"XYZ\"), allow_null=False)\n    ], \n    min_size=size, max_size=size)\n\noriginal = generate().example()\n</code></pre></p> <p>The output is random, i.e. evey call to the <code>example</code> method would generate a new dataframe with the prescribed characteristics (this method is for interactive use only). One can test their data pipelines on fake data with precise schema and simulated data quality deficiencies (eg null values, nan, inf, etc). </p> <p>For unittesting, here is an example from the offical docs</p> <pre><code>from hypothesis import given\n@given(df=dataframes(min_size=3, max_size=5))\ndef test_df_height(df: pl.DataFrame) -&gt; None:\n    assert 3 &lt;= df.height &lt;= 5\n</code></pre>","tags":["data engineering"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/","title":"LLM eval for my personal usage","text":"<p>As common evaluation metrics for LLM performance are being gamified, everyone starts to run some evaluation thingy most relevant for their own use case.</p> <p>In this post I keep track of queries that I asked multiple LLMs and record the winner who gave the best answer.  Here are the contesters: </p> contester stands for G Gemini D deepseek C ChatGPT X Grok <p>The reason for excluding Claude is personal: the last time I checked, it appears to be not as generous as other frontier labs to offer their best-ish model to free tier users. And now I have too many chatbot tabs to manage </p> <p>I include the date of the query to highlight the dynamic nature of these evals and help identify model versions if I/anyone wanted to. </p> <p>By default, I always enable the most powerful model available  to a free tier user (think and search for ChatGPT. Gemini 2.5 Pro etc in April 2025) for a \"fair\" comparison.</p> <p>This post is regularly updated. </p>","tags":["LLM"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#paste-image-to-markdown-20250424","title":"Paste image to markdown - 20250424","text":"<p>in vscode in WSL, paste image from clipboard to markdown in customized directory.</p> <p>C  wins. Other LLMs refer to some obsolete vscode extension which does not work. In think mode C started thinking about using the same extensions, but then made some search in vscode github project and elsewhere,  ended up finding  that this is a feature already built-in in vscode after v1.79 and provided the correct json for me to copy to user settings.   </p> <p>For example. Paste figures into the assets folder in the document directory (one that contains the markdown file) with the name of the image =  document name + time + '.png'</p> <pre><code>{\n    \"markdown.copyFiles.destination\": {\n            \"**/*.md\": \"${documentDirName}/assets/${documentBaseName}-${unixTime}.${fileExtName}\"\n        }\n}\n</code></pre>","tags":["LLM"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#search-the-url-of-a-file-in-github-20250501","title":"search the url of a file in github - 20250501","text":"<p>show me the link to the source code of the new polars streaming engine. this should be hosted in github</p> <p>G wins. C and X provide irrelevant links. D cannot search (functionality not working in the region where I am which is EU). </p>","tags":["LLM"]},{"location":"2025/04/24/llm-eval-for-my-personal-usage/#writing-task-20250518","title":"Writing task - 20250518","text":"<p>when checking dataquality what are principles that are mutually exclusive so that I don't forget anaything. this project invovles many datasets forming a graph and I need to check indiviudal datasets as well as the way datasets are joined, transformed from one to another.</p> <p>I've gone through the drafting, criticizing, improving cycle a few times, with several llms doing part of the job. </p> <p>The topic is kinda common and all llms have seen articles, books, blogposts about it already, in one way or another. </p> <p>Both D and X followed the prompt and provided solid ideas. G raised some valid points questioning the prompt itself (like DQ checks are often interconnected). C revised the answer G gave that improved readability. </p> <p>My votes go to G and C. The result is here.</p>","tags":["LLM"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/","title":"Puzzling query optimization behaviour and dtypes shrinking in polars","text":"<p>The official blog<sup>1</sup> of <code>polars</code> shared a recent case study that's part counterintuitive, part educational. </p>","tags":["data engineering"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/#counterintuitive-bits","title":"Counterintuitive bits","text":"<p>The authors found that for large DataFrames (~400M rows), using lazy execution with the streaming engine for a massive query plan -- built via the <code>pipe</code> method --  was not the optimal approach. This is basically saying: the query engine, one of polars' key selling point, struggles to optimize giant plans effectively.</p> <p>The solution they found is to use lazy + streaming only on very expensive joins, and run  other parts of the query plan  eagerly. </p> Update on 2025-06-15 <p>The new streaming engine achieves some impressive benchmark results per official blog in May 2025, potentially changing the verdict of this case study if it were conducted now. </p>","tags":["data engineering"]},{"location":"2025/04/25/puzzling-query-optimization-behaviour-and-dtypes-shrinking-in-polars/#shrink-dtype","title":"Shrink dtype","text":"<p>Feels obvious, but it's a solid reminder: why would you use int64 if all your values never go above a few millions? Use  <code>shrink_dtype</code> on expressions/series and <code>shrink_to_fit</code> on dataframe to downcast dtypes to just what's needed. </p> <p>Watch out when joining on downcast columns - dtypes need to match. </p> <ol> <li> <p>https://pola.rs/posts/case-metro-digital/ \u21a9</p> </li> </ol>","tags":["data engineering"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/","title":"Static site generator using fasthtml","text":"","tags":["dev tools"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/#genesis","title":"genesis","text":"<p>Currently I use mkdocs (with the <code>material</code> theme) for both blog and documentation. </p> <p>The thing I like the most about material for mkdocs is that it is easy to get started. Copy mkdocs.yml from your favorite projects which uses material for mkdocs (polars, uv, fastapi in my case) and start writing your md files. Essentially a no code solution for writing static contents.</p> <p>The ease of use comes at a price though. It may be hard to tweak one little thing you want which is not supported in the yaml config file or/and mkdocs plugin. To do that, you have to dig into the template, the partials and learn Jinja2 template language if you haven't already. It's too much.</p> <p>Practically all SSG out there seem to use the same components: a markdown parser, a template system with Jinja2-like language and html/css files. </p> <p>It strikes me that fasthtml can do all that with just python. I am not interested in something achieving feature parity with mkdocs, hugo, jekyll, but rather a minimal system that I have full control and maximum customization for SSG use case. </p>","tags":["dev tools"]},{"location":"2025/05/01/static-site-generator-using-fasthtml/#archetecture","title":"archetecture","text":"<p>Write a function that chains fasttags leaving as variable the things I want to inject into the function from a parsed markdown file (metadata, content etc). This function is the template. All styles and scripts are embedded into the function.  Markdown files can be rendered with mistletoe (which come with monsterui,  fasthtml's UI components library).  </p> <p>for example</p> <pre><code>import yaml\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom fasthtml.common import *\nfrom monsterui.all import * # include render_md\n\n\n@dataclass\nclass Page:\n    \"\"\"from markdown to html\"\"\"\n    path: Path\n    content: str\n    meta: dict\n\n    @classmethod\n    def from_file(cls, path: Path):\n        \"\"\"Load a page from a file, parsing front matter and content.\"\"\"\n        text = path.read_text()\n        if \"---\" in text:\n            meta_str, content = text.split(\"---\", 2)[1:]\n            meta = yaml.safe_load(meta_str)\n        else:\n            meta, content = {}, text\n        return cls(path, content, meta)\n\n    def render(self) -&gt; str:\n        \"\"\"Plug page data into a template.\"\"\"        \n        content = render_md(self.content)\n        title=  self.meta.get(\"title\", \"Untitled\"),\n        return base_template(title, content, self.meta)\n\ndef base_template(title, content, meta, hdrs=Theme.blue.headers(daisy=True, highlightjs=True)) -&gt; str:\n    \"\"\"Core layout template using MonsterUI components\"\"\"\n    return to_xml(Html(\n            Head(\n                Title(title),\n                Meta(charset=\"utf-8\"),\n                Meta(name=\"viewport\", content=\"width=device-width, initial-scale=1.0\"),\n                *hdrs  # Theme is a list\n            ),\n            Body(\n                Container(\n                    NavBar(\n                        A(\"Home\", href=\"/\"),\n                        A(\"Blog\", href=\"/blog\"),\n                        brand=H2(\"My SSG\"),\n                    ),\n                    Main(\n                        Article(\n                            Header(\n                                H1(title),\n                                Div(\n                                    Small(meta.get('date',None)),\n                                    cls=TextPresets.muted_sm\n                                ) if meta.get('date') else None,\n                            ),\n                            Div(Safe(content)),  # Markdown content\n                        ),\n                        cls=\"max-w-3xl mx-auto\"\n                    ),\n                    Footer(\n                        P(f\"\u00a9 {datetime.now().year} My Static Site\", cls=TextPresets.muted_sm),\n                        cls=\"mt-8 text-center\")))))\n</code></pre> <p>Now I can do this to generate one file (or write a loop to handle a directory of md docs). </p> <pre><code>Path(\"index.html\").write_text(Page.from_file(Path('README.md')).render())\n</code></pre>","tags":["dev tools"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/","title":"A dimensional approach to data quality principles","text":"<p>courtesy: Gemini, ChatGPT, DeepSeek, Grok</p> <p>Data quality checks often overlap\u2014rather than forcing each principle into its own silo, it\u2019s more practical to group them into logical \u201cdimensions.\u201d This approach helps teams understand where issues live, how they relate, and how to fix them.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#i-intrinsic-data-quality-characteristics-of-the-data","title":"I. Intrinsic Data Quality (Characteristics of the Data)","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-accuracy","title":"1. Accuracy","text":"<p>What it is: How well data reflects the real world. Why it matters: Bad accuracy means bad decisions. Example: Customer address matches actual GPS coordinates.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-completeness","title":"2. Completeness","text":"<p>What it is: All required data is present. - Schema completeness: All expected columns exist. - Column completeness (density): % of non-null values in a column. - Population completeness: All expected rows exist. Example: Every customer record has an <code>email_address</code>.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-consistency","title":"3. Consistency","text":"<p>What it is: No contradictions within or across datasets. - Intra-record: Fields inside one record don\u2019t clash (e.g., age vs. date_of_birth). - Inter-record: Same entity looks the same over time (e.g., name spelling). - Cross-dataset: Shared fields use the same units and formats (e.g., revenue in USD). Example: Product codes use the same format in sales, inventory, and marketing.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-timeliness","title":"4. Timeliness","text":"<p>What it is: Data is fresh enough for its purpose. - Currency: How recent it is. - Frequency: How often it\u2019s updated. - Punctuality: It arrives when you expect it. Example: Stock prices updated every second for trading dashboards.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#5-uniqueness","title":"5. Uniqueness","text":"<p>What it is: No duplicates of the same real-world entity. Example: Each customer has exactly one <code>customer_id</code>.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#6-validity","title":"6. Validity","text":"<p>What it is: Data follows rules, formats, and logical constraints. - Syntactic: Correct type/format (e.g., dates as YYYY\u2011MM\u2011DD). - Semantic: Meets business logic (e.g., order_date \u2264 ship_date). - Referential integrity: Foreign keys point to real parent records. Example: Emails match a regex pattern; <code>order_status</code> is one of \u201cPending,\u201d \u201cShipped,\u201d \u201cDelivered.\u201d</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#ii-data-management-operational-excellence","title":"II. Data Management &amp; Operational Excellence","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-transformation-integrity","title":"1. Transformation Integrity","text":"<p>What it is: ETL/ELT logic is correct and preserves quality. Example: <code>profit = revenue \u2212 cost</code> is implemented correctly downstream.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-metadata-lineage","title":"2. Metadata &amp; Lineage","text":"<p>What it is: You know what each field means and where data came from. - Schemas match docs. - Business rules documented. - Data dictionaries/glossaries exist. - Lineage tracking end-to-end. Example: A data catalog shows original source files, transformations, and load targets.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-performance-scalability","title":"3. Performance &amp; Scalability","text":"<p>What it is: Systems handle current workloads and growth. Example: Indexed tables for fast queries; pipelines auto-scale on spikes.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-resilience-error-handling","title":"4. Resilience &amp; Error Handling","text":"<p>What it is: Systems detect, retry, and recover from failures. Example: ETL jobs retry on transient errors; backups restore lost data.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#5-security-privacy","title":"5. Security &amp; Privacy","text":"<p>What it is: Data is protected and handled per regulations. Example: PII is encrypted in transit and at rest; RBAC enforces least privilege.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#iii-governance-risk-compliance","title":"III. Governance, Risk &amp; Compliance","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-stewardship-ownership","title":"1. Stewardship &amp; Ownership","text":"<p>What it is: Clear accountability for each data domain. Example: Finance owns transaction data; Marketing owns campaign metrics.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-regulatory-compliance","title":"2. Regulatory Compliance","text":"<p>What it is: Processes meet GDPR, HIPAA, SOX, etc. Example: Customer opt\u2011outs are enforced in all systems.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#3-auditability-reproducibility","title":"3. Auditability &amp; Reproducibility","text":"<p>What it is: You can trace every change and rerun analyses. Example: Versioned datasets and code; audit logs for data changes.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#4-lifecycle-management","title":"4. Lifecycle Management","text":"<p>What it is: Data is created, stored, archived, and deleted per policy. Example: Transaction logs archived after 7 years, purged at 10.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#iv-user-centric-value-realization","title":"IV. User-Centric &amp; Value Realization","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-relevance","title":"1. Relevance","text":"<p>What it is: Data fits the users\u2019 needs and use cases. Example: Sales dashboard shows KPIs the team actually uses.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-usability-interpretability","title":"2. Usability &amp; Interpretability","text":"<p>What it is: Data is easy to find, understand, and use. - Clarity: Human\u2011readable names and definitions. - Accessibility: Delivered in formats users can work with. - Context: Code tables, descriptions, and examples provided. Example: Dashboards with clear labels and linked data definitions.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#v-continuous-improvement-monitoring","title":"V. Continuous Improvement &amp; Monitoring","text":"","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#1-monitoring-measurement","title":"1. Monitoring &amp; Measurement","text":"<p>What it is: Automated checks track data quality over time. Example: Daily jobs that flag spikes in null rates or invalid formats.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#2-issue-management-remediation","title":"2. Issue Management &amp; Remediation","text":"<p>What it is: Process to log, prioritize, and fix data issues. Example: Tickets in a bug tracker for data errors, complete with severity levels and SLAs.</p>","tags":["data engineering"]},{"location":"2025/05/14/a-dimensional-approach-to-data-quality-principles/#putting-it-all-together-a-sample-workflow","title":"Putting It All Together: A Sample Workflow","text":"<p>Define requirements &amp; metrics </p> <ul> <li>Talk to users: What does \u201cfit for purpose\u201d look like?  </li> <li>Set thresholds for accuracy, completeness, timeliness, etc.</li> </ul> <p>Profile &amp; assess data </p> <ul> <li>Run accuracy checks, null counts, format validators, duplication scans.</li> </ul> <p>Evaluate operations </p> <ul> <li>Audit ETL logic, metadata, system health, security controls.</li> </ul> <p>Review governance </p> <ul> <li>Confirm ownership, compliance controls, audit trails, retention policies.</li> </ul> <p>Establish monitoring &amp; fixes </p> <ul> <li>Schedule automated checks and ticketing for quick resolution.</li> </ul> <p>By viewing data quality through these dimensions, you get a holistic framework that\u2019s both comprehensive and practical\u2014nothing slips through the cracks, and every team knows where they fit in.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/","title":"Intro to Snowpark API","text":"<p>Polars users might find the Snowpark/PySpark API verbose. But the high-level abstractions roughly map: <code>snowpark.Column</code> is like <code>polars.Expr</code>, and <code>snowpark.DataFrame</code> is like <code>polars.LazyFrame</code>.</p> <p>Caveat: the list of methods on <code>Column</code> is much smaller than on <code>Expr</code>, and there\u2019s no clear namespace separating methods by dtype. If you don\u2019t see a method for your use case, check the gigantic list under <code>snowpark.functions</code>.</p> <p>Let\u2019s explore.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#first-imports","title":"First, imports","text":"<pre><code>from snowflake import snowpark as sp\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#column-class","title":"<code>Column</code> class","text":"<p>Accessible via <code>F.col</code>, e.g. <code>lf.select(F.col(\"A\"))</code>.</p> <p>A few notable methods on <code>Column</code> instances:</p> <ul> <li><code>cast</code></li> <li><code>name</code> (alias)</li> <li><code>asc</code> / <code>desc</code></li> <li><code>is_null</code></li> <li><code>is_in</code></li> <li><code>over</code></li> <li><code>substr</code></li> </ul> <p>Unlike Polars, there\u2019s no method namespace separation by dtype. Still, <code>Column</code> feels a lot like <code>polars.Expr</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#f-namespace-functions","title":"<code>F</code> namespace (functions)","text":"<p>The <code>F</code> namespace provides many ways to manipulate <code>Column</code>s.</p> <p>Note: all functions return a <code>Column</code>, not just <code>F.col</code>, e.g.</p> <ul> <li><code>F.concat</code></li> <li><code>F.contains</code></li> <li><code>F.array_size</code></li> <li><code>F.year</code></li> <li><code>F.when(...).otherwise(...)</code></li> <li><code>F.explode</code> and <code>F.flatten</code></li> </ul> <p>Again, no subnamespaces by dtype \u2014 everything is dumped into <code>functions</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#window-class","title":"<code>Window</code> class","text":"<p>Provides a bunch of class methods:</p> <ul> <li><code>Window.partition_by</code></li> <li><code>Window.order_by</code></li> <li><code>Window.range_between</code> (time or int)</li> </ul> <p>And constants like:</p> <ul> <li><code>Window.CURRENT_ROW</code></li> </ul> <p>used as an argument to the <code>over</code> method.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#t-types","title":"<code>T</code> (types)","text":"<p>For type hints and casting, use:</p> <ul> <li><code>T.ArrayType</code></li> <li><code>T.DateType</code></li> <li><code>T.StructType</code></li> </ul> <p>Caveat: these are callables \u2014 instantiate them like <code>T.DateType()</code>.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#dataframe-class","title":"<code>DataFrame</code> class","text":"<p>It\u2019s called a DataFrame, but really behaves like a LazyFrame \u2014 call <code>.collect()</code> to materialize.</p> <p>Notable methods (all return a new DataFrame):</p> <ul> <li><code>select</code></li> <li><code>filter</code></li> <li><code>pivot</code> / <code>unpivot</code></li> <li><code>join</code></li> <li><code>union</code></li> <li><code>with_columns</code> (takes <code>list[str]</code>, <code>list[Column]</code>)</li> <li><code>with_column</code></li> <li><code>distinct</code></li> <li><code>fillna</code></li> </ul> <p>and the <code>.columns</code> attribute.</p> <p>Note: after joining, columns with the same name need to be aliased explicitly to save/view the joined table. e.g. t1.col.alias(\"c1\"), t2.col.alias(\"c2\").  </p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#io","title":"IO","text":"<p>Ways to read data from snowflake:</p> <ul> <li><code>session.table(\"TABLE_NAME\")</code> </li> <li><code>session.sql(\"SELECT ...\")</code> </li> <li><code>session.read.option(\"infer_schema\", False).parquet(\"@mystage/test.parquet\")</code>  Multiple file formats are supported, simply replace parquet by the file extension e.g. csv</li> </ul> <p>Ways to push in-memory data to snowflake: </p> <ul> <li><code>session.create_dataframe(df: pd.DataFrame).write.format(...).save(...)</code></li> <li><code>session.create_dataframe(df: list[tuple|Row]).write.mode(\"overwrite\").save_as_table(\"my_table\", table_type=\"temporary\")</code></li> </ul>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#row-class","title":"<code>Row</code> class","text":"<p>You get a list of Rows after <code>collect()</code> The <code>.as_dict()</code> method on Row makes it easy to interoperate with Polars: just pass a list of dicts to construct a Polars DataFrame.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#testing","title":"Testing","text":"<ul> <li><code>sp.testing.assert_dataframe_equal</code></li> </ul>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#horizontal-ops-in-snowpark","title":"Horizontal ops in snowpark","text":"<p>Horizontal ops such as horizontal any (think <code>np.any(..., axis=1)</code>) can be achieved with a chain of logical OR e.g.  <code>(... | ... | ... )</code> or by using <code>F.when().otherwise()</code>. When the number of conditions/columns increases, I would like to use something similar to polars </p> <pre><code>import polars as pl\nlf.select(pl.any_horizontal(col_names)) # col_names: list[str]\n</code></pre> <p>or the general purpose reduce/fold function for horizontal ops in polars. </p> <pre><code>lf.select(pl.reduce(lambda a,b:a|b, exprs = pl.col(col_names)))\n</code></pre> <p>In snowpark there is no any_horizontal, nor reduce function.  But one can use python <code>functools.reduce</code>.</p> <pre><code>from functools import reduce\n\nany_expr = reduce(lambda a, b: a | b, map(F.col, col_names))\nlf.select(any_expr)\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#exception","title":"Exception","text":"<p>The most common is <code>sp.exceptions.SnowparkSQLException</code>, corresponding to 13xx SQL error.</p>","tags":["data engineering"]},{"location":"2025/05/19/intro-to-snowpark-api/#null-handling","title":"NULL handling","text":"<p>In aggregate functions like count, sum etc, NULL values are ignored. If all values in a column are NULL, the aggregate function acting on the column returns NULL.</p> <p>In comparison logic (&gt;, &lt;, &lt;&gt;, =, &gt;=, &lt;=) the result of NULL vs any value is NULL. Therefore in filters, NULL needs to be explicitly catched by the clause <code>A IS NULL</code> or <code>A IS NOT NULL</code>, for otherwise the filter <code>where A &lt; 1</code> excludes all the records where A is NULL.    Several functions are bound to NULL handling e.g. coalesce, ifnull, nullif, equalnull.</p>","tags":["data engineering"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/","title":"Repurposing gitHub for mathematical research","text":"<p>courtesy: Claude</p> <p>Marin project triggers my interest to repurpose github as a tool for open and reproducible mathematical research. Below is an example of how this can work, mostly one-shot generated by Claude.  </p>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#basic-repository-structure","title":"Basic Repository Structure","text":"<pre><code>conjecture-name/\n\u251c\u2500\u2500 README.md         # Overview, current status, and roadmap\n\u251c\u2500\u2500 conjecture.md     # Formal statement and background\n\u251c\u2500\u2500 proofs/           # Your proof attempts and progress\n\u251c\u2500\u2500 simulations/      # Code for experimental verification\n\u2514\u2500\u2500 notes/            # Working thoughts and observations\n</code></pre>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#simple-workflow","title":"Simple Workflow","text":"","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#1-setup-problem-definition","title":"1. Setup &amp; Problem Definition","text":"<ul> <li>Create the repository with a clear README explaining your goal</li> <li>Write <code>conjecture.md</code> with the formal statement, notation, and known related results</li> <li>Set up an initial project board with basic columns: \"Ideas\", \"In Progress\", \"Needs Review\", \"Completed\"</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#2-working-process-using-issues","title":"2. Working Process Using Issues","text":"<ul> <li>Create issues for specific approaches: \"Approach via induction\", \"Probabilistic method attempt\"</li> <li>Create issues for subproblems: \"Prove special case when n=2\", \"Find counterexample for condition X\"</li> <li>Use issue descriptions to outline your thinking process</li> <li>Reference mathematical literature with links or DOIs</li> <li>Close issues when an approach proves unsuccessful (with notes on why)</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#3-exploration-via-branches","title":"3. Exploration via Branches","text":"<ul> <li>Create a new branch for each significant approach (<code>inductive-approach</code>, <code>computational-verification</code>)</li> <li>Commit frequently with descriptive messages that explain your mathematical thinking</li> <li>Use commit messages to document insights: \"Realized lemma fails when x approaches infinity\"</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#4-documentation-using-pull-requests","title":"4. Documentation Using Pull Requests","text":"<p>When you make meaningful progress: - Open a PR from your working branch to main - Write a detailed description of your findings - Self-review your work by reading through changes with fresh eyes - Merge when you're confident in the correctness</p>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#5-simulation-verification","title":"5. Simulation &amp; Verification","text":"<ul> <li>Place code in the <code>simulations/</code> directory</li> <li>Document computational experiments in PR descriptions</li> <li>Link simulation results to theoretical approaches</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#6-progress-tracking","title":"6. Progress Tracking","text":"<ul> <li>Use GitHub milestones to mark significant achievements (\"Special case proven\", \"Main lemma established\")</li> <li>Update the README regularly with current status</li> <li>Pin important issues that represent active lines of inquiry</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#key-github-features-to-leverage","title":"Key GitHub Features to Leverage","text":"","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#issues-use-as-mathematical-questions-approaches-or-subproblems","title":"Issues - Use as mathematical questions, approaches, or subproblems","text":"<ul> <li>Add labels like \"promising\", \"stuck\", \"counterexample\"</li> <li>Reference equations with LaTeX in Markdown: <code>$E = mc^2$</code></li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#project-board-visual-organization-of-your-research-flow","title":"Project Board - Visual organization of your research flow","text":"<ul> <li>Track which approaches are active</li> <li>See what needs verification</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#pr-process-even-as-a-solo-researcher-use-prs-as-checkpoints","title":"PR Process - Even as a solo researcher, use PRs as checkpoints","text":"<ul> <li>Force yourself to clearly articulate progress</li> <li>Create a record of major developments</li> <li>Serve as a self-review mechanism</li> </ul>","tags":["research"]},{"location":"2025/05/21/repurposing-github-for-mathematical-research/#discussions-for-longer-form-mathematical-exploration","title":"Discussions - For longer-form mathematical exploration","text":"<ul> <li>Work through complex ideas</li> <li>Record insights that don't fit elsewhere</li> </ul>","tags":["research"]},{"location":"2025/05/25/gpu-puzzles-annotated/","title":"GPU puzzles annotated","text":"<p>Modular's adaptation of CUDA puzzles (by Sasha Rush) in mojo, solved and annotated below.</p>","tags":["low latency programming","mojo"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-1","title":"Puzzle 1","text":"<p>Notes:</p> <ul> <li>be aware of host and device (CPU and GPU), sync and async</li> <li>enqueue operations in GPU stream, execute them async</li> </ul> <pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = SIZE\nalias dtype = DType.float32\n\nfn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):\n    i = thread_idx.x\n    out[i] = 10 + a[i]\n\ndef main():\n    with DeviceContext() as ctx:\n        print(ctx.api()) # cuda\n        out = ctx.enqueue_create_buffer[dtype](SIZE) # gpu async \n        out = out.enqueue_fill(0) # gpu async \n        a = ctx.enqueue_create_buffer[dtype](SIZE)\n        a = a.enqueue_fill(0)\n        with a.map_to_host() as a_host: # sync at mapping to ensure buffer is created\n            for i in range(SIZE):\n                a_host[i] = i\n\n        ctx.enqueue_function[add_10](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        ) # gpu async\n\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE)\n        expected = expected.enqueue_fill(0) # gpu async\n\n        ctx.synchronize() # code would fail if sync after modifying expected i.e. first modify then fill 0 at sync time. \n\n        for i in range(SIZE): \n            expected[i] = i + 10\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming","mojo"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-2","title":"Puzzle 2","text":"<pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = SIZE\nalias dtype = DType.float32\n\nfn add(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    b: UnsafePointer[Scalar[dtype]],\n):\n    i = thread_idx.x # 1d thread index map on 1d array data \n    out[i] = a[i] + b[i]\n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0) # size of the input data is conveniently block_size\n        b = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0) \n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host, b.map_to_host() as b_host:\n            for i in range(SIZE):\n                a_host[i] = i\n                b_host[i] = i\n                expected[i] = a_host[i] + b_host[i]\n\n        ctx.enqueue_function[add](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            b.unsafe_ptr(),\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming","mojo"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-3","title":"Puzzle 3","text":"<pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\n\nalias SIZE = 4\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (8, 1) # more threads than data size\nalias dtype = DType.float32\n\nfn add_10_guard(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    i = thread_idx.x\n    if i&lt;size: # avoid out of bounds \n        out[i] = a[i] + 10.0 \n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(0)\n\n        with a.map_to_host() as a_host:\n            for i in range(SIZE):\n                a_host[i] = i\n                expected[i] = i + 10\n\n        ctx.enqueue_function[add_10_guard](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                assert_equal(out_host[i], expected[i])\n</code></pre>","tags":["low latency programming","mojo"]},{"location":"2025/05/25/gpu-puzzles-annotated/#puzzle-4","title":"Puzzle 4","text":"<p>with pointer</p> <pre><code>from memory import UnsafePointer\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\nfrom testing import assert_equal\n\nalias SIZE = 2\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (3, 3)\nalias dtype = DType.float32\n\n#  2d block, 1d input, 1d output\nfn add_10_2d(\n    out: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    row = thread_idx.y\n    col = thread_idx.x\n    if row &lt; size and col&lt;size: # prevent out of bounds\n        out[row * size + col] = a[row * size + col] + 10\n\ndef main():\n    with DeviceContext() as ctx:\n        out = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        expected = ctx.enqueue_create_host_buffer[dtype](\n            SIZE * SIZE\n        ).enqueue_fill(0)\n        a = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host:\n            # row-major\n            for i in range(SIZE):\n                for j in range(SIZE):\n                    a_host[i * SIZE + j] = i * SIZE + j\n                    expected[i * SIZE + j] = a_host[i * SIZE + j] + 10\n\n        ctx.enqueue_function[add_10_2d](\n            out.unsafe_ptr(),\n            a.unsafe_ptr(),\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out.map_to_host() as out_host:\n            print(\"out:\", out_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE):\n                for j in range(SIZE):\n                    assert_equal(out_host[i * SIZE + j], expected[i * SIZE + j])\n</code></pre> <p>with <code>LayoutTensor</code></p> <pre><code>from gpu import thread_idx, block_dim, block_idx\nfrom gpu.host import DeviceContext\nfrom layout import Layout, LayoutTensor\nfrom testing import assert_equal\n\nalias SIZE = 2\nalias BLOCKS_PER_GRID = 1\nalias THREADS_PER_BLOCK = (3, 3)\nalias dtype = DType.float32\nalias layout = Layout.row_major(SIZE, SIZE)\n\n# 2d block, 2d input, 2d output\nfn add_10_2d(\n    out: LayoutTensor[mut=True, dtype, layout],\n    a: LayoutTensor[mut=True, dtype, layout],\n    size: Int,\n):\n    row = thread_idx.y\n    col = thread_idx.x\n    if row &lt; size and col &lt; size: \n        out[row, col] = a[row, col] + 10 # index arithmetic is handled by LayoutTensor\n\n\ndef main():\n    with DeviceContext() as ctx:\n        out_buf = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        out_tensor = LayoutTensor[mut=True, dtype, layout](out_buf.unsafe_ptr()) # NOT created with ctx\n        print(\"out shape:\", out_tensor.shape[0](), \"x\", out_tensor.shape[1]())\n\n        expected = ctx.enqueue_create_host_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n\n        a = ctx.enqueue_create_buffer[dtype](SIZE * SIZE).enqueue_fill(0)\n        with a.map_to_host() as a_host:\n            for i in range(SIZE * SIZE):\n                a_host[i] = i\n                expected[i] = a_host[i] + 10\n\n        a_tensor = LayoutTensor[mut=True, dtype, layout](a.unsafe_ptr())\n\n        ctx.enqueue_function[add_10_2d](\n            out_tensor,\n            a_tensor,\n            SIZE,\n            grid_dim=BLOCKS_PER_GRID,\n            block_dim=THREADS_PER_BLOCK,\n        )\n\n        ctx.synchronize()\n\n        with out_buf.map_to_host() as out_buf_host:\n            print(\"out:\", out_buf_host)\n            print(\"expected:\", expected)\n            for i in range(SIZE * SIZE):\n                assert_equal(out_buf_host[i], expected[i])\n</code></pre>","tags":["low latency programming","mojo"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/","title":"Working with large datasets in Snowflake","text":"<p>This post covers how to work with large datasets in Snowflake e.g. 100+ GB, billions of rows. You can't really do this kind of stuff on your personal laptop (mine has 6 cores and 16GB RAM), so most people turn to the cloud for this workload.</p> <p>We'll go over some practical tips, using the Snowpark API version 1.32 to illustrate.</p> <pre><code>from snowflake import snowpark as sp\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#choose-the-right-warehouse","title":"Choose the Right Warehouse","text":"<p>There\u2019s no one-size-fits-all. Larger warehouses cost more, so don\u2019t scale up unless you actually need the extra horsepower. Match the warehouse size to the workload. Start small and scale up only if performance is a problem.</p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#cache-the-results","title":"Cache the Results","text":"<p>Snowpark DataFrames are lazily evaluated. That means if you reuse part of a query in multiple branches, Snowflake will recompute that shared part unless you cache it explicitly.</p> <p>Example:</p> <pre><code>a = session.table(\"...\")\nquery = a.group_by(\"A\").agg(...)  # simulate expensive intermediate step, and this line won't execute anything yet due to lazy evaluation\n\n# branching out\nres1 = query.filter(...).collect()\nres2 = query.select(...).collect()\n</code></pre> <p>Both <code>res1</code> and <code>res2</code> will trigger the same expensive computation. To avoid that, materialize the intermediate result:</p>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#temp-table-session-scoped","title":"Temp Table (session scoped)","text":"<pre><code>query.write.mode(\"overwrite\").save_as_table(\"temp_table\", table_type=\"temporary\")\nlf = session.table(\"temp_table\")\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#materialized-view-persistent-auto-refreshed-costs-money","title":"Materialized View (persistent, auto-refreshed, costs money)","text":"<pre><code>session.sql(\"\"\"\n    create materialized view my_view as\n    select id, count(*)\n    from my_table\n    group by id\n\"\"\").collect()\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#parquet-file-in-stage","title":"Parquet File (in stage)","text":"<pre><code>path = f\"{session.get_session_stage()}/f.parquet\"\na.group_by(\"A\").agg(...).write.format(\"parquet\").save(path)\nlf = sp.DataFrameReader.parquet(path)\n</code></pre>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#predicate-and-projection-pushdown","title":"Predicate and Projection Pushdown","text":"<p>\"Predicate\" = filter. \"Projection\" = column selection.</p> <p>Snowflake tries to optimize automatically, but it only knows what you tell it\u2014and lazy evaluation doesn't help. If you're building up a complex query, be explicit:</p> <ul> <li>Filter early, especially before joins</li> <li>Drop columns you don\u2019t need</li> <li>Use <code>DataFrame.explain()</code> to inspect the query plan</li> </ul>","tags":["data engineering"]},{"location":"2025/05/26/working-with-large-datasets-in-snowflake/#monitoring","title":"Monitoring","text":"<p>Use <code>QUERY_HISTORY</code> to monitor performance. Look at:</p> <ul> <li><code>bytes_scanned</code></li> <li><code>partitions_scanned</code></li> </ul> <p>Lower is better. If you're scanning too much data, it's a sign that pushdowns or clustering could be improved.</p>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/","title":"duckdb basics","text":"","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#in-memory","title":"in-memory","text":"<p>duckdb can query files and dataframes</p> <pre><code>import duckdb\nimport polars as pl\n\nduckdb.sql(\"SELECT * FROM 'example.csv'\")     # directly query a CSV file\nduckdb.sql(\"SELECT * FROM 'example.parquet'\") # directly query a Parquet file\nduckdb.sql(\"SELECT * FROM 'example.json'\")    # directly query a JSON file\n\npolars_df = pl.DataFrame({\"a\": [42]})\nduckdb.sql(\"SELECT * FROM polars_df\")\n</code></pre> <p>What's returned is called Relation. duckdb can turn Relation back to dataframes/disk.</p> <pre><code>duckdb.sql(\"SELECT 42\").fetchall()   # Python objects\nduckdb.sql(\"SELECT 42\").df()         # Pandas DataFrame\nduckdb.sql(\"SELECT 42\").pl()         # Polars DataFrame\nduckdb.sql(\"SELECT 42\").arrow()      # Arrow Table\nduckdb.sql(\"SELECT 42\").fetchnumpy() # NumPy Arrays\n\nduckdb.sql(\"SELECT 42\").write_parquet(\"out.parquet\") # Write to a Parquet file\nduckdb.sql(\"SELECT 42\").write_csv(\"out.csv\")         # Write to a CSV file\nduckdb.sql(\"COPY (SELECT 42) TO 'out.parquet'\")      # Copy to a Parquet file\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#connection","title":"connection","text":"<pre><code>import duckdb\n\nwith duckdb.connect(\"file.db\") as con:\n    con.sql(\"CREATE TABLE test (i INTEGER)\")\n    con.sql(\"INSERT INTO test VALUES (42)\")\n    con.table(\"test\").show()\n    # the context manager closes the connection automatically\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#extensions","title":"extensions","text":"<p>to interoperate with sqlite </p> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.install_extension(\"sqlite\")\ncon.load_extension(\"sqlite\")\n</code></pre> <p>core extensions e.g. ducklake, azure, excel ...</p>","tags":["data engineering"]},{"location":"2025/06/07/duckdb-basics/#example","title":"example","text":"<p>in this example, we load csv files into a duckdb file, copy a table from an existing sqlite file into it using the  sqlite extension, then copy all tables in the duckdb file into a new sqlite file. </p> <p>the goal of this script is to use duckdb's schema inference to harmonize csv date format for sqlite to consume. The reason for the conversion to sqlite is that other parts of the codebase assumes sqlite db in the backend.   </p> <pre><code># script to load CSV files into DuckDB and convert to SQLite; use duckdb's schema inference.\n\nimport os\nimport glob\nimport re\nimport sqlite3\nimport duckdb\n\noriginal = \"og.db\" # assume there is a table called table_metadata in the db\n\ndef parse_filename(filename_string):\n    \"\"\"\n    remove leading numbers and dashes, replace non-alphanumeric characters by underscores.\n    \"\"\"\n    parsed_name = re.sub(r'^\\d+-', '', filename_string)\n    cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '_', parsed_name)\n    return cleaned_name\n\ndef load_csv_files_as_separate_tables(csv_folder_path, db_file_path):\n    \"\"\"\n    Loads each CSV file from a specified folder into its own separate table\n    in a DuckDB database file. The table name will be derived from the CSV filename.\n\n    Args:\n        csv_folder_path (str): The path to the folder containing the CSV files.\n        db_file_path (str): The path where the DuckDB database file will be created/stored.\n    \"\"\"\n    # Connect to DuckDB. If the file doesn't exist, it will be created.\n    with duckdb.connect(database=db_file_path, read_only=False) as con:\n        csv_files = glob.glob(os.path.join(csv_folder_path,'*.csv'))\n        if not csv_files: return\n        for csv_path in csv_files:\n            base_filename = os.path.splitext(os.path.basename(csv_path))[0]\n            table_name = parse_filename(base_filename)            \n            # auto_detect=TRUE is very helpful for inferring schema\n            con.execute(f\"\"\"\n                CREATE OR REPLACE TABLE \"{table_name}\" AS\n                SELECT * FROM read_csv('{csv_path}', auto_detect=TRUE);\n            \"\"\")\n\n\ndef convert_duckdb_to_sqlite(duckdb_file, sqlite_file):\n    \"\"\"\n    Converts a DuckDB database to a SQLite database by copying all tables.\n\n    Args:\n        duckdb_file (str): Path to the existing DuckDB database file.\n        sqlite_file (str): Path where the new SQLite database file will be created.\n    \"\"\"\n    con_duck = duckdb.connect(database=duckdb_file)\n    con_duck.execute(\"INSTALL sqlite; LOAD sqlite;\")\n    con_duck.execute(f\"ATTACH '{sqlite_file}' AS new_sqlite_db (TYPE sqlite);\") # empty \n    con_duck.execute(f\"ATTACH '{original}' AS original (TYPE sqlite);\") # contains table_metadata\n    con_duck.sql(\"\"\"CREATE OR REPLACE TABLE table_metadata as (select * from original.table_metadata)\"\"\")\n    duckdb_tables = con_duck.execute(\"SHOW TABLES;\").fetchall() # should see table_metadata in it\n\n    for table_name_tuple in duckdb_tables:\n        table_name = table_name_tuple[0]\n        con_duck.execute(f\"CREATE TABLE new_sqlite_db.\\\"{table_name}\\\" AS SELECT * FROM \\\"{table_name}\\\";\")\n\n    con_duck.close()\n    print(f\"Successfully converted '{duckdb_file}' to '{sqlite_file}'.\")\n\nif __name__ == \"__main__\":\n\n    csv_folder_path = \"./raw_datasets\"  \n    db_file_path = \"000_duck.db\"  \n    sqlite_file_path = \"001_sqlite.db\"\n\n    load_csv_files_as_separate_tables(csv_folder_path, db_file_path)\n    convert_duckdb_to_sqlite(db_file_path, sqlite_file_path)\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/","title":"SQLite: the absolute basics","text":"","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#routines","title":"Routines","text":"<p>create </p> <pre><code>CREATE TABLE t (\n    id INTEGER PRIMARY KEY,\n    title TEXT NOT NULL\n);\n\nINSERT INTO t (id, title) VALUES (1, 'hi'), (2, 'world');\n</code></pre> <p>read</p> <pre><code>SELECT * FROM t WHERE id=2;\n</code></pre> <p>update</p> <pre><code>ALTER TABLE t ADD COLUMN score REAL;\nUPDATE t SET score = 5.0 WHERE id = 1;\n</code></pre> <p>delete</p> <pre><code>DELETE FROM t WHERE id = 1 AND title = 'hi';\nDROP TABLE t;\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#basics","title":"Basics","text":"<ul> <li>limited storage classes (dtypes): <code>null</code>, <code>text</code>, <code>integer</code>, <code>real</code>, <code>blob</code></li> <li><code>create table (...) strict</code> here strict is sqlite specific because its default dtype behaviour is quite the opposite of strict.   </li> <li>constraints: <code>primary key</code>, <code>not null</code>, <code>unique</code>, <code>default</code>, <code>references</code>, <code>check</code> (e.g. price REAL CHECK(price &gt; 0))</li> <li>PRAGMA pragma_name; (to query a setting)</li> <li>PRAGMA pragma_name = value; (to set a setting)</li> <li>ACID compliant: atomicity, consistency, isolation, durability<ul> <li>PRAGMA foreign_key = on; # for integrity. </li> <li>PRAGMA journal_mode= WAL; # for concurrency</li> <li>PRAGMA synchronous = NORMAL;</li> </ul> </li> </ul>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#examples","title":"Examples","text":"<p>Mostly standard SQL</p>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#create-strict-table-and-index","title":"Create strict table and index","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,\n    username TEXT NOT NULL UNIQUE,\n    email TEXT UNIQUE,\n    password_hash TEXT,\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n) STRICT;\n\nCREATE INDEX idx_users_email ON users (email);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#compare-primary-key-and-unique","title":"Compare primary key and unique","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,           -- The unique, non-null, main identifier for each user\n    username TEXT NOT NULL UNIQUE,    -- Unique, but could theoretically be NULL if not specified NOT NULL\n    email TEXT UNIQUE,                -- Unique, but allows multiple NULL emails if some users don't provide one\n    phone_number TEXT UNIQUE,         -- Another unique attribute, allows NULL\n    last_login_ip TEXT\n);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#set-foreign-key","title":"Set foreign key","text":"<pre><code>CREATE TABLE users (\n    id INTEGER PRIMARY KEY,           -- Parent table's Primary Key\n    username TEXT NOT NULL UNIQUE,\n    email TEXT UNIQUE\n);\nCREATE TABLE posts (\n    id INTEGER PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT,\n    user_id INTEGER NOT NULL REFERENCES users (id),  -- defines the Foreign Key column\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/07/sqlite-the-absolute-basics/#parametrized-query","title":"Parametrized query","text":"<p>sqlite3 is a python interface for the SQLite C library.</p> <pre><code>import sqlite3\n\nwith sqlite3.connect(\"my.db\") as con:\n    username = request.args.get('username')\n    cursor = con.cursor()\n    cursor.execute(\"SELECT * FROM users WHERE username = ?\", (username,))\n</code></pre>","tags":["data engineering"]},{"location":"2025/06/15/basel-framework/","title":"Basel Framework","text":"<p>courtesy: Gemini 2.5 Flash</p> <p>The Basel Framework, developed by the Basel Committee on Banking Supervision (BCBS), provides international standards for banking regulation. It is built upon three pillars:</p> <ul> <li>Pillar 1: Minimum Capital Requirements</li> <li>Pillar 2: Supervisory Review Process</li> <li>Pillar 3: Market Discipline</li> </ul> <p>Here's a breakdown of the differences between Pillar 1 and Pillar 2:</p> <p>Pillar 1: Minimum Capital Requirements</p> <ul> <li>Focus: This pillar sets out the minimum amount of capital that banks must hold to cover specific, quantifiable risks.</li> <li>Risks Covered: Primarily covers:<ul> <li>Credit Risk: The risk of loss due to a borrower's failure to repay a loan or meet contractual obligations.</li> <li>Market Risk: The risk of losses in on- and off-balance-sheet positions arising from movements in market prices (e.g., interest rates, exchange rates, equity prices, commodity prices).</li> <li>Operational Risk: The risk of loss resulting from inadequate or failed internal processes, people, and systems, or from external events (e.g., fraud, system failures, legal risks).</li> </ul> </li> <li>Calculation: Banks can use either standardized approaches (prescribed by regulators) or, if approved, internal models (developed by the bank itself) to calculate their risk-weighted assets (RWAs) for these risks. The minimum capital requirement is typically a percentage (e.g., 8%) of these RWAs.</li> <li> <p>Nature: More rules-based and quantitative. It provides a standardized baseline for capital adequacy across banks.</p> </li> <li> <p>Primarily set out in the Capital Requirements Regulation (CRR). The CRR (Regulation (EU) No 575/2013, and its subsequent amendments like CRR II and CRR III) is a directly applicable EU law that lays down the detailed rules for banks' capital requirements, including the calculation of Risk-Weighted Assets (RWAs) for credit, market, and operational risks. It's a regulation because it's uniformly applied across all EU member states without needing national transposition.</p> </li> </ul> <p>Pillar 2: Supervisory Review Process</p> <ul> <li>Focus: This pillar addresses risks not fully captured by Pillar 1 and ensures that banks have adequate capital to support all material risks in their business, as well as robust risk management processes. It's about a more holistic and qualitative assessment of a bank's risk profile.</li> <li>Risks Covered:<ul> <li>Includes risks already covered in Pillar 1, but with a deeper assessment of how well the bank manages them (e.g., concentration risk within credit risk).</li> <li>Crucially, it also covers other material risks not fully or adequately captured by Pillar 1, such as:<ul> <li>Interest Rate Risk in the Banking Book (IRRBB)</li> <li>Liquidity Risk</li> <li>Strategic Risk</li> <li>Reputational Risk</li> <li>Business Model Risk</li> <li>Pension Risk</li> <li>Environmental, Social, and Governance (ESG) risks (increasingly relevant)</li> </ul> </li> </ul> </li> <li>Process: This involves:<ul> <li>Internal Capital Adequacy Assessment Process (ICAAP): Banks are required to conduct their own comprehensive assessment of their risks and determine the appropriate level of capital needed to cover them, along with a strategy for maintaining that capital.</li> <li>Supervisory Review and Evaluation Process (SREP): Supervisors review the bank's ICAAP and its overall risk profile. Based on this review, supervisors can require banks to hold additional capital above the Pillar 1 minimum (known as Pillar 2 capital requirements or add-ons), address deficiencies in their risk management, or take other supervisory actions.</li> </ul> </li> <li>Nature: More principles-based, qualitative, and tailored to the individual bank's specific circumstances. It allows for supervisory judgment and flexibility.</li> <li>Primarily set out in the Capital Requirements Directive (CRD). The CRD (Directive 2013/36/EU, and its subsequent amendments like CRD V) is an EU directive, which means member states must transpose its provisions into their national laws. The CRD provides the legal basis for the supervisory review process (SREP), including the requirement for banks to conduct an Internal Capital Adequacy Assessment Process (ICAAP) and for supervisors to assess these and potentially impose additional capital requirements (Pillar 2 Requirements - P2R) or guidance (Pillar 2 Guidance - P2G).</li> </ul> <p>Key Differences Summarized:</p> Feature Pillar 1: Minimum Capital Requirements Pillar 2: Supervisory Review Process Primary Goal To set a minimum, quantitative capital floor for specific, common risks. To ensure holistic capital adequacy and sound risk management. Scope of Risks Credit, Market, and Operational Risks (quantifiable). All material risks (including those not fully captured by Pillar 1). Approach Rules-based, standardized (or internal model-based but still prescriptive). Principles-based, judgmental, and bank-specific. Tool/Process Calculation of Risk-Weighted Assets (RWAs) and minimum capital ratios. Internal Capital Adequacy Assessment Process (ICAAP) and Supervisory Review and Evaluation Process (SREP). Output Minimum capital ratios. Bank-specific capital requirements (P2R), and supervisory guidance (P2G) on top of Pillar 1, and requirements for improved risk management. Enforcement Binding minimum capital ratios. Binding Pillar 2 requirements (P2R) and non-binding guidance (P2G). <p>In essence, Pillar 1 provides the foundational minimum capital rules, while Pillar 2 acts as a crucial complement, ensuring that banks not only meet these minimums but also proactively manage all their risks and hold sufficient capital for their specific risk profile, under the oversight of supervisors.</p>","tags":["Basel framework"]},{"location":"2025/06/17/exploring-a-large-database/","title":"Exploring a large database","text":"<p>Big organizations typically have their data stored in one or multiple database.  Sometimes, analytics team find themselves buried in the jungle of tables/views with non consistent naming conventions,  therefore hard to build a holistic view about where to find which data etc.  </p> <p>In this post we explore a few tricks to ease the initial phase of understanding a large database. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#having-applications-in-mind","title":"Having applications in mind","text":"<p>Very often in business, database are the backend of some frontend applications with GUI that one can interact with. Therefore, it is not a bad idea to use the frontend application first hand to see what's possible, the meaning of a column, how it relates to other columns etc.</p> <p>This can be a back-and-forth process, while doing analytics work, check the GUI to see where are the column names appear in the application, gradually completing a good mental picture of the 'geography' of the tables.  </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#learn-the-data-model-from-an-architect","title":"Learn the data model from an architect","text":"<p>Data architect is responsible for creating data model: how the business is broken into concepts (stored as tables) and how they relate (via foreign keys). If talking to an architect is an option, it is the fastest way of understaning the data model. </p> <p>Next we explore how to reverse engineer the data model. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#visualizations","title":"Visualizations","text":"<p>Some DB has their native visualization tools, ER generators (e.g. Oracle SQL developer). Not a bad idea to take advantage of that. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#use-metadata-tables","title":"Use metadata tables","text":"<p>Almost all database systems have a kind of information schema which contains tables that describe the metadata of the database such as the existing schemas, tables, views, columns etc. </p> <p>In Oracle it's <code>ALL_TABLES</code> <code>ALL_VIEWS</code> <code>ALL_TAB_COLUMNS</code> (not a schema per se, rather individual tables that start with <code>ALL_</code> prefix).  In Postgres/DuckDB/Snowflake it's the <code>information_schema</code>  schema, with tables such as <code>tables</code>, <code>columns</code> in it. </p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#foreign-keys","title":"Foreign keys","text":"<p>For obvious reasons, it is important to know these keys for the tables of interest. In Oracle, use <code>ALL_CONSTRAINTS</code> table. In Snowflake/DuckDB/Postgres, use <code>information_schema.REFERENTIAL_CONSTRAINTS</code>.</p>","tags":["data engineering"]},{"location":"2025/06/17/exploring-a-large-database/#describe-a-table","title":"Describe a table","text":"<p>Literally <code>DESCRIBE _NAME_</code> to show the column names and types. </p>","tags":["data engineering"]},{"location":"2025/07/07/random-forest-scaling-mini-benchmark/","title":"Random forest scaling mini-benchmark","text":"<p>My goal in this minibench is to see how random forest models scale with the sample size/feature size.</p> <p>I run the script at the end of this post on my Intel i5-10400 6 cores + 8GB RAM machine. I cannot scale sample size any larger i.e. 400k rows with this hardware. </p> <p>Software side, I have scikit-learn=1.7 and numpy=2.2.5 running on python3.12</p> <p>Here is the result </p> <p></p> <p>Two observations</p> <ul> <li>it is easier to scale the feature number than to scale the sample size</li> <li>there is some sort of crossover regime change in that 2x sample size requires 3x or more training time at larger sample size, something that is not happening when the sample size is small. </li> </ul> <pre><code>import time\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n\ndef generate_data(n_samples, n_features):\n    print(f\"Generating data: {n_samples} samples, {n_features} features...\")\n    X = np.random.randint(2, size=(n_samples, n_features))\n    y = np.random.randint(2, size=n_samples)\n    return X, y\n\ndef benchmark_training(X, y):\n    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n    start_time = time.time()\n    model.fit(X, y)\n    end_time = time.time()\n    return end_time - start_time\n\ndef run_benchmarks():\n    results = []\n\n    print(\"-\" * 50)\n    print(\"Benchmark 1: Increasing Number of Samples (Rows)\")\n    print(\"-\" * 50)\n    n_features_fixed = 50\n    sample_sizes = [1000, 5000, 10000, 50000, 100000, 200000, 300000]\n\n    for n_samples in sample_sizes:\n        try:\n            X, y = generate_data(n_samples, n_features_fixed)\n            training_time = benchmark_training(X, y)\n            results.append({\n                \"Test\": \"Increasing Samples\",\n                \"Samples\": n_samples,\n                \"Features\": n_features_fixed,\n                \"Bits\": n_samples * n_features_fixed,\n                \"Training Time (s)\": round(training_time, 4)\n            })\n            del X, y\n        except MemoryError:\n            print(f\"MemoryError at {n_samples} samples. Stopping this benchmark.\")\n            break\n\n    print(\"\\n\" + \"-\" * 50)\n    print(\"Benchmark 2: Increasing Number of Features (Columns)\")\n    print(\"-\" * 50)\n    n_samples_fixed = 10000\n    feature_sizes = [10, 50, 100, 500, 1000, 2000]\n\n    for n_features in feature_sizes:\n        X, y = generate_data(n_samples_fixed, n_features)\n        training_time = benchmark_training(X, y)\n        results.append({\n            \"Test\": \"Increasing Features\",\n            \"Samples\": n_samples_fixed,\n            \"Features\": n_features,\n            \"Bits\": n_samples_fixed * n_features,\n            \"Training Time (s)\": round(training_time, 4)\n        })\n        del X, y\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Benchmark Results Summary\")\n    print(\"=\" * 50)\n    df_results = pd.DataFrame(results)\n    print(df_results.to_string(index=False))\n\n\nif __name__ == \"__main__\":\n    run_benchmarks()\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/10/logging-in-python/","title":"Logging in python","text":"<p>The basic setup </p> <ol> <li>create a logger with a name</li> <li>define a handler </li> <li>set handler level</li> <li>set handler  format</li> <li>add handler to logger</li> <li>if needed, repeat 2-5 to define another handler and add it to logger</li> </ol> <p>We consolidate the above in a reusable function. Its functionality is only slightly more than minimal, but probably sufficient for 99% of a data person's use case. </p> <pre><code>import logging\n\ndef setup_logger(\n    name: str,\n    level=logging.DEBUG,\n    log_to_file=False,\n    filename=\"app.log\",\n    file_level=None  # optional file-specific level\n):\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n    # Clear any existing handlers (optional but helps in Jupyter)\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    # Console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    # Optional file handler with separate level\n    if log_to_file:\n        fh = logging.FileHandler(filename)\n        fh.setLevel(file_level if file_level else level)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n</code></pre> <p>Usage</p> <pre><code>logger = setup_logger(\"mylog\", level=10, log_to_file=True, file_level=30) # debug=10, warning=30\nlogger.info(\"30&gt;info=20&gt;= 10, go to console but NOT file\")\nlogger.error(\"error=40 &gt;= 30, go to console and file\")\n</code></pre>","tags":["dev tools"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/","title":"Hierarchical clustering with 36 LOC","text":"<p>Clustering is the action of dividing samples into subgroups. </p> <p>Hierarchical clustering is a bottom up method that gradually merge individual samples into bigger groups until all samples are merged into one group. The criterion for the merge is distance-based. For example, the single linkage algorithm merges clusters that are closest to each other where the distance between clusters is measured using Euclidean distance between sets. </p> <p>The abundant choice of distance functions lead to a zoo of HC algorithms, but the idea is the same. </p> <p>For example we have 5 samples indexed by 1, ..., 5. Each sample is a cluster of its own in the beginning. Let's say 1 and 3 are the closest among all pairs. We merge them and obtain clusters {1,3}, {2}, {4}, {5}. Then we compute the inter-cluster distance and, say, find the closest ones to be {1,3} and {2}.  We merge them to obtain clusters {1,2,3}, {4}, {5}. We repeat this process until there is only one cluster {1,2,3,4,5}. </p> <p>It is up to the user of the algorihm to decide the number of clusters needed for the problem at hands. Say the user wants 2 clusters, then they can use the 2 clusters just before the last merge. If they want 3 clusters, they can stop before the one but last merge, etc etc.</p> <p>Hopefully the idea is clear. Let's see how to implement it. We need to keep track of two things (in fact three, hold on for a second for the explanation):</p> <ul> <li>current cluster structure (a partition if you wish)</li> <li>current inter-cluster distances</li> </ul> <p>For the first, we can use a hashmap <code>dict[int, set]</code> where the key is the index of cluster, and the value is the collection of samples indices beloging to that cluster. </p> <p>For the second, we can use a heap (priority queue) so that the retrieval of minimum distance and its update is fast. For faster computation of inter-cluster distance, it is actually beneficial to have a copy of the distances stored as a hashmap too (hence 3 things to maintain). </p> <p>In summary, here's the plan of attack. </p> <p>Initialization: </p> <ul> <li>define each individual as one cluster </li> <li>compute pairwise distances and store them in a heap and a hashmap</li> </ul> <p>Main loop:</p> <ol> <li>retrieve smallest distance</li> <li>merge: update clusters hashmap</li> <li>compute distances: update distances heap and hashmap</li> </ol> <p>Here is all the code. Also a few comments just below. </p> <pre><code>import math \nimport heapq\n\ndef euclid(u,v):\n    return math.sqrt(sum((ui-vi)**2 for ui,vi in zip(u,v)))\n\ndef init(arr):\n    \"\"\"space O(m^2)\"\"\"\n    m = len(arr)\n    clusters = {i:{i} for i in range(m)}\n    dmap = {(i,j): euclid(arr[i],arr[j])\n        for i in range(m-1)\n        for j in range(i+1,m)\n    }\n    dheap = [(d,i,j) for (i,j), d in dmap.items()]\n    heapq.heapify(dheap)\n    return clusters, dmap, dheap\n\ndef main(arr, low):\n    \"\"\"time O(m^2 log m)\"\"\"\n    clusters, dmap, dheap = init(arr)\n    res = []\n    while len(clusters)&gt;low:\n        # retrieve smallest\n        while True: \n            d, i, j = heapq.heappop(dheap)\n            if i in clusters and j in clusters:\n                break\n        res.append((d,i,j))\n        # update clusters: + 1 - 2\n        new_idx = max(clusters) + 1\n        clusters.update({new_idx: clusters[i].union(clusters[j])})\n        clusters.pop(i)\n        clusters.pop(j)\n        # compute distance\n        for k in clusters:\n            if k != new_idx:\n                # update dmap O(1)\n                dmap[(k,new_idx)] = newtok = min(\n                    dmap[tuple(sorted([i,k]))],  \n                    dmap[tuple(sorted([j,k]))]\n                )\n                # udpate dheap O(log m)\n                heapq.heappush(dheap,(newtok,k,new_idx))\n    return clusters, res\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#why-heap","title":"Why heap?","text":"<p>A heap is a complete binary tree (stored in a sequence by arithmetic mapping between nodes and index). In python <code>heapq</code> implements the min heap which means that the root is the smallest element and  all parent node is smaller than the children nodes. The retrival of smallest element is O(1) operation. </p> <p>A heap must be dynamically maintained with the insertion and deletion of elements. Since the height of a complete binary tree is O(log n) for a tree with n elements, the cost of maintain the min heap structure when inserting (push) new element to the heap or removing (pop) smallest element is O(log n) (because an update is basically a chain of swapping operations between parent and child node, upwards / downwards). </p> <p>The python module <code>heapq</code> is in the stdlib with a simple API. To pop an element from the root: </p> <pre><code>heapq.heappop(dheap)\n</code></pre> <p>To push a new element into the heap:</p> <pre><code>heapq.heappush(dheap, element)\n</code></pre> <p>At initialization, one can turn a list into a heap all at once with O(n) time instead of push one by one which would require O(n log n) time</p> <pre><code>heapq.heapify(my_list)\n</code></pre>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#merge","title":"Merge","text":"<p>Notice that we mint a new index for the merged cluster, which becomes the hash key. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#inter-cluster-distance","title":"Inter-cluster distance","text":"<p>This is where different algorithms vary. What we have implemented is the single linkage algorithm.  </p> <pre><code>dmap[(k,new_idx)] = min(\n    dmap[tuple(sorted([i,k]))],  \n    dmap[tuple(sorted([j,k]))]\n)\n</code></pre> <p>The  complete  linkage algorithm amounts to take max instead of min in the above. Each choice has its own trade-off and focus. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#main-loop","title":"Main loop","text":"<p>The first while loop is simple: we merge one at a time until we reach <code>low</code> number of clusters.</p> <p>The inner while loop is necessary becasue when a merge happens, the old cluster index is removed from  <code>clusters</code> but they still exists in <code>dheap</code>. We keep removing them until we get both indices in the current <code>clusters</code> (as keys). </p> <p>The purpose of the for loop is to push distances of the new cluster with other clusters into <code>dheap</code>, as well as the <code>dmap</code>. </p>","tags":["quant methods"]},{"location":"2025/07/15/hierarchical-clustering-with-36-loc/#demo","title":"Demo","text":"<p>Here is an interactive demo. You can activate the app mode by pressing (Ctrl+ .)  The visualization is generated one-shot by Gemini 2.5 pro. </p> <p></p> <p>The source is here in case the link is not valid any more when you read this post. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/","title":"Unpacking the n-1 in the t test","text":"<p>If you ask an AI (below Grok 3 think mode) to explain the degree of freedom in a t test, or open an introductory statistics textbook you'll get explanation like this </p> <p>https://grok.com/share/bGVnYWN5_e1fcd789-991d-4593-beba-99a78a8f71c1</p> <p>This explainer is OK for someone who gets introduced to the topic for the first time - this had been how I taught the topic for a first year statistics class.  But there are a lot of hand-waving which may not be deemed satisfactory for someone who likes precise statement. </p> <p>My goal in this post is to spell out all the  details. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#fun-fact","title":"Fun fact","text":"<p>I'll start with a fun fact that may seem off topic. I promise it is not.</p> <p>Let \\(X,Y\\) be independent standard Gaussian random variables. Consider the mean \\((X+Y)/2\\) and the distance to the mean \\(X - (X+Y)/2 = (X-Y)/2\\). THe interesting fact is that they are independent Gaussians (with variance \\(1/2\\)). </p> <p>In other word knowing the mean does not provide any information  about the distance from the mean to a particular sample.  The fact can be proved by showing that the covariance is zero, which implies independence under Gaussian distribution (compute the Laplace transform to prove the latter).  </p> <p>Ok, now let's push the example a bit by considering \\(n\\) iid Gaussian \\(X_1,...,X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Analogously, consider </p> \\[  G = (X_i - \\bar X)_{i= 1,...,n} \\] <p>where \\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Note that this is a Gaussian vector.  It is a degenerate one because this random vector lives in a hyperplane \\(\\{x\\in\\mathbb{R}^n: \\sum_i^n x_i = 0 \\}\\). Again we can show that each coordinate of this Gaussian vector is independent of \\(\\bar X\\) by checking the covariance because of Gaussianity. Therefore \\(G\\) and \\(\\bar X\\) are independent.</p> <p>Recall that the sample variance is the scaled squared 2-norm of the vector \\(G\\) </p> \\[ \\hat\\sigma^2 = \\frac{1}{n-1}\\|G\\|^2 \\] <p>where the normalization \\(n-1\\) ensures that \\(\\hat\\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\). </p> <p>As a direct consequence of the independence of \\(G\\) and \\(\\bar X\\), we get independence of \\(\\hat\\sigma^2\\) and \\(\\bar X\\).</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#relate-to-t-statistic","title":"Relate to t statistic","text":"<p>t statistic for iid Gaussians of hypothesised population mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\) is given by</p> \\[ t = \\frac{\\bar X - \\mu}{\\hat\\sigma/\\sqrt{n}} \\] <p>From our discussion in the previous section, we know that the numerator and the denominator are independent random variables. Since \\(\\bar X\\) is Gaussian, we know that t statistic is actually a mixture of Gaussian with random variance \\(\\sigma^2/\\hat\\sigma^2\\).</p> <p>We know Gaussian very well but what is the distriubtion of the sample variance?</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#setting-up-the-target","title":"Setting up the target","text":"<p>The goal is to show</p> \\[ \\hat\\sigma^2 = \\sigma^2 \\frac{\\chi^2_{n-1}}{n-1} \\] <p>where \\(\\chi^2_d\\) is the chi-square distribution with \\(d\\) degrees of freedom.</p> <p>Ah, degree of freedom ! Hold on, we are close.</p> <p>Recall that \\(\\chi^2_d\\) can be defined as the sum of independent standard Gaussian squared where the degree of freedom is the number of terms in the sum. </p> <p>Looking at \\(\\hat\\sigma^2\\), we do have a sum of Gaussian squared, but they are not indepedent (\\(G\\) is degenerate) and there are \\(n\\) terms in the sum. We need to find a way to somehow reformualte it as a sum of \\(n-1\\) independent Gaussian squares.</p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#some-linear-algebra","title":"Some linear algebra","text":"<p>Notice that the distribution of \\(G\\) is invariant with respect to the value of \\(\\mu\\). We therefore can assume \\(\\mu=0\\). Our goal is to show </p> \\[ \\frac{\\|G\\|^2}{\\sigma^2} = \\chi^2_{n-1} \\] <p>We can write \\(G\\) in matrix form</p> \\[ G = A X \\] <p>where \\(A = I - (1/n)E\\) with \\(E\\) being \\(n \\times n\\) filled with 1 and \\(I\\) being the identity matrix.</p> <p>THe matrix \\(A\\) has a very interesting property</p> \\[ A^2 = A \\] <p>which can be checked easily. This property (aka idempotent) implies that all the eigenvalues are either 0 or 1. Indeed, if \\(Av = \\lambda v\\) for some non-zero vector \\(v\\), then \\(\\lambda v = Av = A^2v = A(Av) = \\lambda^2 v\\).  </p> <p>By eigenvalue decomposition, there exists orthonormal matrix \\(O\\) and diagomal matrix \\(\\Lambda\\) (filled with 0 and 1 on the diagonal because of idempotence) such that \\(A= O^T\\Lambda O\\), leading to </p> \\[ \\|G\\|^2 = \\langle AX, AX \\rangle = \\langle AX,X\\rangle = \\langle  \\Lambda OX, OX\\rangle = \\langle  \\Lambda X, X\\rangle \\] <p>where the last equality is distributional identity, using the fact that standard multivariate normal distribution is invariant under orthonormal transformation.  </p> <p>Therefore we have shown that  \\(\\|G\\|^2\\) can indeed be represented as the sum of independent mean-zero (variance \\(\\sigma^2\\)) Gaussian sqaured. The one last thing to check is the number of terms in the sum which corresponds to the number of ones in the spectrum of \\(A\\). This must be equal to the rank of \\(A\\) which is \\(n-1\\). CQFD.  </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#putting-all-together","title":"Putting all together","text":"<p>To conclude, we have</p> \\[ t = \\frac{N(0,1)}{\\sqrt{\\chi^2_{n-1} / (n-1)}} \\] <p>with independent numerator and denominator. We can now subscript \\(t\\) with parameter \\(n-1\\)  to indicate the degree of freedom on the right hand side of the equation. </p>","tags":["quant methods"]},{"location":"2025/07/23/unpacking-the-n-1-in-the-t-test/#a-slight-generalisation","title":"A slight generalisation","text":"<p>Consider two sample t test. Given \\(\\{X_1,...,X_n\\}\\) and \\(\\{Y_1,...,Y_m\\}\\) two collections of Gaussian with same variance \\(\\sigma^2\\). </p> <p>The null hypothesis is that the population mean of two samples are the same. The t statistic is </p> \\[ t = \\frac{\\bar X - \\bar Y}{      (1/n + 1/m) \\hat\\sigma_p } \\] <p>where \\(\\hat\\sigma_p^2 = \\frac{(n-1)\\hat\\sigma^2_X + (m-1)\\hat\\sigma^2_Y}{n+m-2}\\), the pooled sample variance. The numerator is the squared norm of a Gaussian vector </p> \\[(X_1 - \\bar X, .... X_n-\\bar X, Y_1 - \\bar Y, ...,  Y_m - \\bar Y)\\] <p>which lives in a \\((n+m-2)\\)-dimensional subspace. One checkes readily that this vector can be represented as an idempotent transformation of the (\\(m+n\\))-variate standard Gaussian \\((X_1,...,X_n, Y_1,...,Y_m)\\). Arguing as before, the squared norm can be written, via spectral decomposition, as a sum of independent squared Gaussians (i.e. chisquare). The number of terms in the sum coincide with the rank of the idempotent transformation which is \\(m+n-2\\). We have thus \\(t = \\frac{N(0,1)}{      \\sqrt{\\chi^2_{m+n-2} / (m+n-2)}}\\). </p>","tags":["quant methods"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/","title":"Use code when Dataiku's UI gets in the way","text":"","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#feed-partition-numbers-with-code","title":"Feed partition numbers with code","text":"<p>To programmatically feed partition numbers, do</p> <pre><code>import dataiku as dk\np = dk.api_client().get_project(\"MYPROJECT\")\np.get_recipe(\"MYRECIPE\").run(partitions=...)\n</code></pre> <p>This is particularly useful when the UI cannot autocomplete the partition keys (e.g. discrete values), but it is trivial to  write a python function/SQL query to retrive them. Also possible is to use the grammer for partition identifiers:</p> <p>https://doc.dataiku.com/dss/latest/partitions/identifiers.html</p> <p>Another way of doing it is through Job object. There are more configs which can be useful. </p> <p>https://developer.dataiku.com/latest/concepts-and-examples/jobs.html https://developer.dataiku.com/latest/api-reference/python/projects.html#dataikuapi.dss.project.DSSProject.list_jobs</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#access-custom-variables","title":"Access custom variables","text":"<p>There are several possible ways of accessing variables. For me, the one in the dataiku namespace is best for the ease of use</p> <pre><code>import dataiku as dk\n\ndk.get_custom_variables() \n</code></pre> <p>Indeed, although one gets the project level variables like this </p> <pre><code>dk.api_client().project(\"MYPROJECT\").get_variables()\n</code></pre> <p>the returned object is a nested dictionary, with standard variables dict and local variables dict inside it. </p> <p>As far as I can tell, <code>dk.get_custom_variables</code> has everything in it, containing both standard and local variables, and it is a flat dictionary. I prefer this one over the project-level variables dict. </p> <p>Yet another kind of variables are runtime variables. If we call the below in a notebook we get <code>None</code>. It may come in handy in a SQL/Python recipe and can be used to refer to a column that is the partition key (be it in the input table or the ouput table). </p> <pre><code>dk.dku_flow_variables # dict\n</code></pre>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#sql-recipe-that-takes-as-input-a-partitioned-dataset","title":"SQL recipe that takes as input a partitioned dataset","text":"<p>if I want to partition the output on a column that is not necessarily the input partition key</p> <p></p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#python-recipe-that-takes-as-input-a-partitioned-dataset","title":"Python recipe that takes as input a partitioned dataset","text":"<p>more rules about partition variable substitution</p> <p>https://doc.dataiku.com/dss/latest/partitions/variables.html</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/01/use-code-when-dataikus-ui-gets-in-the-way/#partition-dependency-function","title":"Partition dependency function","text":"<p>Haven't use it in my projects but I can see it being flexible in specifying the dependency of input and output partitions. </p> <p>https://doc.dataiku.com/dss/latest/partitions/dependencies.html</p>","tags":["dev tools","data engineering"]},{"location":"2025/08/05/vasicek-in-basel/","title":"Vasicek in Basel","text":"<p>be me be Vasicek chill Czech dude decide to model credit risk with Brownian motion invent one-factor Gaussian copula before it was cool \"default happens when latent variable crosses threshold\" everyone claps  </p> <p>fast-forward Basel Committee steals my idea slaps some capital requirement on it banks now simulating millions of standard normals get no royalties</p> <p>Merton out here pricing corporate debt with structural models I just make it rain closed-form expressions still gets all the fame  </p> <p>Moody\u2019s KMV be like \u201cthanks for the latent variable bro\u201d turn it into a product banks pay millions I get academic citation  </p> <p>still better than being Black or Scholes at least nobody\u2019s blowing up hedge funds with my model yet.  </p> <p>(prose courtesy ChatGPT)</p> <p>This post is not about Vasicek's life. It is about a legacy Vasicek left for the banking regulations.</p>","tags":["quant methods","Basel framework"]},{"location":"2025/08/05/vasicek-in-basel/#capital-requirement","title":"Capital requirement","text":"<p>The above formula figures in the Basel framework for regulatory capital computation. You see the Gaussian distribution function, the probability of default, the correlation of obligors's asset value with a common factor for all obligors such as economic situation.</p> <p>Where is this coming from?   </p> <p>Reference</p> <p>http://mx.nthu.edu.tw/~jtyang/Teaching/Risk_management/Papers/Models/Probability%20of%20Loss%20on%20Loan%20Portfolio.pdf</p> <p>https://www.bankofgreece.gr/MediaAttachments/Vasicek.pdf</p>","tags":["quant methods","Basel framework"]},{"location":"2025/08/05/vasicek-in-basel/#one-factor-model","title":"One factor model","text":"<p>Capital is required to cover unexpected loss. Mathematically, the expected loss just means the expectation of the loss of a portfolio of instruments. The regulatory unexpected loss can be roughly translated to the difference between the quantile of the loss distribution and the expected value of the loss.</p> <p>The one factor model postulates the following, the asset value of obligor \\(i\\) is the </p> \\[ A_i = \\sqrt{\\rho} X + \\sqrt{1-\\rho} \\epsilon_i \\] <p>where \\((X,\\epsilon_i, i=1,2,...)\\) is iid Gaussian standard normal, and \\(X\\) represents the common factor that influences the value of all assets. Notice that \\((A_i)\\) are standard Gaussian correlated with correlation \\(\\rho\\).  </p> <p>Default event occurs if the asset value falls below a certain threshold \\(B_i\\). The probability of default given \\(X\\) is therefore</p> \\[ P[A_i&lt; B_i|X] = \\Phi(\\frac{B_i -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}}) \\] <p>where \\(\\Phi\\) is the CDF of a standard Gaussian. </p> <p>In real life, the (unconditional) probability of default may come from an internal model (say a logistic regression on obligor characteristics), denoted by \\((p_i)\\)</p> \\[ p_i = P[A_i&lt;B_i] \\] <p>namely </p> \\[ B_i  = \\Phi^{-1}(p_i). \\]","tags":["quant methods","Basel framework"]},{"location":"2025/08/05/vasicek-in-basel/#law-of-large-numbers","title":"Law of large numbers","text":"<p>When the number of obligors in a portfolio is large, the law of large numbers kicks in,</p> \\[ P\\Big[\\frac{1}{n}\\sum_{i=1}^n I(A_i&lt;\\Phi^{-1}(p_i)) \\to  \\Phi(\\frac{B_i -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}})|X \\Big] =1 \\] <p>Assume that the loss incured by a default event is 1/n across all obligors in the portfolio, then the loss is </p> \\[ L = \\frac{1}{n}\\sum_{i=1}^n I(A_i&lt;\\Phi^{-1}(p_i)) \\approx  \\Phi(\\frac{\\Phi^{-1}(p_i) -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}}). \\]","tags":["quant methods","Basel framework"]},{"location":"2025/08/05/vasicek-in-basel/#quantile-of-the-loss","title":"Quantile of the loss","text":"<p>We show that </p> \\[ q_{L,\\alpha} \\approx \\Phi(\\frac{\\Phi^{-1}(p_i) - \\sqrt{\\rho} q_{X,1-\\alpha}}{\\sqrt{1-\\rho}}) \\] <p>where  \\(q_{D,\\alpha}\\) is the \\(\\alpha\\) quantile of random variable \\(D\\), therefore \\(q_{X,1-\\alpha} = \\Phi^{-1}(1-\\alpha)= -\\Phi^{-1}(\\alpha)\\). The Basel formula follows right off with \\(\\alpha = 0.999\\). </p> <p>Indeed, we are interested in the 0.999 quantile of \\(L\\) which is approximately a monotone function of the Gaussian \\(X\\). Using the approximation, we have </p> \\[ \\begin{align*} 0.999 = P[L\\le q] &amp; \\approx P[\\Phi(\\frac{\\Phi^{-1}(p_i) -\\sqrt{\\rho}X}{\\sqrt{1-\\rho}})\\le q] \\\\ &amp; = P\\Big[X\\ge \\frac{\\Phi^{-1}(p_i) - \\sqrt{1-\\rho} \\Phi^{-1}(q)}{\\sqrt{\\rho}}\\Big] \\\\ &amp;= 1 - \\Phi( \\frac{\\Phi^{-1}(p_i) - \\sqrt{1-\\rho} \\Phi^{-1}(q)}{\\sqrt{\\rho}}) \\end{align*} \\] <p>Rearranging terms</p> \\[ \\Phi^{-1}(q) = \\frac{ \\Phi^{-1}(p_i) - \\sqrt{\\rho}\\Phi^{-1}(0.001)  } {\\sqrt{1-\\rho} }. \\] <p>Hence,</p> \\[ q = \\Phi(\\frac{\\Phi^{-1}(p_i)+\\sqrt{\\rho}\\Phi^{-1}(0.999)}{\\sqrt{1-\\rho}})   \\] <p>as desired. </p>","tags":["quant methods","Basel framework"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/","title":"Unpacking the k-1 in the chi-square test","text":"<p>This post is a continuation of my previous one about t-test. The aim, as before, is to spell out all the details about the degrees of freedom in the chi-square test. </p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#setting","title":"Setting","text":"<p>Let \\(\\mathbf{X} = (X_1, \\dots, X_k)\\) be a multinomial random vector with \\(k\\) categories, total count \\(n\\), and probabilities \\(\\mathbf{p} = (p_1, \\dots, p_k)\\). Pearson\u2019s chi-square test statistic is</p> \\[ \\chi^2 = \\sum_{i=1}^k \\frac{(X_i - n p_i)^2}{n p_i}. \\] <p>As \\(n \\to \\infty\\), the distribution of \\(\\chi^2\\) converges to a chi-square distribution with \\(k - 1\\) degrees of freedom.</p> <p>The goal of this post is to explain where the \\(k - 1\\) comes from.</p> <p>This post can be read independently of my previous one on the t-test, but a core part of the argument was already presented there, so reading that first might help.</p> <p>Core ideas:</p> <ul> <li>Represent the multinomial vector as a sum of i.i.d. random vectors (so normal approximation applies).</li> <li>The chi-square statistic becomes close to the squared norm of a Gaussian vector lying in a \\((k - 1)\\)-dimensional subspace.</li> <li>Spectral decomposition shows that this squared norm is distributed as a sum of \\(k - 1\\) independent standard normal squares.</li> </ul> <p>That last part is a generalization of the linear algebra trick we used in the t-test post.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#multivariate-central-limit-theorem","title":"Multivariate central limit theorem","text":"<p>Let \\(E = \\{e_1, \\dots, e_k\\}\\) be the canonical basis of \\(\\mathbb{R}^k\\) where each \\(e_i\\) has a 1 in the \\(i\\)-th position and 0 elsewhere.</p> <p>Let \\(I_i\\) be a random vector taking values in \\(E\\) such that</p> \\[ P[I_i = e_j] = p_j. \\] <p>Then</p> \\[ (X_1, \\dots, X_k) = \\sum_{i=1}^n I_i. \\] <p>The mean of this sum is \\(n \\mathbf{p}\\). Its covariance, normalized by \\(n\\), does not depend on \\(n\\), so we denote it by \\(\\Sigma = \\Sigma(\\mathbf{p})\\).</p> <p>By the multivariate central limit theorem:</p> \\[ \\frac{1}{\\sqrt{n}}\\left( \\sum_{i=1}^n I_i - n \\mathbf{p} \\right) \\to \\mathcal{N}(0, \\Sigma). \\] <p>Let \\(G\\) denote this limiting Gaussian vector. Then the chi-square statistic is asymptotically approximated by</p> \\[ \\sum_{i=1}^k \\frac{G_i^2}{p_i} = \\left\\| \\left( \\frac{G_1}{\\sqrt{p_1}}, \\dots, \\frac{G_k}{\\sqrt{p_k}} \\right) \\right\\|^2. \\] <p>We now want to show that this norm squared follows a chi-square distribution with \\(k - 1\\) degrees of freedom.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#checking-the-covariance","title":"Checking the covariance","text":"<p>We claim:</p> \\[ \\text{Cov}[X_i, X_j]/n = p_i \\delta_{ij} - p_i p_j. \\] <p>Hence, the Gaussian vector \\(H = (G_1 / \\sqrt{p_1}, \\dots, G_k / \\sqrt{p_k})\\) has covariance matrix</p> \\[ V_{ij} = \\delta_{ij} - p_j. \\] <p>So we\u2019ve shown that \\(\\chi^2\\) converges in distribution to \\(\\|H\\|^2\\). The rest of the argument is nearly identical to the t-test post, so I'll just sketch it here.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#linear-algebra-again","title":"Linear algebra again","text":"<p>Note that \\(V = V^2\\), so it is a projection matrix. Its eigenvalues are either 0 or 1.</p> <p>We can write \\(H = VZ\\), where \\(Z\\) is a standard Gaussian in \\(\\mathbb{R}^k\\). Then \\(\\|H\\|^2 = \\|VZ\\|^2\\) is the sum of squares of the projections of \\(Z\\) onto the image of \\(V\\).</p> <p>Spectral decomposition lets us write this as a sum of \\(r\\) independent standard normal squares, where \\(r = \\text{rank}(V)\\).</p> <p>We now check that \\(\\text{rank}(V) = k - 1\\).</p> <p>Recall:</p> \\[ \\Sigma = \\text{diag}(\\mathbf{p}) - \\mathbf{p} \\mathbf{p}^\\top. \\] <p>So if \\(\\Sigma x = 0\\), then necessarily \\(x_1 = x_2 = \\dots = x_k = \\sum_i x_i p_i\\). That is, the null space consists of constant vectors, hence it's one-dimensional.</p> <p>Therefore, \\(\\text{rank}(\\Sigma) = k - 1\\), and since \\(V\\) is derived from \\(\\Sigma\\), we also have \\(\\text{rank}(V) = k - 1\\).</p> <p>So \\(\\|H\\|^2\\) is the sum of \\(k - 1\\) independent standard normal squares, namely, chi-square distributed with \\(k - 1\\) degrees of freedom. CQFD.</p>","tags":["quant methods"]},{"location":"2025/08/06/unpacking-the-k-1-in-the-chi-square-test/#generalization-again","title":"Generalization again","text":"<p>Now consider contingency tables which are pivot tables of bivariate categorical data. We assume iid sequence \\(\\{(X_i,Y_i), i=1,...,n \\}\\). Both are categorical with \\(a\\) and \\(b\\) categories respectively. The data can be stored as a table of shape (n,2). Pivoting on the second column (Y's) with the first column (X's) as index, we get frequencies of each one of the \\(ab\\) combinations. The pivot table is of shape (a,b). In dataframe land, this is done as follows:</p> <pre><code>import polars as pl\nimport numpy as np\n\nx = pl.from_numpy(np.random.randint(0,2,size=(10,2)))\nx.pivot(on=\"column_1\",index=\"column_0\",values=\"column_0\",aggregate_function=\"len\")\n</code></pre> <p>The cells in the pivot table sum to n.</p> <p>It is again possible to express the frequencies as sum of \\(n\\) iid random matrices \\(A_1, \\dots, A_n\\). Each is of shape (a,b) matrix. If \\((X_1,Y_1)=(k,l)\\) then \\(A_{1,kl}=1\\) and 0 elsewhere. Again we have multinomial distribution with parameters \\((n, p_{ij})\\) with \\(p_{ij}=P[X_1=i, Y_1=j]\\). </p> <p>Except from a fancier way of indexing n values (using 2d array), the situation is exactly the same as before, so that when n get larger, we can approximate the statistic</p> \\[ C = \\sum_{1\\le i\\le a, 1\\le j\\le b} \\frac{(X_{ij}-n p_{ij})^2}{n p_{ij}} \\] <p>by a chisquare with \\(ab-1\\) degrees of freedom. </p> <p>In fact, a common setup for contingency tables is to assume independence between variables. In such case the matrix \\((p_{ij})\\) is rank 1 because \\(p_{ij}=r_i q_j\\) where \\(\\mathbf{r} = (r_1, \\dots, r_a)\\) and \\(\\mathbf{q} = (q_1, \\dots, q_b)\\) are marginal distributions of the pair \\((X_1,Y_1)\\).</p> <p>We claim that \\(C\\) is asymptotically chisquare with degrees of freedom \\((a-1)(b-1)\\). It suffices to show that the covariance matrix of \\((X_{ij})\\) (a projection as we have shown already), divided by \\(n\\),  is of rank \\((a-1)(b-1)\\). We can show this by investigating the null space of the covariance matrix (show it's of dimension \\(a+b-1\\)). We leave this as an exercise to the interested reader </p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/","title":"Monotonic binning with PAVA","text":"<p>In credit-risk modelling, a common requirement is to maintain monotonicity of the target variable with respect to risk drivers. Think PD modelling where the target is the probability of default and the risk drivers are customer characteristics and/or their financial situation.</p> <p>A useful technique for this is monotonic binning. The idea is to bin the continuous variable in some user-defined way (e.g., evenly-spaced breakpoints or quantile-based), then merge consecutive bins so that their default rates are monotonic.</p> <p>PAVA, or the pooled adjacent violators algorithm, is a classic method for solving optimization problems with monotonicity constraints, such as isotonic regression. In this post, we adapt its core idea to solve the monotonic-binning problem.</p> <p>We\u2019ve simplified the implementation as much as possible. What you see below is just the bare bones. Even though the code is short, it deserves some analysis for its complexity, which we\u2019ll get to next.</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#pava","title":"PAVA","text":"<p>We maintain two dynamic states:</p> <ul> <li>A list of pools that represents current clusters</li> <li>A list of values that stores the statistics (number of defaults and total in each cluster)</li> </ul> <p>We iteratively compare the current pool and the next one (based on their values):</p> <ul> <li>If the constraint is met, we advance by one index.</li> <li>If the constraint is violated, we merge the current and next pools and backtrack to check again. We keep merging and backtracking as needed until the constraint holds.</li> </ul> <p>That\u2019s it!</p> <pre><code>def pava(x, constraint=lambda a, b: a[0] / a[1] &lt;= b[0] / b[1]):\n    \"\"\"x : list of pairs of counts [(n_default, total), ...]\"\"\"\n    n = len(x)\n    active_pools = [[i] for i in range(n)]\n    active_values = list(np.asarray(x))\n\n    i = 0\n    while i &lt; len(active_pools) - 1:\n        if not constraint(active_values[i], active_values[i + 1]):\n            # Violation found, merge pools\n            active_pools[i].extend(active_pools[i + 1])\n            del active_pools[i + 1]\n            active_values[i] = active_values[i] + active_values[i + 1]\n            del active_values[i + 1]\n            # Step back to check for new violations with the merged pool\n            if i &gt; 0:\n                i -= 1\n        else:\n            i += 1\n    return active_pools\n</code></pre>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#complexity","title":"Complexity","text":"<p>Because the index can both advance and backtrack, it's not immediately obvious what the time complexity is.</p> <p>We can think of the process as alternating between two phases:</p> <ol> <li>A forward merge occurs, followed by backtracking and merging until monotonicity is restored.</li> <li>Or, if no merge is needed, the index keeps advancing by 1 until it encounters a violation that forces a merge.</li> </ol> <p>Index advancement happens at most n times, because merging doesn\u2019t affect how many positions remain ahead of the current index. Every time we advance, there are fewer spots left to advance into.</p> <p>Let \\(b_i\\) be the number of backtracks during the i-th forward-merge round. We can show:</p> \\[ \\sum_{i=1}^r b_i = r + \\sum_{i=1}^r (b_i - 1) \\le \\#\\{\\text{forward merges}\\} + \\#\\{\\text{backward merges}\\} \\le \\#\\{\\text{merges}\\} \\le n \\] <p>Since merges\u2014and thus backtracks\u2014occur at most n times and each merge is \\(O(1)\\), the overall complexity is \\(O(n)\\).</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#the-catch","title":"The catch","text":"<p>A sharp reader will notice that removing elements from a Python list (<code>del</code>) inside the loop is actually \\(O(n)\\), so the binning algorithm as written is \\(O(n^2)\\).</p> <p>To truly achieve \\(O(n)\\) complexity, you\u2019d need a data structure that supports \\(O(1)\\) element removal (like a linked list), which we\u2019re not using here for clarity.</p>","tags":["quant methods"]},{"location":"2025/08/23/monotonic-binning-with-pava/#bonus-stack-based-approach","title":"Bonus: stack-based approach","text":"<p>Since rewriting the function <code>pava</code> with a custom linked-list feels a bit obscure, I ask an AI (Gemini 2.5 pro) to find an alternative</p> <p>i feel that the code with linked link is harder to follow because of all the method calling swapping things around. is there a cleaner way?</p> <p>It\u2019s as short as the original but achieves \\(O(n)\\) without needing a linked list:</p> <pre><code>def pavai(x, constraint=lambda a, b: a[0] / a[1] &lt;= b[0] / b[1]):\n    \"\"\"\n    x: list of pairs of counts, e.g., [(numerator, denominator), ...]\n    \"\"\"\n    if not x:\n        return []\n\n    stack = []  # stores tuples of (value, indices) for each active pool\n\n    for i, val in enumerate(x):\n        current_value = np.asarray(val)\n        current_indices = [i]\n\n        # --- Backwards Merging ---\n        # While the stack isn\u2019t empty and the top pool violates the constraint\n        # with the current pool, merge them.\n        while stack and not constraint(stack[-1][0], current_value):\n            prev_value, prev_indices = stack.pop()\n            # Merge it into the current pool\n            current_value += prev_value\n            current_indices = prev_indices + current_indices\n\n        # Now that all violations are resolved, push the new pool onto the stack\n        stack.append((current_value, current_indices))\n\n    # --- Finalization: unpack the stack into the final list of pools ---\n    final_pools = [indices for value, indices in stack]\n\n    return final_pools\n</code></pre>","tags":["quant methods"]},{"location":"2025/08/28/a-patching-pattern/","title":"A patching pattern","text":"<p>When some functionality is not quite what you expected from a library, or you wanted to enhance it with some additonal features, a useful pattern is to patch them dynamically. </p> <p>Here are a few examples</p>","tags":["dev tools"]},{"location":"2025/08/28/a-patching-pattern/#example-1-add-a-lookup-method-to-dataframe","title":"Example 1: add a lookup method to DataFrame","text":"<p>As a cute utility function, we can add a lookup method to a DataFrame, which returns the column names that contains the queried string</p> <pre><code>import polars as pl\n\ndef lookup(self, name:str):\n    return [c for c in self.columns if name.lower() in c.lower()]\n\npl.DataFrame.lookup = lookup\n</code></pre> <p>and use it like so </p> <pre><code>df = pl.DataFrame({\"apple\":[1], \"pear\":[2]})\nassert df.lookup(\"app\") == [\"apple\"]\n</code></pre>","tags":["dev tools"]},{"location":"2025/08/28/a-patching-pattern/#example-2-change-the-formatting-of-ttestresult","title":"Example 2: change the formatting of TtestResult","text":"<p>We may want to display the result differently than the default e.g. with a theme or formatting.  We can overwrite the TtestResult class in the global scope, but here we do it with a context manager.    </p> <pre><code>import contextlib\nfrom scipy.stats._stats_py import TtestResult\n\n@contextlib.contextmanager\ndef add_html_repr_to_ttest():\n\n    original_repr_html = getattr(TtestResult, '_repr_html_', None)\n\n    def custom_repr_html(self):\n        ...\n        return ...\n\n    setattr(TtestResult, '_repr_html_', custom_repr_html)\n\n    try:\n        yield\n    finally:\n        if original_repr_html is None:\n            delattr(TtestResult, '_repr_html_')\n        else:\n            setattr(TtestResult, '_repr_html_', original_repr_html)\n</code></pre> <p>and use it like so</p> <pre><code>from scipy.stats import ttest_ind\n\nwith add_html_repr_to_ttest():\n    result = ttest_ind([1, 2, 3], [4, 5, 6])\n    display(result)  \n</code></pre>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/","title":"Cheatsheet for regular expression","text":"<p>Regex is a DSL with a very small vocabulary. It is so common that everyone has to deal with it, sooner or later. I have finally reached to the point that I want to know the exact rules instead of consulting an AI for even very simple regex patterns. So here we are, a cheatsheet.  </p> <p>In the most common cases, the goal is to retrieve a substring out of a given string that presents some pattern. Here are the tools at our disposal. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#anchors","title":"anchors","text":"<p>^ matches the start of the string and $ the end. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#characer-class","title":"characer class","text":"<p>Use square bracket [] inside which we put the character we want to match e.g. </p> <ul> <li>[abc] matches any of the three characters a,b,c</li> <li>[a-zA-Z] matches an alphabet lower/upper case</li> </ul>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#grouping","title":"grouping","text":"<p>Use round bracket () inside which we put the entire string we want to match e.g.</p> <p><pre><code>(awesome)\n</code></pre> matches exactly awesome</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#quantifiers","title":"quantifiers","text":"<p>To be composed with character class or grouping,</p> Quantifier Meaning * 0 or more + 1 or more ? 0 or 1 {n} Exactly n times {n,} n or more {n,m} Between n and m times <p>For example,</p> <p><pre><code>(go)+\n</code></pre>  would match go, gogo, gogogo etc</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#escape-with","title":"escape with <code>\\</code>","text":"<p>Quantifiers and backets are special characters that we may want to match too. To do this use backslash to escape like so </p> <pre><code>\\([0-9]?\n</code></pre> <p>to match an openning bracket followed by one or zero digits. </p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#shorthand","title":"shorthand","text":"Shorthand Meaning Matches \\d Digit [0-9] \\D Non-digit [^0-9] \\w Word character [a-zA-Z0-9_] \\W Non-word character [^a-zA-Z0-9_] \\s Whitespace character [ \\t\\r\\n\\f\\v] \\S Non-whitespace character [^ \\t\\r\\n\\f\\v] . Any character (except \\n)","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#or-with-negation-with","title":"OR with | ,   NEGATION with ^","text":"<p>(live|die) matches live, it also matches die. We already saw  the use of negation in the shorthand table.</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#example-match-the-endpoints-in-intervals","title":"Example: match the endpoints in intervals","text":"<p>We have strings like this</p> <pre><code>(-inf,1.1]\n(1.1,6]\n(6,inf]\n</code></pre> <p>We can retrieve the left endpoint like so  </p> <pre><code>import re\n\ns = \"(-inf, 1.1]\"\nmatch = re.match(r\"\\(([^,]+),\", s)\nif match:\n    print(match.group(1))\n</code></pre> <p>here <code>.group(1)</code> means we retrieve the first appearance of the grouping, which is what we want. </p> <p>In polars we can </p> <pre><code>import polars as pl\n\npattern = r\"\\(([^,]+),\"\npl.select(pl.col(\"interval\").str.extract(pattern))\n</code></pre> <p>where <code>.extract</code> method has the default <code>group=1</code>.</p>","tags":["dev tools"]},{"location":"2025/09/02/cheatsheet-for-regular-expression/#bonus-gpt4-split-pattern","title":"Bonus: GPT4 split pattern","text":"<p>Try this </p> <pre><code>GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n</code></pre> <p>This is used in the BPE algorithm as the initial step to split a large chunk of text into words, handing punctuation, spaces, unicode etc. </p> <p>Reference</p> <p>https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py</p>","tags":["dev tools"]},{"location":"2025/09/07/repurpose-hierarchical-clustering/","title":"Repurpose hierarchical clustering for feature engineering","text":"<p>A while ago, I wrote about hierarchical clustering with a focus on clean implementation of this classic method. </p> <p>In this short post, my goal is to repurpose it for consolidation of categorical features based on hypothesis testing. By consolidation I mean merge together values of a categorical variable which behave similarly with regard to the target variable (think default tag in credit risk modelling) in some statistical sense. </p> <p>The idea is simple: when we merge clusters in the bottom up fashion a la hierarchical clustering, we could use p-value as a \"distance\". Here distince is in quote because p-value should actually be viewed as a similarity measure: lower p-value would reject the null that two clusters have the same distribution, therefore suggesting a large distributional distance. On the other hand, higher p-value would suggest a low distributional distance. </p> <p>The most similar clusters would have the highest p-value, which we merge first. The clusering stops once p-values for all pairs of current clusters is lower than a pre-set threshold, say 0.05, indicating pairwise heteogeneity.</p> <p>Implementation is \\(\\varepsilon\\)-away from the original one. </p> <pre><code>import heapq\nimport numpy as np\nfrom scipy.stats import ks_2samp \n\ndef ks_test(x, y):\n    \"\"\"ks_2samp expects 1d arrays. OK if X is 1d to begin with.\n    we can implement a customized test depending on needs.  \n    \"\"\"\n    return ks_2samp(x, y).pvalue\n\ndef test_clustering(X, test_func=ks_test, alpha=0.05):\n    clusters = {i: [i] for i in range(len(X))}\n    heap = []\n    for i in clusters:\n        for j in clusters:\n            if i &lt; j:\n                p = test_func(clusters[i], clusters[j])\n                heapq.heappush(heap, (-p, i, j))\n    while heap:\n        negp, i, j = heapq.heappop(heap)\n        p = -negp\n        if i not in clusters or j not in clusters:\n            continue\n        if p &lt;= alpha:\n            break\n        clusters[i].extend(clusters[j])\n        del clusters[j]\n        for k in clusters:\n            if k != i:\n                p = test_func(X[clusters[i]], X[clusters[k]])\n                heapq.heappush(heap, (-p, i, k))\n    return list(clusters.values())\n</code></pre>","tags":["quant methods"]},{"location":"2025/10/31/jax-as-a-differential-calculus-tool/","title":"JAX as a differential calculus tool","text":"<p>Back in January, I wrote a tutorial about JAX, highlighting its power in high performance computing and its near-mathematical syntax. Now I show how to use JAX as a differential calculus tool for students and educators. </p> <p>The goal is to approximate a differentiable function by a few terms in its Taylor expansion near a fixed point. The neat mathematical statement is </p> \\[ f(x) - f(x_0) = \\nabla f(x_0)(x-x_0) +  \\frac{1}{2}\\langle Hf(x_0) (x-x_0), (x-x_0)\\rangle + O(\\|x-x_0\\|^3) \\] <p>where \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) is sufficiently differentiable,  \\(\\nabla f\\) is the gradient of \\(f\\) and \\(Hf\\) is its Hessian matrix. </p> <p>Based on the expansion, we can use the linear term (first on the right) or the quadratic form (first two terms on the right) to approximate \\(f\\).  We can go further down the expansion too. In the code below we go down the quadratic route.  </p> <p>The key steps of the experiment</p> <ol> <li>define Ackley function</li> <li>define quadratic approximation of a generic function around an arbitrary point</li> <li>compute the diff between a function with its quadratic approximation</li> <li>evaluate the diff over a few scales.  </li> </ol> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\n\njax.config.update(\"jax_enable_x64\", True)\n\ndef f(x):\n    \"\"\"Ackley function\"\"\"\n    a = 20.0\n    b = 0.2\n    c = 2 * jnp.pi\n    d = x.size\n\n    sum_sq_term = -b * jnp.sqrt(jnp.sum(x**2) / d)\n    sum_cos_term = jnp.sum(jnp.cos(c * x)) / d\n\n    term1 = -a * jnp.exp(sum_sq_term)\n    term2 = -jnp.exp(sum_cos_term)\n\n    return term1 + term2 + a + jnp.exp(1.0)\n\ndef approx(f, xo):\n    df = jax.grad(f)\n    hf = jax.jacobian(df)\n    def Q(x):\n        return f(xo) + df(xo).dot(x-xo) +  0.5* hf(xo).dot(x-xo).dot(x-xo) \n    return Q\n\ndef diff(x,xo):\n    return approx(f,xo)(x) - f(x)\n\nd = 3\nxo, u = jr.normal(jr.key(0), (2,d))\nscales = jnp.logspace(-1,-5,5) # 1e-1, 1e-2, ... ,1e-5\nxs = xo + u * scales[:,None] # broadcast to (d,5)\n\nerrors = jax.vmap(diff, in_axes=(0,None))(xs, xo) # (5,) one for each scale\n\nprint(errors)\n</code></pre> <pre><code>[1.01747675e-03 1.03140167e-06 1.03225695e-09 1.03295150e-12\n 8.88178420e-16]\n</code></pre> <p>We observe that, roughly speaking, as we reduce the distance from x to xo by 10x we see an improvement of the approximation by 1000x, which  confirms the cubic term in Taylor expansion described above. </p> <p>From the example above you see how easy it is to use Jax for showcasing differential caculus results. We can imagine its use in solving differential equations or designing optimization algorithm. Check out jax based projects <code>diffrax</code> and <code>optax</code> for those use case.</p>","tags":["low latency programming"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/analysis/","title":"Analysis","text":""},{"location":"category/tutorial/","title":"Tutorial","text":""},{"location":"category/til/","title":"TIL","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"page/3/","title":"Blog","text":""},{"location":"page/4/","title":"Blog","text":""},{"location":"page/5/","title":"Blog","text":""},{"location":"page/6/","title":"Blog","text":""},{"location":"page/7/","title":"Blog","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2025/page/3/","title":"2025","text":""},{"location":"archive/2025/page/4/","title":"2025","text":""},{"location":"archive/2025/page/5/","title":"2025","text":""},{"location":"archive/2025/page/6/","title":"2025","text":""},{"location":"archive/2025/page/7/","title":"2025","text":""},{"location":"category/analysis/page/2/","title":"Analysis","text":""},{"location":"category/til/page/2/","title":"TIL","text":""},{"location":"category/til/page/3/","title":"TIL","text":""},{"location":"category/til/page/4/","title":"TIL","text":""},{"location":"category/tutorial/page/2/","title":"Tutorial","text":""}]}