---
date:
    created: 2025-01-12
    updated: 2025-01-31
authors: [xy]
categories: [Tutorial]
tags: [low latency programming]
---

# Intro to Jax

<!-- more -->

!!! abstract
    Mutiple packages came to solve the speed issue of `python` in scientific computing. The de facto standard is of course `numpy`. Think also `numba`, `jax`, `torch` or even a new language :fire: that go beyond what `numpy` offers.  This post is a quick intro to my personal preference as of Jan 2025. 


## PyTree

At the center of Jax is undoubtedly `jax.Array`.  A nested container of these objects is called a PyTree, e.g. lists of dicts of Array. The Array's are the leaves of the PyTree. It is possible to register a custom Pytree class.

## Numpy operations

`jax.numpy` module is easy to use for `numpy` users because they share near identical API, intentionally. Operations in `jax.numpy` are built on top of the XLA compiler primitives for high performance numerical computations. 

There is the distinction between abstract structure and concete value of arrays in Jax. A `ShapeDtypeStruct` object captures dtype and shape, whereas `Array` carries the concrete values too.

The caveat of using `jax.Array` is that they are immutable. The "mutation" syntax is different from `numpy` and actually generates new arrays instead of mutate them in place, in constrast to `numpy`. 

```py exec="on" result="text" source="above"
import jax.numpy as jnp

x = jnp.array([1, 2, 3])

y = x.at[1:].set(0)
print(y)

z = x.at[1:].add(1)
print(z)
```

## Random numbers 

Be aware that `jax` follows the functional programming paradigm. This implies explicit key handling for samplers. The samplers can be composed with `vmap` to achieve vectorization
across all parameters. 

For example,  `random.t` has two parameters PRNG_key and degree of freedom, one can supply one array of keys to generate a collection of t distributed random variables with the same degree of freedom like so 

```py
jax.vmap(random.t, in_axis=(0,None))(keys:Array, df=2)
```

or generate a collection each with a different degree of freedom like so 

```py
# identical
jax.vmap(random.t)(keys:jax.Array, df:jax.Array)
jax.vmap(random.t, in_axis=0)(keys:jax.Array, df:jax.Array)
jax.vmap(random.t, in_axis=(0,0))(keys:jax.Array, df:jax.Array)
```

By default, `in_axes=0` which means vectorizing all leave nodes (arrays) with the leading dimension being the batch dimenesion. Note that the input of the function at hand can be an arbitrary PyTree. In such case the axis 0 will be the batch dimension for all arrays in each  PyTree, and the shape of the supplied arrays embedded in the PyTree should be compatible with the vectorization in_axis. 

Here is a example of working with PRNG keys. The warning from the official docs is NEVER reuse a key. Split them.  

```py exec="on" result="text" source="above"
import jax.random as jr
import jax.numpy as jnp

key = jr.key(42)
out1 = jr.normal(key, (10,))
out2 = jax.vmap(jr.normal)(jr.split(key, 10)) # vmap adds batch dimension at leading axis by default
out3 = jax.vmap(jr.t)(
    key = jr.split(jr.key(21),10),
    df = jnp.arange(10)+1 # degree of freedom of 10 t-distributions
)

print(out1)
print(out2)
print(out3)
```

Again we provide an Array `df` because Jax expects Array for vectorized arguments. Providing list (pytree!) of the same size wouldn't work. 

See details in the [official docs](https://jax.readthedocs.io/en/latest/jax.random.html#module-jax.random). 

## `vmap`

More control over the operating axes of `vmap` is possible. Below  `in_axes=(0,None)` imposes that the vectorization occurs in the first argument of the function with the batch axis 0. Without specifying `in_axes`, the `vmap(f)` would expect its arguments to be
arrays of rank (at least) 1 and having the same leading axis, which is not the case for our inputs.  

Notice that broadcasting Ã  la numpy is performed for the base function (before vmap). The effect of vmap is `np.stack` 
individual results of the function along the new out_axes (in this example the columns). Using `vmap` can avoid  manual batching, manual stacking etc.  

```py exec="on" result="text" source="above"
from jax import vmap, numpy as jnp

def f(x, y): return x + y

xs = jnp.array([0, 1, 2, 3])
y = jnp.array([4, 5])
out = vmap(f, in_axes=(0, None), out_axes=1)(xs, y)
print(out)
```
More details [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)

## `jit` if you can

In eager mode, `jax` transformations/opertors run sequentially one at a time.
With `jit` compilation, `jax` program, more precisely the underlying computation graph, is optimized (e.g. rearrange, fuse transformations) by XLA compiler so that it runs faster.

The idea of `jit` compilation is to run/compile the program in python once, and cache the compiled program for repetitive evaluations. Because of the overhead of compilation, it would be the best if similar inputs would not trigger re-compilation. To this end, `jax` transformations must be agnostic of the values of the inputs, and they must know the shape and dtype of the inputs and outputs to comply wiht the XLA's requirement of being compile-time static. 

It may sould like one should `jit` everything but this is not always possible. Consider this.

```py exec="on" result="text" source="above"
# NOT WORKING!
from jax import jit

@jit
def f(x):
  if x > 0: return x
  else: return jnp.stack([x,x])

try: f(3)
except Exception as e: print(e)
```

This function is not `jit`'able. This would error out becasue the *value* of `x` must be known upfront to be able to determine the shape of the output. Of course one can get around the `if` statement with `jnp.where` (which makes explicit both branches). Conveniently, ALL `jax.numpy` operations are jittable. 
But consider this

```py exec="on" result="text" source="above"
# NOT WORKING!
from jax import jit

@jit
def f(x):
  if x > 0: return x
  else: return jnp.stack([x,x])

try: f(3)
except Exception as e: print(e)
```

This function is not `jit` compatible neither.  XLA really does not like python while loops because they are fully dynamic. Intermediate states COULD (they don't necessary have to but they could) change dtype and shape depending on the value, so XLA has no hope to analyze it statically.

Two fixes are possible. One is to make it a static control flow by specifying the number of iterations i.e. treating `n` as constant at compile time. Effectively the loop is unrolled at compile time. 

```py exec="on" result="text" source="above"
from jax import jit
from functools import partial

@partial(jit, static_argnums=1)
def g(x, n):
    i = 0
    while i < n: i += 1
    return x + i

print(g(1, 5))
```

Another is to use (dynamic) structured control flow `jax.lax.while_loop`. The number of iterations is allowed to be dynamic (with static dtype and shape of course), but the structure of the contidion and body functions are static.  

```py exec="on" result="text" source="above"
from jax import jit
from jax.lax import while_loop

def cond_fun(val):
    i, n = val
    return i < n

def body_fun(val):
    i, n = val
    return i + 1, n

@jit
def g(x, n):
    end, _ = while_loop(cond_fun, body_fun, (0, n))
    return x + end

print(g(1, 5))
```

Details [here](https://jax.readthedocs.io/en/latest/jit-compilation.html#). More control flow operators: check [this page](https://jax.readthedocs.io/en/latest/jax.lax.html#lax-control-flow).

## `grad` `grad`!



When you can compose grad and jacobian to get Hessian, you know automatic differentiation of `jax` is done right. 
 Define your function and grad your way out with respect to any variable you are interested. Compose it with `jit` for performance. Some obvious caveats:

- functions must be scalar-valued (there is `jax.jacobian` for vector valued funcs)
- inputs must be continous (e.g. float)
- functions must be differentiable (indexing, argmax etc are not ok)


## Simple profiling/testing

Use `.block_until_ready()` on the output `jax.Array` of functions to measure the time consumed to execute the function. 
There is no testing module in `jax`.  Use `np.testing.assert_allclose` to check the results.  

## Type hints

Use `jax.typing.ArrayLike` for array input and `jax.Array` for array output. 

## Reference

!!! reference ""
    https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods
    https://jax.readthedocs.io/en/latest/working-with-pytrees.html#example-of-jax-tree-map-with-ml-model-parameters
    https://jax.readthedocs.io/en/latest/working-with-pytrees.html#custom-pytree-nodes
    https://jax.readthedocs.io/en/latest/stateful-computations.html#simple-worked-example-linear-regression


