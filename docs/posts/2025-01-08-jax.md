---
date:
    created: 2025-01-12
    updated: 2025-10-31
authors: [xy]
categories: [Tutorial]
tags: [low latency programming]
---

# Intro to Jax

<!-- more -->

!!! abstract
    Mutiple packages came to solve the speed issue of `python` in scientific computing. The de facto standard is of course `numpy`. Think also `numba`, `jax`, `torch` or even a new language :fire: that go beyond what `numpy` offers.  This post is a quick intro to my personal preference as of Jan 2025. 


## PyTree

At the center of jax is undoubtedly `jax.Array`.  A nested container of these objects is called a PyTree, e.g. lists of dicts of Array. The Array's are the leaves of the PyTree. It is possible to register a custom Pytree class.

## Numpy operations

`jax.numpy` module is easy to use for `numpy` users because they share near identical API, intentionally. Operations in `jax.numpy` are built on top of the XLA compiler primitives for high performance numerical computations. 

There is the distinction between abstract structure and concete value of arrays in jax. A `ShapeDtypeStruct` object captures dtype and shape, whereas `Array` carries the concrete values too.

The caveat of using `jax.Array` is that they are immutable. The "mutation" syntax is different from `numpy` and actually generates new arrays instead of mutate them in place, in constrast to `numpy`. 

```py exec="on" result="text" source="above"
import jax.numpy as jnp

x = jnp.array([1, 2, 3])

y = x.at[1:].set(0)
print(y)

z = x.at[1:].add(1)
print(z)
```

Broadcasting is arguably an elegant design of the numpy array API. It works the same for jax arrays. 

## `vmap`

`vmap` vectorizes a function by adding a batch dimension at the leading axis to *all* the leave nodes of a function's arguments.  
In other words, vmap is like `np.stack` individual results of a sequence of function calls. We use `vmap` to avoid manual batching, manual stacking etc.  

Since the arguments of the function at hand can be an arbitrary PyTree, the axis 0 will be the batch dimension for all arrays in each  PyTree, and the shape of the supplied arrays embedded in the PyTrees should be compatible with that.

More control over the operating axes of `vmap` is possible. Below  `in_axes=(None,0)` adds batch dimension at the leading axis for the  second positional argument of the function only. If we do not specify `in_axes` (defaults to 0 or equivalently (0,0)), the vectorized function `vmap(f)` would add batch dimension to all leave nodes of the input PyTree's. This behaviour is not compatible with the inputs in our example, because beta.shape is (5,) and x.shape is (2,5), so their leading dimensions are not the same.
    
```py
import jax
import jax.numpy as jnp

def li(beta, x):
    """linear predictor with multiple regressors"""
    return beta.dot(x)

beta = jnp.arange(5)

jax.vmap(li, in_axes=(None,0))(
    beta, 
    jr.normal(jr.key(1), (2,5))
)
```

Additionally, we can specify `out_axes` along which we stack the results, to use our `np.stack` analogy again. But in our example above since the output is 1D array, we can specify nothing other than `out_axes=0`, which is the default already.  

More details about vmap [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)

## Random numbers 

Be aware that `jax` follows the functional programming paradigm. This implies explicit key handling for samplers. Samplers can be composed with `vmap` to achieve vectorization across all parameters. 

For example,  `random.t` has two parameters key and df, one can supply one array of keys to generate a collection of t distributed random variables with the same degree of freedom like so 

```py
jax.vmap(random.t, in_axes=(0,None))(keys:Array, df=2)
```

or generate a collection each with a different degree of freedom like so 

```py
# identical
jax.vmap(random.t)(keys:jax.Array, df:jax.Array)
jax.vmap(random.t, in_axes=0)(keys:jax.Array, df:jax.Array)
jax.vmap(random.t, in_axes=(0,0))(keys:jax.Array, df:jax.Array)
```

Here is an example of working with PRNG keys. The warning from the official docs is NEVER reuse a key. Split them.  

```py exec="on" result="text" source="above"
import jax
import jax.random as jr
import jax.numpy as jnp

key = jr.key(42)
out1 = jr.normal(key, (10,))
out2 = jax.vmap(jr.normal)(jr.split(key, 10)) # vmap adds batch dimension at leading axis by default
out3 = jax.vmap(jr.t)(
    key = jr.split(jr.key(21),10),
    df = jnp.arange(10)+1 # degree of freedom of 10 t-distributions
)

print(out1)
print(out2)
print(out3)
```

Again we provide `df` of type Array because jax expects Array for vectorized arguments. Providing list (pytree!) of the same size wouldn't work. 

See details in the [official docs](https://jax.readthedocs.io/en/latest/jax.random.html#module-jax.random). 


## `jit` if you can

In eager mode, jax transformations/opertors run sequentially one at a time.
With `jit` compilation, the computation graph of a jax program is optimized (e.g. rearrange, fuse transformations) by XLA compiler so that it runs faster.

The idea of `jit` compilation is to run/compile the program in python once, and cache the compiled program for repetitive evaluations. Compilation introduces overhead. So what we want is that inputs of the same dtype and shape would not trigger re-compilation. jax achieves this by tracing the dtype and shape of all operands in a computational graph, and optimize this abstract structure without having to know the exact values.  

The approach has some implications. jax transformations such as vmap must be agnostic of the values of the inputs, and they must know the shape and dtype of the inputs and outputs to comply with the XLA's requirement of being compile-time static. 

It may sould like one should `jit` everything but this is not always possible. Consider this.

```py exec="on" result="text" source="above"
# NOT WORKING!
from jax import jit

@jit
def f(x):
  if x > 0: return x
  else: return jnp.stack([x,x])

try: f(3)
except Exception as e: print(e)
```

This function is not `jit`'able. This would error out becasue the *value* of `x` must be known upfront to be able to determine the shape of the output. Of course one can get around the `if` statement with `jnp.where` (which makes explicit both branches). Conveniently, ALL `jax.numpy` operations are jittable. 
But consider this

```py exec="on" result="text" source="above"
# NOT WORKING!
from jax import jit

@jit
def g(x, n):
    i = 0
    while i < n: i += 1
    return x + i

try: g(0, 10)
except Exception as e: print(e)
```

This function is not `jit` compatible neither because jax requires that the number of loop iterations in standard Python control flow be known at compile time (trace time), not run time. jax needs to know the exact number of operations to include in the computational graph before compilation. 

Two fixes are possible. One is to make it a **static** control flow by specifying the number of iterations i.e. treating `n` as constant at compile time. Effectively the loop is unrolled at compile time. 

```py exec="on" result="text" source="above"
from jax import jit
from functools import partial

@partial(jit, static_argnums=1)
def g(x, n):
    i = 0
    while i < n: i += 1
    return x + i

print(g(1, 5))
```

Another is to use dynamic structured control flow `jax.lax.while_loop`. The number of iterations is allowed to be dynamic (with static dtype and shape of course), but the structure of the contidion and body functions are static.  

```py exec="on" result="text" source="above"
from jax import jit
from jax.lax import while_loop

def cond_fun(val):
    i, n = val
    return i < n

def body_fun(val):
    i, n = val
    return i + 1, n

@jit
def g(x, n):
    end, _ = while_loop(cond_fun, body_fun, (0, n))
    return x + end

print(g(1, 5))
```

Details [here](https://jax.readthedocs.io/en/latest/jit-compilation.html#). More control flow operators: check [this page](https://jax.readthedocs.io/en/latest/jax.lax.html#lax-control-flow).

## `grad` `grad`!



When you can compose grad and jacobian to get Hessian, you know automatic differentiation of `jax` is done right. 
 Define your function and grad your way out with respect to any variable you are interested. Compose it with `jit` for performance. Some obvious caveats:

- functions must be scalar-valued (there is `jax.jacobian` for vector valued funcs)
- inputs must be continous (e.g. float)
- functions must be differentiable (indexing, argmax etc are not ok)


## Simple profiling/testing

Use `.block_until_ready()` on the output `jax.Array` of functions to measure the time consumed to execute the function. 
There is no testing module in `jax`.  Use `np.testing.assert_allclose` to check the results.  

## Type hints

Use `jax.typing.ArrayLike` for array input and `jax.Array` for array output. 

## Reference

!!! reference ""
    https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods
    https://jax.readthedocs.io/en/latest/working-with-pytrees.html#example-of-jax-tree-map-with-ml-model-parameters
    https://jax.readthedocs.io/en/latest/working-with-pytrees.html#custom-pytree-nodes
    https://jax.readthedocs.io/en/latest/stateful-computations.html#simple-worked-example-linear-regression


