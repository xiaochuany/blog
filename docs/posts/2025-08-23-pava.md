---
date: 
    created: 2025-08-23
authors: [xy]
categories: [TIL]
tags: [machine learning]
---

# Monotonic binning with PAVA

In credit risk modelling, a common requirement is to maintain monotonicity of the target variable wrt risk drivers. Think PD modelling where the target is the probability of default and the risk drivers can be customer characteristics and/or their financial situation. 

A useful technique in this regard is ***monotonic binning***. The idea is to bin the continous variable in some user-defined way (e.g. evenly-spread breakpoints or quantile based) then merge consecutive bins so that the default rate of these bins are monotone. 

PAVA or ***pooled adjacent validators algorithm*** is a classical algorithm for solving optimization problems with monotonicity constraint such as isotonic regression. In this post we adapt its core idea to solve monotonic binning problem.  

We took some effort simplifying as much as possible the implementation. What you see below is just the barebone.
Even though the code is short, it deserves some analysis for the correctness, which we will do after. 

## PAVA

We maintain two states

- list of pools that store current clusters
- list of values that stores the statistics (number of default and total in each cluster)

Both states have dynamic length. We iteratively compare current and next pool (value)

- either constraint is met we advance by one index
- or constraint is vialated we merge the current and next pool and backtrack to check the constraint, keep merging and backtracking if necessary until constraint is met (so the values before current maintains monotonicity).

That's it!

```py
def pava(x, constraint=lambda a, b: a[0] / a[1] <= b[0] / b[1]):
    """x : list of pairs of counts [(n_default, total),...]"""
    n = len(x)
    active_pools = [[i] for i in range(n)]
    active_values = list(np.asarray(x))

    i = 0
    while i < len(active_pools) - 1:
        if not constraint(active_values[i], active_values[i + 1]):
            # Violation found, merge pools
            active_pools[i].extend(active_pools[i + 1])
            del active_pools[i + 1]
            active_values[i] = active_values[i] + active_values[i + 1]
            del active_values[i + 1]
            # Step back to check for new violations with the merged pool
            if i > 0: i -= 1
        else:
            i += 1

    return active_pools
```

## Corretness

Because the index could avance and backtrack, it is not immediately obvious what the time complexity of the algorithm should be.

We can decompose the whole process into two alternating phases

1. either forward merge happens and backtracks and merges until monotonicity is restored;
1. or forward merge does not happen and index advances by 1 (keep doing this until cannot advance)

Index advancing happens at most n times because the number of indices ahead of the current index stays unchanged during the "merge and backtrack" phase. Everytime we advances index there are less places for the advancement to happen.

Let $b_i$ denote the number of index backtracks in the i-th round of phase 1 (forward merge followed by a number of backward merges). we bound 

$$
\sum_{i}^r b_i  =  r + \sum_{i=1}^r (b_i-1) \le \sharp\{forward\ merge\}+\sharp\{backward\ merge\} \le \sharp\{merge\}\le n
$$


the bulk of the work is done by the merges which are O(1) operations. there are at most n merges because the total number of merges cannot be larger than the number of data points.

Putting things together we see that PAVA is O(n). 

## The catch

Careful reader must have noticed that the element removal from a list in the while loop is an O(n) operation in the worst case, which makes the binning algorithm $O(n^2)$. 

To really achieve the claimed O(n) complexity, we can  use a different data structure that support O(1) element removal (e.g. linked list), which we do not do here for clarity of the code. 

## Bonus: stack based approach

The following is genrated one-shot by Gemini Pro 2.5. I would say it's as short as the previous one, but achieving O(n) without using a linked list. 

```py
def pavai(x, constraint=lambda a, b: a[0] / a[1] <= b[0] / b[1]):
    """
    x: list of pairs of counts, e.g., [(numerator, denominator), ...]
    """
    if not x:
        return []

    stack = [] # store tuples of (value, indices) for each active pool

    for i, val in enumerate(x):
        current_value = np.asarray(val)
        current_indices = [i]

        # --- Backwards Merging ---
        # While the stack is not empty and the top pool violates the constraint
        # with the current pool, merge them.
        while stack and not constraint(stack[-1][0], current_value):
            # Pop the last pool from the stack.
            prev_value, prev_indices = stack.pop()

            # Merge it into the current pool.
            current_value += prev_value
            current_indices = prev_indices + current_indices

        # Now that all violations are resolved, push the new pool onto the stack.
        stack.append((current_value, current_indices))

    # --- Finalization: Unpack the stack into the final list of pools ---
    final_pools = [indices for value, indices in stack]

    return final_pools
```